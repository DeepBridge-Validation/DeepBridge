{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepBridge AutoDistiller Demo - Using Pre-calculated Probabilities\n",
    "\n",
    "This notebook demonstrates how to use the DeepBridge `AutoDistiller` to compress a complex model into a simpler one, using pre-calculated probabilities from the teacher model.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Knowledge distillation is a technique where a simpler model (student) learns to mimic the behavior of a more complex model (teacher). This can help create models that are:\n",
    "- Smaller and faster\n",
    "- More interpretable\n",
    "- Easier to deploy\n",
    "\n",
    "In this demo, we'll use pre-calculated probabilities from a neural network model (which we'll assume was already trained) and distill its knowledge into a simpler model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# Import DeepBridge components\n",
    "from deepbridge.db_data import DBDataset\n",
    "from deepbridge.auto_distiller import AutoDistiller\n",
    "from deepbridge.distillation.classification.model_registry import ModelType\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Data\n",
    "\n",
    "In a real scenario, you would load your actual dataset. For this demo, we'll generate a synthetic classification dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a binary classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, \n",
    "    n_features=20, \n",
    "    n_informative=10, \n",
    "    n_redundant=5, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convert to DataFrame for better handling\n",
    "feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "data = pd.DataFrame(X, columns=feature_names)\n",
    "data['target'] = y\n",
    "\n",
    "# Display the first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Teacher Model Probabilities\n",
    "\n",
    "In a real-world scenario, you would have probabilities from a pre-trained complex model (like a neural network). For this demo, we'll simulate those probabilities by creating a complex probability distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate teacher model probability outputs (typically from a neural network)\n",
    "# For the positive class (y=1), generate values that correlate with the true labels but have some noise\n",
    "# This simulates how a real model would produce probabilities\n",
    "\n",
    "# Base probability highly correlated with true label\n",
    "base_probs = y * 0.7 + (1 - y) * 0.3\n",
    "\n",
    "# Add some Gaussian noise but keep values between 0 and 1\n",
    "noise = np.random.normal(0, 0.1, size=len(y))\n",
    "probabilities = np.clip(base_probs + noise, 0.01, 0.99)\n",
    "\n",
    "# Create DataFrame with probabilities\n",
    "prob_df = pd.DataFrame({\n",
    "    'prob_class_0': 1 - probabilities,\n",
    "    'prob_class_1': probabilities\n",
    "})\n",
    "\n",
    "# Display the first few rows of probabilities\n",
    "print(\"Teacher model probability outputs:\")\n",
    "prob_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DBDataset with Pre-calculated Probabilities\n",
    "\n",
    "The `DBDataset` class is DeepBridge's way of organizing data, features, and model predictions. We'll create one using our data and the simulated teacher model probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train/test split\n",
    "train_indices = np.random.choice(len(data), int(0.8 * len(data)), replace=False)\n",
    "test_indices = np.array([i for i in range(len(data)) if i not in train_indices])\n",
    "\n",
    "train_data = data.iloc[train_indices].reset_index(drop=True)\n",
    "test_data = data.iloc[test_indices].reset_index(drop=True)\n",
    "\n",
    "train_probs = prob_df.iloc[train_indices].reset_index(drop=True)\n",
    "test_probs = prob_df.iloc[test_indices].reset_index(drop=True)\n",
    "\n",
    "# Create DBDataset with probabilities\n",
    "dataset = DBDataset(\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    target_column='target',\n",
    "    train_predictions=train_probs,\n",
    "    test_predictions=test_probs,\n",
    "    prob_cols=['prob_class_0', 'prob_class_1']\n",
    ")\n",
    "\n",
    "# Verify the dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and Run the AutoDistiller\n",
    "\n",
    "Now we'll use the `AutoDistiller` to find the best simple model to replace our complex one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the AutoDistiller with our dataset\n",
    "distiller = AutoDistiller(\n",
    "    dataset=dataset,\n",
    "    output_dir=\"distillation_results\",\n",
    "    test_size=0.2,  # For internal validation\n",
    "    n_trials=10,    # Number of hyperparameter trials\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Customize the configuration (optional)\n",
    "# We'll test multiple model types, temperatures, and alpha values\n",
    "distiller.customize_config(\n",
    "    model_types=[\n",
    "        ModelType.LOGISTIC_REGRESSION,\n",
    "        ModelType.DECISION_TREE,\n",
    "        ModelType.GBM\n",
    "    ],\n",
    "    temperatures=[0.5, 1.0, 2.0],\n",
    "    alphas=[0.3, 0.7]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the distillation process\n",
    "# This will test all combinations of model types, temperatures, and alphas\n",
    "results_df = distiller.run(use_probabilities=True, verbose_output=True)\n",
    "\n",
    "# Display the results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Let's analyze the results to find the best model configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model based on accuracy\n",
    "best_accuracy_config = distiller.find_best_model(metric='test_accuracy')\n",
    "print(\"Best model configuration by accuracy:\")\n",
    "for key, value in best_accuracy_config.items():\n",
    "    if key not in ['best_params']:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model based on KL divergence (which measures how well the student mimics the teacher)\n",
    "best_kl_config = distiller.find_best_model(metric='test_kl_divergence', minimize=True)\n",
    "print(\"Best model configuration by KL divergence (lower is better):\")\n",
    "for key, value in best_kl_config.items():\n",
    "    if key not in ['best_params']:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Report\n",
    "\n",
    "The AutoDistiller can automatically generate a comprehensive report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a summary report\n",
    "summary = distiller.generate_summary()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Best Distilled Model\n",
    "\n",
    "Finally, let's save the best distilled model for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model based on KL divergence\n",
    "model_path = distiller.save_best_model(\n",
    "    metric='test_kl_divergence', \n",
    "    minimize=True,\n",
    "    file_path='best_distilled_model.pkl'\n",
    ")\n",
    "print(f\"Best model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've successfully distilled the knowledge from a complex model (represented by pre-calculated probabilities) into a simpler, more efficient model. The distilled model maintains accuracy while being much simpler than the original neural network.\n",
    "\n",
    "Key benefits:\n",
    "- Smaller model size\n",
    "- Faster inference time\n",
    "- Potentially more interpretable (depending on the model type)\n",
    "\n",
    "This approach is particularly useful when:\n",
    "- You have a large, complex model that needs to be deployed on resource-constrained environments\n",
    "- You want to maintain accuracy while improving inference speed\n",
    "- You need a more interpretable model for regulatory or explainability requirements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_prod_deepbridge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
