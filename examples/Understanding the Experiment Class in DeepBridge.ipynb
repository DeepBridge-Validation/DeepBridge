{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Experiment Class in DeepBridge\n",
    "\n",
    "This notebook explains the purpose and functionality of the `Experiment` class in the DeepBridge library, which is a key component for managing and executing model validation and distillation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to the Experiment Class\n",
    "\n",
    "The `Experiment` class in DeepBridge serves as a container for experiments related to model validation and distillation. It encapsulates the entire workflow of preparing data, running experiments, and evaluating results.\n",
    "\n",
    "Let's first import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guhaase/projetos/deepbridge_homol/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "sys.path.append(os.path.expanduser(\"~/projetos/DeepBridge\"))\n",
    "\n",
    "# Import DeepBridge components\n",
    "from deepbridge.db_data import DBDataset\n",
    "from deepbridge.experiment import Experiment\n",
    "from deepbridge.distillation.classification.model_registry import ModelType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating a Basic Experiment\n",
    "\n",
    "To demonstrate the Experiment class, let's first generate some synthetic data and a teacher model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, \n",
    "                           n_classes=2, random_state=42)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "X_df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "y_df = pd.Series(y, name='target')\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a \"teacher\" model (e.g., a complex RandomForest)\n",
    "teacher_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "teacher_model.fit(X_train, y_train)\n",
    "\n",
    "# Generate probability predictions from the teacher model\n",
    "train_probs = teacher_model.predict_proba(X_train)\n",
    "test_probs = teacher_model.predict_proba(X_test)\n",
    "\n",
    "# Create DataFrame with probabilities\n",
    "train_probs_df = pd.DataFrame(train_probs, columns=['prob_class_0', 'prob_class_1'], index=X_train.index)\n",
    "test_probs_df = pd.DataFrame(test_probs, columns=['prob_class_0', 'prob_class_1'], index=X_test.index)\n",
    "\n",
    "# Create a DBDataset instance\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "dataset = DBDataset(\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    target_column='target',\n",
    "    features=X_df.columns.tolist(),\n",
    "    train_predictions=train_probs_df,\n",
    "    test_predictions=test_probs_df,\n",
    "    prob_cols=['prob_class_0', 'prob_class_1']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset prepared, we can create an Experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating distillation model on train dataset ===\n",
      "Student predictions shape: (800, 2)\n",
      "First 3 student probabilities: [[3.20679257e-04 9.99679321e-01]\n",
      " [9.99951268e-01 4.87318541e-05]\n",
      " [9.99710938e-01 2.89061720e-04]]\n",
      "Teacher probabilities type: <class 'pandas.core.frame.DataFrame'>\n",
      "Using 'prob_class_1' column from teacher probabilities\n",
      "Teacher probabilities shape: (800, 2)\n",
      "First 3 teacher probabilities (positive class): [0.71 0.   0.08]\n",
      "KS Statistic calculation: 0.47, p-value: 3.1408946187192175e-80\n",
      "R² Score calculation: 0.8846692436908336\n",
      "Teacher prob type: <class 'numpy.ndarray'>, shape: (800,)\n",
      "Student prob type: <class 'numpy.ndarray'>, shape: (800,)\n",
      "Teacher prob first 5 values: [0.71 0.   0.08 0.88 0.39]\n",
      "Student prob first 5 values: [9.99679321e-01 4.87318541e-05 2.89061720e-04 9.99927453e-01\n",
      " 6.18127369e-04]\n",
      "KS calculation successful: (0.47, 3.1408946187192175e-80)\n",
      "Sorted teacher dist - min: 0.0, max: 1.0, length: 800\n",
      "Sorted student dist - min: 2.8611958523003585e-05, max: 0.9999662378411595, length: 800\n",
      "R² calculation result: 0.8846692436908336\n",
      "R² calculation successful: 0.8846692436908336\n",
      "Evaluation metrics: {'accuracy': 0.97625, 'precision': 0.9655172413793104, 'recall': 0.9874055415617129, 'f1_score': 0.9763387297633873, 'auc_roc': 0.9964810520591784, 'auc_pr': 0.9950938354800609, 'log_loss': 0.11264260878532095, 'kl_divergence': 0.6581820973227428, 'ks_statistic': 0.47, 'ks_pvalue': 3.1408946187192175e-80, 'r2_score': 0.8846692436908336, 'distillation_method': 'SurrogateModel'}\n",
      "=== Evaluation complete ===\n",
      "\n",
      "\n",
      "=== Evaluating distillation model on test dataset ===\n",
      "Student predictions shape: (200, 2)\n",
      "First 3 student probabilities: [[9.99749716e-01 2.50284358e-04]\n",
      " [1.78102985e-04 9.99821897e-01]\n",
      " [8.62804031e-02 9.13719597e-01]]\n",
      "Teacher probabilities type: <class 'pandas.core.frame.DataFrame'>\n",
      "Using 'prob_class_1' column from teacher probabilities\n",
      "Teacher probabilities shape: (200, 2)\n",
      "First 3 teacher probabilities (positive class): [0.92 0.94 0.92]\n",
      "KS Statistic calculation: 0.46, p-value: 1.8323127534838935e-19\n",
      "R² Score calculation: 0.8975702803391267\n",
      "Teacher prob type: <class 'numpy.ndarray'>, shape: (200,)\n",
      "Student prob type: <class 'numpy.ndarray'>, shape: (200,)\n",
      "Teacher prob first 5 values: [0.92 0.94 0.92 0.77 0.02]\n",
      "Student prob first 5 values: [2.50284358e-04 9.99821897e-01 9.13719597e-01 9.77039272e-01\n",
      " 4.38807656e-05]\n",
      "KS calculation successful: (0.46, 1.8323127534838935e-19)\n",
      "Sorted teacher dist - min: 0.0, max: 1.0, length: 200\n",
      "Sorted student dist - min: 2.846419174500984e-07, max: 0.9999919390814311, length: 200\n",
      "R² calculation result: 0.8975702803391267\n",
      "R² calculation successful: 0.8975702803391267\n",
      "Evaluation metrics: {'accuracy': 0.85, 'precision': 0.8135593220338984, 'recall': 0.9230769230769231, 'f1_score': 0.8648648648648649, 'auc_roc': 0.945713141025641, 'auc_pr': 0.9384151864215601, 'log_loss': 0.5436413500322007, 'kl_divergence': 0.6192884480731168, 'ks_statistic': 0.46, 'ks_pvalue': 1.8323127534838935e-19, 'r2_score': 0.8975702803391267, 'distillation_method': 'SurrogateModel'}\n",
      "=== Evaluation complete ===\n",
      "\n",
      "Experiment type: binary_classification\n",
      "Training data shape: (800, 20)\n",
      "Test data shape: (200, 20)\n",
      "Training labels shape: (800,)\n",
      "Test labels shape: (200,)\n",
      "Training probability predictions shape: (800, 2)\n",
      "Test probability predictions shape: (200, 2)\n"
     ]
    }
   ],
   "source": [
    "# Create an experiment\n",
    "experiment = Experiment(\n",
    "    dataset=dataset,\n",
    "    experiment_type=\"binary_classification\"\n",
    ")\n",
    "\n",
    "# Let's verify our experiment has been initialized correctly\n",
    "print(f\"Experiment type: {experiment.experiment_type}\")\n",
    "print(f\"Training data shape: {experiment.X_train.shape}\")\n",
    "print(f\"Test data shape: {experiment.X_test.shape}\")\n",
    "print(f\"Training labels shape: {experiment.y_train.shape}\")\n",
    "print(f\"Test labels shape: {experiment.y_test.shape}\")\n",
    "print(f\"Training probability predictions shape: {experiment.prob_train.shape}\")\n",
    "print(f\"Test probability predictions shape: {experiment.prob_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1763773627.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    =\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Train the distillation model first\n",
    "experiment.fit(\n",
    "    student_model_type=ModelType.GBM,  # or whichever model you prefer\n",
    "    temperature=1.0,\n",
    "    alpha=0.5,\n",
    "    use_probabilities=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opção 1: Importar diretamente\n",
    "from deepbridge.validation.experiment_extensions import analyze_hyperparameters_workaround_fixed\n",
    "importance = analyze_hyperparameters_workaround_fixed(experiment)\n",
    "\n",
    "# Opção 2: Usar como método do Experiment\n",
    "importance = experiment.analyze_hyperparameters_workaround_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try simpler fit call\n",
    "print(\"Trying simplified fit...\")\n",
    "experiment.fit(verbose=True\n",
    ")\n",
    "\n",
    "# Check if model was created\n",
    "print(f\"Distillation model after simplified fit: {experiment.distillation_model is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if extensions are properly integrated\n",
    "print(\"Extensions check:\")\n",
    "print(f\"Has analyze_hyperparameter_importance: {hasattr(experiment, 'analyze_hyperparameter_importance')}\")\n",
    "print(f\"Has optimize_hyperparameters: {hasattr(experiment, 'optimize_hyperparameters')}\")\n",
    "print(f\"Has estimate_uncertainty: {hasattr(experiment, 'estimate_uncertainty')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.analyze_hyperparameter_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the hyperparameter importance analysis\n",
    "importance = experiment.analyze_hyperparameter_importance(\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training a Distillation Model\n",
    "\n",
    "The most common use case for the Experiment class is training a distillation model. Let's demonstrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a distillation model using the experiment\n",
    "# Usar fit() com SurrogateModel (método padrão)\n",
    "experiment.fit(\n",
    "    student_model_type=ModelType.GBM,\n",
    "    student_params={'n_estimators': 100, 'learning_rate': 0.1},\n",
    "    use_probabilities=True,\n",
    "    verbose=True\n",
    "    # distillation_method=\"surrogate\" não é necessário especificar, já é o padrão\n",
    ")\n",
    "\n",
    "# Check the results of our distillation\n",
    "print(\"\\nDistillation Results:\")\n",
    "print(\"Train metrics:\", experiment.results['train'])\n",
    "print(\"\\nTest metrics:\", experiment.results['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_treinado = experiment.distillation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_previsoes = experiment.get_student_predictions(dataset='test')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_previsoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Key Features of the Experiment Class\n",
    "\n",
    "The Experiment class provides several key features and methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Automatic Data Preparation\n",
    "\n",
    "The class automatically handles data splitting and preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can access the prepared data splits directly\n",
    "print(f\"X_train shape: {experiment.X_train.shape}\")\n",
    "print(f\"X_test shape: {experiment.X_test.shape}\")\n",
    "\n",
    "# We can also get both features and target using the get_dataset_split method\n",
    "X_train, y_train, prob_train = experiment.get_dataset_split('train')\n",
    "X_test, y_test, prob_test = experiment.get_dataset_split('test')\n",
    "\n",
    "print(f\"Features from get_dataset_split (train): {X_train.shape}\")\n",
    "print(f\"Target from get_dataset_split (train): {y_train.shape}\")\n",
    "print(f\"Probabilities from get_dataset_split (train): {prob_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Getting Student Model Predictions\n",
    "\n",
    "We can easily get predictions from the trained student model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from the student model\n",
    "student_predictions = experiment.get_student_predictions(dataset='test')\n",
    "\n",
    "# Let's look at the first few predictions\n",
    "print(\"Student model predictions:\")\n",
    "print(student_predictions.head())\n",
    "\n",
    "# Calculate metrics for the student model\n",
    "student_metrics = experiment.calculate_student_metrics(dataset='test')\n",
    "print(\"\\nStudent model metrics:\")\n",
    "for metric, value in student_metrics.items():\n",
    "    print(f\"  {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Comparing Teacher and Student Models\n",
    "\n",
    "One of the most valuable features is the ability to compare teacher and student models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare teacher and student model metrics\n",
    "comparison_df = experiment.compare_teacher_student_metrics()\n",
    "\n",
    "# Let's see the comparison\n",
    "print(\"Teacher vs Student Model Comparison:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Evaluating Custom Predictions\n",
    "\n",
    "The class also allows us to evaluate custom predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some simple predictions to evaluate\n",
    "custom_predictions = pd.DataFrame({\n",
    "    'y_pred': (X_test.iloc[:, 0] > 0).astype(int),  # Simple threshold-based prediction\n",
    "    'prob_1': X_test.iloc[:, 0].clip(0, 1)  # Simple probability based on first feature\n",
    "})\n",
    "\n",
    "# Evaluate these predictions\n",
    "custom_metrics = experiment.evaluate_predictions(\n",
    "    predictions=custom_predictions,\n",
    "    dataset='test',\n",
    "    prob_column='prob_1'\n",
    ")\n",
    "\n",
    "print(\"\\nCustom prediction metrics:\")\n",
    "for metric, value in custom_metrics.items():\n",
    "    print(f\"  {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. How It Works: The Experiment Workflow\n",
    "\n",
    "Let's explore the main workflow of the Experiment class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Data Preparation\n",
    "\n",
    "The `_prepare_data` method handles the train-test split. Here's what it does internally:\n",
    "\n",
    "```python\n",
    "def _prepare_data(self) -> None:\n",
    "    \"\"\"\n",
    "    Prepare the data by performing train-test split on features and target.\n",
    "    \"\"\"\n",
    "    X = self.dataset.X\n",
    "    y = self.dataset.target\n",
    "    \n",
    "    self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=self.test_size,\n",
    "        random_state=self.random_state\n",
    "    )\n",
    "    \n",
    "    # Split probabilities if available\n",
    "    if self.dataset.original_prob is not None:\n",
    "        prob_train_idx = self.X_train.index\n",
    "        prob_test_idx = self.X_test.index\n",
    "        \n",
    "        self.prob_train = self.dataset.original_prob.loc[prob_train_idx]\n",
    "        self.prob_test = self.dataset.original_prob.loc[prob_test_idx]\n",
    "    else:\n",
    "        self.prob_train = None\n",
    "        self.prob_test = None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Distillation Training\n",
    "\n",
    "The core of the distillation process happens in the `fit` method, which initializes a KnowledgeDistillation object. Here's a simplified version of what happens internally:\n",
    "\n",
    "```python\n",
    "def fit(self, student_model_type, ...) -> 'Experiment':\n",
    "    if use_probabilities:\n",
    "        # Create distillation model using pre-calculated probabilities\n",
    "        self.distillation_model = KnowledgeDistillation.from_probabilities(\n",
    "            probabilities=self.prob_train,\n",
    "            student_model_type=student_model_type,\n",
    "            temperature=temperature,\n",
    "            alpha=alpha,\n",
    "            ...\n",
    "        )\n",
    "    else:\n",
    "        # Create distillation model using the teacher model directly\n",
    "        self.distillation_model = KnowledgeDistillation(\n",
    "            teacher_model=self.dataset.model,\n",
    "            student_model_type=student_model_type,\n",
    "            ...\n",
    "        )\n",
    "    \n",
    "    # Train the model\n",
    "    self.distillation_model.fit(self.X_train, self.y_train, verbose=verbose)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    train_metrics = self._evaluate_distillation_model('train')\n",
    "    test_metrics = self._evaluate_distillation_model('test')\n",
    "    \n",
    "    # Store results\n",
    "    self.results['train'] = train_metrics['metrics']\n",
    "    self.results['test'] = test_metrics['metrics']\n",
    "    \n",
    "    return self\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Evaluation Process\n",
    "\n",
    "The evaluation process is handled by `_evaluate_distillation_model`. Here's a simplified version of what happens internally:\n",
    "\n",
    "```python\n",
    "def _evaluate_distillation_model(self, dataset: str = 'test') -> dict:\n",
    "    # Get the appropriate data for evaluation\n",
    "    if dataset == 'train':\n",
    "        X, y, prob = self.X_train, self.y_train, self.prob_train\n",
    "    else:\n",
    "        X, y, prob = self.X_test, self.y_test, self.prob_test\n",
    "    \n",
    "    # Get predictions from the student model\n",
    "    y_pred = self.distillation_model.predict(X)\n",
    "    y_prob = self.distillation_model.predict_proba(X)\n",
    "    \n",
    "    # Extract probability of positive class\n",
    "    student_prob_pos = y_prob[:, 1] if y_prob.shape[1] > 1 else y_prob\n",
    "    \n",
    "    # Process teacher probabilities for comparison\n",
    "    teacher_prob_pos = self._extract_teacher_probabilities(prob)\n",
    "    \n",
    "    # Calculate metrics using the Classification class\n",
    "    metrics = self.metrics_calculator.calculate_metrics(\n",
    "        y_true=y,\n",
    "        y_pred=y_pred,\n",
    "        y_prob=student_prob_pos,\n",
    "        teacher_prob=teacher_prob_pos\n",
    "    )\n",
    "    \n",
    "    return {'metrics': metrics, 'predictions': predictions_df}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Saving and Loading Models\n",
    "\n",
    "While not directly implemented in the shown code, you would typically save and load the trained distillation model like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the distilled model\n",
    "joblib.dump(experiment.distillation_model, 'distilled_model.pkl')\n",
    "\n",
    "# Later, load the model\n",
    "loaded_model = joblib.load('distilled_model.pkl')\n",
    "\n",
    "# Use the loaded model for predictions\n",
    "predictions = loaded_model.predict(X_test)\n",
    "probabilities = loaded_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "The `Experiment` class in DeepBridge is a powerful tool for managing the complete knowledge distillation workflow. It provides:\n",
    "\n",
    "1. **Data Management**: Automatic handling of data preparation and splitting\n",
    "2. **Model Training**: Simplified interface for knowledge distillation with various configurations\n",
    "3. **Evaluation**: Comprehensive metrics calculation for both teacher and student models\n",
    "4. **Comparison**: Tools to compare different models and configurations\n",
    "5. **Versatility**: Support for different experiment types and model configurations\n",
    "\n",
    "This makes it an indispensable tool for anyone working with model distillation or looking to create more efficient versions of complex models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepbridge_homol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
