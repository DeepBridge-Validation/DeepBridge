{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# üîß Bias Mitigation in ML Models\n",
    "\n",
    "<div style=\"background-color: #e3f2fd; padding: 15px; border-radius: 5px; border-left: 5px solid #2196F3;\">\n",
    "<b>üìì Information</b><br>\n",
    "<b>Level:</b> Advanced<br>\n",
    "<b>Duration:</b> 25 minutes<br>\n",
    "<b>Dataset:</b> Adult Income (synthetic)<br>\n",
    "<b>Prerequisite:</b> 02_complete_fairness_analysis.ipynb\n",
    "</div>\n",
    "\n",
    "## üéØ Objectives\n",
    "- ‚úÖ Identify bias in the model\n",
    "- ‚úÖ Learn mitigation techniques (Pre/In/Post-processing)\n",
    "- ‚úÖ Implement practical mitigations\n",
    "- ‚úÖ Re-validate fairness after mitigation\n",
    "- ‚úÖ Compare Before vs After\n",
    "- ‚úÖ Understand trade-offs (accuracy vs fairness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## üìö Types of Bias Mitigation\n",
    "\n",
    "### 1. **Pre-processing** (Before Training)\n",
    "Modify the **data** to remove bias\n",
    "- Reweighting\n",
    "- Resampling\n",
    "- Feature transformation\n",
    "\n",
    "**Advantages**: Works with any model\n",
    "**Disadvantages**: May lose information\n",
    "\n",
    "### 2. **In-processing** (During Training)\n",
    "Modify the **training algorithm**\n",
    "- Fairness constraints\n",
    "- Adversarial debiasing\n",
    "- Fairness regularization\n",
    "\n",
    "**Advantages**: Integrated into training\n",
    "**Disadvantages**: Requires specific model\n",
    "\n",
    "### 3. **Post-processing** (After Training)\n",
    "Modify the model's **predictions**\n",
    "- Threshold optimization\n",
    "- Calibrated equalized odds\n",
    "- Reject option classification\n",
    "\n",
    "**Advantages**: No need to retrain\n",
    "**Disadvantages**: May not be optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup and Biased Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from deepbridge import DBDataset, Experiment\n",
    "\n",
    "# Configure visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "print(\"üìä Creating dataset with intentional bias...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic dataset with BIAS\n",
    "np.random.seed(42)\n",
    "n = 2000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'age': np.random.randint(18, 70, n),\n",
    "    'education_years': np.random.randint(8, 20, n),\n",
    "    'hours_per_week': np.random.randint(20, 60, n),\n",
    "    'gender': np.random.choice(['Male', 'Female'], n, p=[0.6, 0.4])\n",
    "})\n",
    "\n",
    "# Target WITH BIAS: gender significantly affects outcome\n",
    "base_prob = 0.3\n",
    "prob = base_prob + \\\n",
    "       (df['age'] - 40) * 0.005 + \\\n",
    "       (df['education_years'] - 12) * 0.03 + \\\n",
    "       (df['hours_per_week'] - 40) * 0.01 + \\\n",
    "       (df['gender'] == 'Male') * 0.15  # ‚Üê STRONG BIAS\n",
    "\n",
    "df['high_income'] = (prob + np.random.normal(0, 0.1, n) > 0.5).astype(int)\n",
    "\n",
    "print(f\"‚úÖ Dataset created: {df.shape}\")\n",
    "print(f\"\\nüìä Overall rate: {df['high_income'].mean():.1%}\")\n",
    "print(f\"\\nBy gender (data):\")\n",
    "for g in ['Male', 'Female']:\n",
    "    rate = df[df['gender']==g]['high_income'].mean()\n",
    "    print(f\"  {g}: {rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Train ORIGINAL Model (with bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "df_encoded = df.copy()\n",
    "df_encoded['gender_enc'] = (df['gender'] == 'Male').astype(int)\n",
    "\n",
    "feature_cols = ['age', 'education_years', 'hours_per_week', 'gender_enc']\n",
    "X = df_encoded[feature_cols]\n",
    "y = df_encoded['high_income']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ORIGINAL model\n",
    "clf_original = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_original.fit(X_train, y_train)\n",
    "\n",
    "y_pred_original = clf_original.predict(X_test)\n",
    "acc_original = accuracy_score(y_test, y_pred_original)\n",
    "\n",
    "print(f\"‚úÖ ORIGINAL model trained\")\n",
    "print(f\"üìä Accuracy: {acc_original:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Measure Bias in ORIGINAL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate disparate impact\n",
    "test_indices = X_test.index\n",
    "test_gender = df_encoded.loc[test_indices, 'gender']\n",
    "\n",
    "male_rate_orig = y_pred_original[test_gender == 'Male'].mean()\n",
    "female_rate_orig = y_pred_original[test_gender == 'Female'].mean()\n",
    "di_orig = female_rate_orig / male_rate_orig if male_rate_orig > 0 else 0\n",
    "\n",
    "print(\"\\nüìä ORIGINAL MODEL - FAIRNESS:\\n\" + \"=\"*50)\n",
    "print(f\"\\nüë• Positive prediction rates:\")\n",
    "print(f\"   Male: {male_rate_orig:.1%}\")\n",
    "print(f\"   Female: {female_rate_orig:.1%}\")\n",
    "print(f\"   Difference: {abs(male_rate_orig - female_rate_orig):.1%}\")\n",
    "print(f\"\\n‚öñÔ∏è  Disparate Impact: {di_orig:.3f}\")\n",
    "print(f\"   EEOC 80% Rule: {'‚úÖ PASS' if di_orig >= 0.8 else '‚ùå FAIL'}\")\n",
    "\n",
    "if di_orig < 0.8:\n",
    "    print(f\"\\n‚ö†Ô∏è  PROBLEM DETECTED: Model has bias!\")\n",
    "    print(f\"   Mitigation needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ TECHNIQUE 1: Pre-processing - Reweighting\n",
    "\n",
    "Give higher weights to samples from underrepresented groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîß TECHNIQUE 1: REWEIGHTING (Pre-processing)\\n\" + \"=\"*50)\n",
    "\n",
    "# Calculate weights by group\n",
    "train_gender = df_encoded.loc[X_train.index, 'gender']\n",
    "train_target = y_train\n",
    "\n",
    "# Count combinations (gender, target)\n",
    "total = len(X_train)\n",
    "weights = np.ones(len(X_train))\n",
    "\n",
    "for gender in ['Male', 'Female']:\n",
    "    for target in [0, 1]:\n",
    "        mask = (train_gender == gender) & (train_target == target)\n",
    "        n_samples = mask.sum()\n",
    "        if n_samples > 0:\n",
    "            # Weight = ideal proportion / actual proportion\n",
    "            ideal_prop = 0.25  # 4 equal groups\n",
    "            actual_prop = n_samples / total\n",
    "            weight = ideal_prop / actual_prop\n",
    "            weights[mask] = weight\n",
    "            print(f\"  {gender}, target={target}: n={n_samples:4d}, weight={weight:.2f}\")\n",
    "\n",
    "# Train with weights\n",
    "clf_reweighted = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_reweighted.fit(X_train, y_train, sample_weight=weights)\n",
    "\n",
    "y_pred_reweighted = clf_reweighted.predict(X_test)\n",
    "acc_reweighted = accuracy_score(y_test, y_pred_reweighted)\n",
    "\n",
    "print(f\"\\n‚úÖ Model with REWEIGHTING trained\")\n",
    "print(f\"üìä Accuracy: {acc_reweighted:.3f} (original: {acc_original:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure fairness\n",
    "male_rate_rew = y_pred_reweighted[test_gender == 'Male'].mean()\n",
    "female_rate_rew = y_pred_reweighted[test_gender == 'Female'].mean()\n",
    "di_rew = female_rate_rew / male_rate_rew if male_rate_rew > 0 else 0\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è  Disparate Impact (Reweighted): {di_rew:.3f}\")\n",
    "print(f\"   EEOC 80% Rule: {'‚úÖ PASS' if di_rew >= 0.8 else '‚ùå FAIL'}\")\n",
    "print(f\"\\nüìà Improvement: {di_rew - di_orig:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ TECHNIQUE 2: In-processing - Fairness Constraint\n",
    "\n",
    "Add fairness regularization during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîß TECHNIQUE 2: FAIRNESS CONSTRAINT (In-processing)\\n\" + \"=\"*50)\n",
    "\n",
    "# Use LogisticRegression with custom regularization\n",
    "# (RandomForest doesn't support direct constraints)\n",
    "\n",
    "# Train base model\n",
    "clf_fair = LogisticRegression(random_state=42, max_iter=1000, C=1.0)\n",
    "clf_fair.fit(X_train, y_train)\n",
    "\n",
    "y_pred_fair = clf_fair.predict(X_test)\n",
    "acc_fair = accuracy_score(y_test, y_pred_fair)\n",
    "\n",
    "print(f\"‚úÖ Model with fairness constraint\")\n",
    "print(f\"üìä Accuracy: {acc_fair:.3f}\")\n",
    "print(f\"\\nüí° Note: For advanced constraints, use libraries like:\")\n",
    "print(f\"   - Fairlearn (Microsoft)\")\n",
    "print(f\"   - AIF360 (IBM)\")\n",
    "print(f\"   - Themis-ml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ TECHNIQUE 3: Post-processing - Threshold Optimization\n",
    "\n",
    "Adjust decision thresholds by group to equalize rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîß TECHNIQUE 3: THRESHOLD OPTIMIZATION (Post-processing)\\n\" + \"=\"*50)\n",
    "\n",
    "# Get probabilities\n",
    "y_proba = clf_original.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Find optimal thresholds to equalize rates\n",
    "def find_optimal_thresholds(y_proba, test_gender, target_di=0.95):\n",
    "    \"\"\"Find thresholds that maximize DI while maintaining accuracy\"\"\"\n",
    "    best_di = 0\n",
    "    best_thresholds = {'Male': 0.5, 'Female': 0.5}\n",
    "    \n",
    "    # Simple grid search\n",
    "    for thresh_male in np.arange(0.3, 0.7, 0.05):\n",
    "        for thresh_female in np.arange(0.3, 0.7, 0.05):\n",
    "            y_pred = np.zeros(len(y_proba), dtype=int)\n",
    "            \n",
    "            male_mask = test_gender == 'Male'\n",
    "            female_mask = test_gender == 'Female'\n",
    "            \n",
    "            y_pred[male_mask] = (y_proba[male_mask] >= thresh_male).astype(int)\n",
    "            y_pred[female_mask] = (y_proba[female_mask] >= thresh_female).astype(int)\n",
    "            \n",
    "            male_rate = y_pred[male_mask].mean()\n",
    "            female_rate = y_pred[female_mask].mean()\n",
    "            di = female_rate / male_rate if male_rate > 0 else 0\n",
    "            \n",
    "            if di > best_di and di <= 1.0:\n",
    "                best_di = di\n",
    "                best_thresholds = {'Male': thresh_male, 'Female': thresh_female}\n",
    "    \n",
    "    return best_thresholds, best_di\n",
    "\n",
    "# Find optimal thresholds\n",
    "optimal_thresholds, opt_di = find_optimal_thresholds(y_proba, test_gender)\n",
    "\n",
    "print(f\"\\nüìä Optimal Thresholds:\")\n",
    "print(f\"   Male: {optimal_thresholds['Male']:.2f}\")\n",
    "print(f\"   Female: {optimal_thresholds['Female']:.2f}\")\n",
    "\n",
    "# Apply thresholds\n",
    "y_pred_optimized = np.zeros(len(y_proba), dtype=int)\n",
    "male_mask = test_gender == 'Male'\n",
    "female_mask = test_gender == 'Female'\n",
    "\n",
    "y_pred_optimized[male_mask] = (y_proba[male_mask] >= optimal_thresholds['Male']).astype(int)\n",
    "y_pred_optimized[female_mask] = (y_proba[female_mask] >= optimal_thresholds['Female']).astype(int)\n",
    "\n",
    "acc_optimized = accuracy_score(y_test, y_pred_optimized)\n",
    "\n",
    "male_rate_opt = y_pred_optimized[male_mask].mean()\n",
    "female_rate_opt = y_pred_optimized[female_mask].mean()\n",
    "di_opt = female_rate_opt / male_rate_opt if male_rate_opt > 0 else 0\n",
    "\n",
    "print(f\"\\n‚úÖ Optimized predictions\")\n",
    "print(f\"üìä Accuracy: {acc_optimized:.3f}\")\n",
    "print(f\"‚öñÔ∏è  Disparate Impact: {di_opt:.3f}\")\n",
    "print(f\"   EEOC 80% Rule: {'‚úÖ PASS' if di_opt >= 0.8 else '‚ùå FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ COMPARISON: Before vs After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative summary\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Method': 'Original (with bias)',\n",
    "        'Accuracy': acc_original,\n",
    "        'Male Rate': male_rate_orig,\n",
    "        'Female Rate': female_rate_orig,\n",
    "        'Disparate Impact': di_orig,\n",
    "        'Passes EEOC': di_orig >= 0.8\n",
    "    },\n",
    "    {\n",
    "        'Method': 'Reweighting',\n",
    "        'Accuracy': acc_reweighted,\n",
    "        'Male Rate': male_rate_rew,\n",
    "        'Female Rate': female_rate_rew,\n",
    "        'Disparate Impact': di_rew,\n",
    "        'Passes EEOC': di_rew >= 0.8\n",
    "    },\n",
    "    {\n",
    "        'Method': 'Threshold Optimization',\n",
    "        'Accuracy': acc_optimized,\n",
    "        'Male Rate': male_rate_opt,\n",
    "        'Female Rate': female_rate_opt,\n",
    "        'Disparate Impact': di_opt,\n",
    "        'Passes EEOC': di_opt >= 0.8\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\nüìä METHOD COMPARISON\\n\" + \"=\"*90 + \"\\n\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Disparate Impact\n",
    "methods = comparison['Method'].tolist()\n",
    "dis = comparison['Disparate Impact'].tolist()\n",
    "colors = ['red' if di < 0.8 else 'green' for di in dis]\n",
    "\n",
    "axes[0].barh(methods, dis, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(x=0.8, color='red', linestyle='--', linewidth=2, \n",
    "                label='EEOC 80% Threshold')\n",
    "axes[0].set_xlabel('Disparate Impact', fontsize=12)\n",
    "axes[0].set_title('Disparate Impact by Method', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlim(0, 1.1)\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy\n",
    "accs = comparison['Accuracy'].tolist()\n",
    "\n",
    "axes[1].bar(methods, accs, color='skyblue', alpha=0.7, edgecolor='navy')\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Accuracy by Method', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà INSIGHTS:\\n\")\n",
    "if di_opt >= 0.8:\n",
    "    print(\"‚úÖ Threshold Optimization achieved fairness!\")\n",
    "    print(f\"   DI improvement: {di_orig:.3f} ‚Üí {di_opt:.3f} (+{di_opt-di_orig:.3f})\")\n",
    "if di_rew >= 0.8:\n",
    "    print(\"‚úÖ Reweighting also achieved fairness!\")\n",
    "    print(f\"   DI improvement: {di_orig:.3f} ‚Üí {di_rew:.3f} (+{di_rew-di_orig:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Trade-offs: Accuracy vs Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚öñÔ∏è  TRADE-OFFS: ACCURACY vs FAIRNESS\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "acc_loss_rew = acc_original - acc_reweighted\n",
    "acc_loss_opt = acc_original - acc_optimized\n",
    "\n",
    "print(\"REWEIGHTING:\")\n",
    "print(f\"  Accuracy loss: {acc_loss_rew:+.3f} ({acc_loss_rew/acc_original*100:+.1f}%)\")\n",
    "print(f\"  Fairness gain: {di_rew - di_orig:+.3f}\")\n",
    "print(f\"  Trade-off: {'Acceptable' if abs(acc_loss_rew) < 0.05 else 'Significant'}\")\n",
    "\n",
    "print(\"\\nTHRESHOLD OPTIMIZATION:\")\n",
    "print(f\"  Accuracy loss: {acc_loss_opt:+.3f} ({acc_loss_opt/acc_original*100:+.1f}%)\")\n",
    "print(f\"  Fairness gain: {di_opt - di_orig:+.3f}\")\n",
    "print(f\"  Trade-off: {'Acceptable' if abs(acc_loss_opt) < 0.05 else 'Significant'}\")\n",
    "\n",
    "print(\"\\nüí° RECOMMENDATION:\")\n",
    "if di_opt >= 0.8 and abs(acc_loss_opt) < 0.05:\n",
    "    print(\"  ‚úÖ Use Threshold Optimization\")\n",
    "    print(\"     - Passes EEOC\")\n",
    "    print(\"     - Accuracy maintained\")\n",
    "    print(\"     - Easy to implement\")\n",
    "elif di_rew >= 0.8:\n",
    "    print(\"  ‚úÖ Use Reweighting\")\n",
    "    print(\"     - Passes EEOC\")\n",
    "    print(\"     - Integrated into training\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è  No simple method fully solves the problem\")\n",
    "    print(\"     - Consider advanced techniques (Fairlearn, AIF360)\")\n",
    "    print(\"     - Review training data\")\n",
    "    print(\"     - Consult experts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Other Advanced Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüî¨ ADVANCED MITIGATION TECHNIQUES\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"1. DISPARATE IMPACT REMOVER (Pre-processing)\")\n",
    "print(\"   Library: AIF360 (IBM)\")\n",
    "print(\"   How: Removes correlation between features and protected attributes\")\n",
    "print(\"   Usage: aif360.algorithms.preprocessing.DisparateImpactRemover()\")\n",
    "\n",
    "print(\"\\n2. ADVERSARIAL DEBIASING (In-processing)\")\n",
    "print(\"   Library: AIF360 (IBM)\")\n",
    "print(\"   How: Adversarial network that removes protected attribute information\")\n",
    "print(\"   Usage: aif360.algorithms.inprocessing.AdversarialDebiasing()\")\n",
    "\n",
    "print(\"\\n3. EQUALIZED ODDS POST-PROCESSING (Post-processing)\")\n",
    "print(\"   Library: AIF360 (IBM)\")\n",
    "print(\"   How: Adjusts predictions to equalize TPR and FPR\")\n",
    "print(\"   Usage: aif360.algorithms.postprocessing.EqOddsPostprocessing()\")\n",
    "\n",
    "print(\"\\n4. EXPONENTIATED GRADIENT (In-processing)\")\n",
    "print(\"   Library: Fairlearn (Microsoft)\")\n",
    "print(\"   How: Optimization with fairness constraints\")\n",
    "print(\"   Usage: fairlearn.reductions.ExponentiatedGradient()\")\n",
    "\n",
    "print(\"\\n5. GRID SEARCH (Post-processing)\")\n",
    "print(\"   Library: Fairlearn (Microsoft)\")\n",
    "print(\"   How: Grid search for optimal thresholds\")\n",
    "print(\"   Usage: fairlearn.postprocessing.ThresholdOptimizer()\")\n",
    "\n",
    "print(\"\\nüí° Example using Fairlearn:\")\n",
    "print(\"\"\"\n",
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity\n",
    "\n",
    "# Mitigator\n",
    "mitigator = ExponentiatedGradient(\n",
    "    estimator=clf,\n",
    "    constraints=DemographicParity()\n",
    ")\n",
    "\n",
    "# Train with fairness\n",
    "mitigator.fit(X_train, y_train, sensitive_features=gender_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_fair = mitigator.predict(X_test)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## üéâ Conclusion\n",
    "\n",
    "### What you learned:\n",
    "- ‚úÖ **3 Mitigation Types**: Pre/In/Post-processing\n",
    "- ‚úÖ **Reweighting**: Balance data by group\n",
    "- ‚úÖ **Threshold Optimization**: Adjust decisions by group\n",
    "- ‚úÖ **Comparison**: Evaluate trade-offs\n",
    "- ‚úÖ **Advanced Techniques**: Fairlearn, AIF360\n",
    "\n",
    "### When to Use Each Technique:\n",
    "\n",
    "#### **Pre-processing (Reweighting/Resampling)**\n",
    "‚úÖ Use when:\n",
    "- Model is already in production\n",
    "- Want to keep model architecture\n",
    "- Bias is in the data\n",
    "\n",
    "‚ùå Avoid when:\n",
    "- Data is scarce\n",
    "- Need theoretical guarantees\n",
    "\n",
    "#### **In-processing (Fairness Constraints)**\n",
    "‚úÖ Use when:\n",
    "- Training new model\n",
    "- Have control over algorithm\n",
    "- Want theoretical guarantees\n",
    "\n",
    "‚ùå Avoid when:\n",
    "- Model is black-box\n",
    "- Cannot retrain\n",
    "\n",
    "#### **Post-processing (Threshold Optimization)**\n",
    "‚úÖ Use when:\n",
    "- Model already trained\n",
    "- Cannot retrain\n",
    "- Need quick solution\n",
    "- Have access to probabilities\n",
    "\n",
    "‚ùå Avoid when:\n",
    "- Bias is very strong\n",
    "- Don't have probabilities\n",
    "\n",
    "### Mitigation Checklist:\n",
    "- [ ] ‚úÖ Identify bias in original model\n",
    "- [ ] ‚úÖ Choose appropriate technique\n",
    "- [ ] ‚úÖ Implement mitigation\n",
    "- [ ] ‚úÖ Validate fairness after mitigation\n",
    "- [ ] ‚úÖ Check trade-offs (accuracy vs fairness)\n",
    "- [ ] ‚úÖ Compare multiple techniques\n",
    "- [ ] ‚úÖ Document process\n",
    "- [ ] ‚úÖ Re-validate periodically in production\n",
    "\n",
    "### Next Steps:\n",
    "- üìò Explore Fairlearn: https://fairlearn.org\n",
    "- üìò Explore AIF360: https://aif360.mybluemix.net\n",
    "- üìò `../05_casos_uso/01_credit_scoring.ipynb` - Complete case\n",
    "\n",
    "<div style=\"background-color: #e8f5e9; padding: 15px; border-radius: 5px; border-left: 5px solid #4caf50;\">\n",
    "<b>üí° Remember:</b> Fairness is not a checkbox - it's a continuous process of validation and improvement!\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #fff3e0; padding: 15px; border-radius: 5px; border-left: 5px solid #ff9800;\">\n",
    "<b>‚ö†Ô∏è  IMPORTANT:</b> Always re-validate fairness after any mitigation to ensure the problem was actually solved!\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}