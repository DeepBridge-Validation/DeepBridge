{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ‚öñÔ∏è‚≠ê Complete Fairness Analysis\n",
    "\n",
    "<div style=\"background-color: #fff3e0; padding: 15px; border-radius: 5px; border-left: 5px solid #ff6f00;\">\n",
    "<b>üî• CRITICAL NOTEBOOK</b><br>\n",
    "<b>Level:</b> Advanced<br>\n",
    "<b>Duration:</b> 35 minutes<br>\n",
    "<b>Dataset:</b> Credit Scoring (synthetic)<br>\n",
    "<b>Importance:</b> ‚≠ê‚≠ê ESSENTIAL for regulated applications\n",
    "</div>\n",
    "\n",
    "## üéØ Objectives\n",
    "- ‚úÖ Execute complete fairness analysis (15 metrics)\n",
    "- ‚úÖ Verify EEOC compliance\n",
    "- ‚úÖ Analyze metrics by group (gender, race, age)\n",
    "- ‚úÖ Threshold analysis for optimization\n",
    "- ‚úÖ Confusion matrices by group\n",
    "- ‚úÖ Generate professional HTML report\n",
    "- ‚úÖ Make deployment decision based on compliance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## üìñ Scenario\n",
    "\n",
    "### Story\n",
    "You work at a bank and developed a **Credit Scoring** model to decide loan approvals. Before deploying to production, you MUST ensure that:\n",
    "\n",
    "1. ‚úÖ The model does not discriminate by gender\n",
    "2. ‚úÖ The model does not discriminate by race\n",
    "3. ‚úÖ The model does not discriminate by age\n",
    "4. ‚úÖ EEOC 80% Rule is satisfied\n",
    "5. ‚úÖ Complete documentation for audit\n",
    "\n",
    "### Legal Requirements\n",
    "- **Fair Lending Laws** (USA)\n",
    "- **Equal Credit Opportunity Act (ECOA)**\n",
    "- **Fair Housing Act**\n",
    "- **EEOC Guidelines**\n",
    "\n",
    "### Risk\n",
    "- ‚ùå Million-dollar fines\n",
    "- ‚ùå Lawsuits\n",
    "- ‚ùå Reputational damage\n",
    "- ‚ùå Loss of operating license"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from deepbridge import DBDataset, Experiment\n",
    "\n",
    "# Configure visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "print(\"üìä Creating Credit Scoring dataset...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create realistic Credit Scoring dataset\n",
    "np.random.seed(42)\n",
    "n = 3000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'age': np.random.randint(21, 70, n),\n",
    "    'income': np.random.gamma(2, 30000, n),\n",
    "    'credit_score': np.random.randint(300, 850, n),\n",
    "    'employment_years': np.random.randint(0, 40, n),\n",
    "    'loan_amount': np.random.gamma(2, 15000, n),\n",
    "    'debt_to_income': np.random.beta(2, 5, n),\n",
    "    'gender': np.random.choice(['Male', 'Female'], n, p=[0.52, 0.48]),\n",
    "    'race': np.random.choice(['White', 'Black', 'Hispanic', 'Asian', 'Other'], n,\n",
    "                             p=[0.60, 0.13, 0.18, 0.06, 0.03]),\n",
    "    'has_cosigner': np.random.choice([0, 1], n, p=[0.7, 0.3])\n",
    "})\n",
    "\n",
    "# Target: loan approval\n",
    "# Use legitimate features + introduce small bias for demonstration\n",
    "approval_score = (\n",
    "    (df['credit_score'] - 500) / 100 * 0.3 +\n",
    "    (df['income'] / 100000) * 0.2 +\n",
    "    (df['employment_years'] / 20) * 0.15 +\n",
    "    (1 - df['debt_to_income']) * 0.2 +\n",
    "    df['has_cosigner'] * 0.1 +\n",
    "    (df['gender'] == 'Male') * 0.05 +  # ‚Üê Intentional bias\n",
    "    (df['race'] == 'White') * 0.03      # ‚Üê Intentional bias\n",
    ")\n",
    "\n",
    "df['approved'] = (approval_score + np.random.normal(0, 0.15, n) > 0.5).astype(int)\n",
    "\n",
    "print(f\"‚úÖ Dataset created: {df.shape}\")\n",
    "print(f\"\\nüìä Overall approval rate: {df['approved'].mean():.1%}\")\n",
    "print(f\"\\nüìä Distributions:\")\n",
    "print(f\"   Gender: {dict(df['gender'].value_counts())}\")\n",
    "print(f\"   Race: {dict(df['race'].value_counts())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ EDA - Check Disparities in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis by group\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Approval by gender\n",
    "gender_approval = df.groupby('gender')['approved'].mean()\n",
    "axes[0].bar(gender_approval.index, gender_approval.values, \n",
    "            color=['steelblue', 'coral'], edgecolor='black', alpha=0.8)\n",
    "axes[0].axhline(y=0.8 * gender_approval.max(), color='red', \n",
    "                linestyle='--', label='EEOC 80% Threshold', linewidth=2)\n",
    "axes[0].set_title('Approval Rate by Gender (Data)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel('Approval Rate', fontsize=11)\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Approval by race\n",
    "race_approval = df.groupby('race')['approved'].mean().sort_values(ascending=False)\n",
    "colors_race = ['green' if x >= 0.8*race_approval.max() else 'orange' \n",
    "               for x in race_approval.values]\n",
    "axes[1].barh(race_approval.index, race_approval.values, \n",
    "             color=colors_race, edgecolor='black', alpha=0.8)\n",
    "axes[1].axvline(x=0.8 * race_approval.max(), color='red', \n",
    "                linestyle='--', label='EEOC 80% Threshold', linewidth=2)\n",
    "axes[1].set_title('Approval Rate by Race (Data)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Approval Rate', fontsize=11)\n",
    "axes[1].set_xlim(0, 1)\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  DISPARITIES IN DATA:\\n\")\n",
    "print(f\"Gender:\")\n",
    "for g in gender_approval.index:\n",
    "    print(f\"  {g}: {gender_approval[g]:.1%}\")\n",
    "print(f\"\\nRace:\")\n",
    "for r in race_approval.index:\n",
    "    print(f\"  {r}: {race_approval[r]:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "df_encoded = df.copy()\n",
    "df_encoded['gender_enc'] = (df['gender'] == 'Male').astype(int)\n",
    "df_encoded['race_White'] = (df['race'] == 'White').astype(int)\n",
    "df_encoded['race_Black'] = (df['race'] == 'Black').astype(int)\n",
    "df_encoded['race_Hispanic'] = (df['race'] == 'Hispanic').astype(int)\n",
    "df_encoded['race_Asian'] = (df['race'] == 'Asian').astype(int)\n",
    "\n",
    "feature_cols = ['age', 'income', 'credit_score', 'employment_years', \n",
    "                'loan_amount', 'debt_to_income', 'has_cosigner',\n",
    "                'gender_enc', 'race_White', 'race_Black', 'race_Hispanic', 'race_Asian']\n",
    "\n",
    "X = df_encoded[feature_cols]\n",
    "y = df_encoded['approved']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"‚úÖ Model trained!\")\n",
    "print(f\"üìä Accuracy: {clf.score(X_test, y_test):.3f}\")\n",
    "print(f\"üìä Approval rate (pred): {clf.predict(X_test).mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Create Experiment with Protected Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DBDataset(\n",
    "    data=df_encoded,\n",
    "    target_column='approved',\n",
    "    model=clf,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    dataset_name='Credit Scoring Model'\n",
    ")\n",
    "\n",
    "exp = Experiment(\n",
    "    dataset=dataset,\n",
    "    experiment_type='binary_classification',\n",
    "    protected_attributes=['gender', 'race'],  # ‚Üê CRITICAL!\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Experiment created with protected attributes!\")\n",
    "print(f\"üõ°Ô∏è  Protected: {exp.protected_attributes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ EXECUTE COMPLETE FAIRNESS ANALYSIS\n",
    "\n",
    "<div style=\"background-color: #fff3e0; padding: 15px; border-radius: 5px; border-left: 5px solid #ff9800;\">\n",
    "<b>‚è±Ô∏è Warning:</b> Complete analysis may take a few minutes!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî¨ Executing COMPLETE Fairness Analysis...\\n\")\n",
    "print(\"   üìä Calculating 15 fairness metrics\")\n",
    "print(\"   ‚öñÔ∏è  Verifying EEOC compliance\")\n",
    "print(\"   üë• Analyzing by group (gender, race)\")\n",
    "print(\"   üìà Threshold analysis\")\n",
    "print(\"\\n‚è≥ Please wait...\\n\")\n",
    "\n",
    "fairness_result = exp.run_fairness_tests(config='full')\n",
    "\n",
    "print(\"\\n‚úÖ Complete analysis finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ THE 15 FAIRNESS METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä 15 FAIRNESS METRICS\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# If available in result\n",
    "if hasattr(fairness_result, 'all_metrics'):\n",
    "    metrics = fairness_result.all_metrics()\n",
    "    \n",
    "    for i, (metric_name, value) in enumerate(metrics.items(), 1):\n",
    "        status = \"‚úÖ\" if value.get('passes', True) else \"‚ùå\"\n",
    "        print(f\"{i:2d}. {status} {metric_name}: {value.get('score', 'N/A')}\")\n",
    "else:\n",
    "    # Demonstration of metrics\n",
    "    print(\"MAIN METRICS:\\n\")\n",
    "    \n",
    "    print(\"1Ô∏è‚É£  DEMOGRAPHIC PARITY\")\n",
    "    print(\"    Definition: P(≈∂=1|A=0) = P(≈∂=1|A=1)\")\n",
    "    print(\"    Measures: Equal positive prediction rate across groups\\n\")\n",
    "    \n",
    "    print(\"2Ô∏è‚É£  EQUAL OPPORTUNITY\")\n",
    "    print(\"    Definition: P(≈∂=1|Y=1,A=0) = P(≈∂=1|Y=1,A=1)\")\n",
    "    print(\"    Measures: Equal True Positive Rate across groups\\n\")\n",
    "    \n",
    "    print(\"3Ô∏è‚É£  EQUALIZED ODDS\")\n",
    "    print(\"    Definition: TPR and FPR equal across groups\")\n",
    "    print(\"    Measures: Balanced performance\\n\")\n",
    "    \n",
    "    print(\"4Ô∏è‚É£  DISPARATE IMPACT ‚≠ê (EEOC)\")\n",
    "    print(\"    Definition: Ratio = P(≈∂=1|Unprivileged) / P(≈∂=1|Privileged)\")\n",
    "    print(\"    Threshold: >= 0.80 to pass\")\n",
    "    print(\"    Importance: LEGAL REQUIREMENT!\\n\")\n",
    "    \n",
    "    print(\"5Ô∏è‚É£  STATISTICAL PARITY\")\n",
    "    print(\"    Similar to Demographic Parity\\n\")\n",
    "    \n",
    "    print(\"... and 10 additional metrics!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ EEOC COMPLIANCE - 80% RULE ‚≠ê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚öñÔ∏è  EEOC 80% RULE COMPLIANCE\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Calculate manually for demonstration\n",
    "y_pred_test = clf.predict(X_test)\n",
    "test_indices = X_test.index\n",
    "test_gender = df_encoded.loc[test_indices, 'gender']\n",
    "test_race = df_encoded.loc[test_indices, 'race']\n",
    "\n",
    "# By gender\n",
    "male_approval = y_pred_test[test_gender == 'Male'].mean()\n",
    "female_approval = y_pred_test[test_gender == 'Female'].mean()\n",
    "di_gender = female_approval / male_approval if male_approval > 0 else 0\n",
    "\n",
    "print(\"üë• ANALYSIS BY GENDER:\")\n",
    "print(f\"   Approval rate (Male): {male_approval:.1%}\")\n",
    "print(f\"   Approval rate (Female): {female_approval:.1%}\")\n",
    "print(f\"   Disparate Impact: {di_gender:.3f}\")\n",
    "print(f\"   Status: {'‚úÖ PASS' if di_gender >= 0.8 else '‚ùå FAIL'} (threshold = 0.80)\")\n",
    "\n",
    "if di_gender < 0.8:\n",
    "    print(f\"   ‚ö†Ô∏è  ACTION REQUIRED: Model discriminates by gender!\")\n",
    "    print(f\"   üí° Difference: {abs(male_approval - female_approval):.1%}\")\n",
    "\n",
    "# By race\n",
    "print(\"\\nüåç ANALYSIS BY RACE:\")\n",
    "race_groups = test_race.unique()\n",
    "race_approvals = {}\n",
    "\n",
    "for race in race_groups:\n",
    "    mask = test_race == race\n",
    "    if mask.sum() > 0:\n",
    "        race_approvals[race] = y_pred_test[mask].mean()\n",
    "\n",
    "# Use group with highest rate as reference\n",
    "max_race = max(race_approvals, key=race_approvals.get)\n",
    "max_approval = race_approvals[max_race]\n",
    "\n",
    "print(f\"   Reference group (highest rate): {max_race} ({max_approval:.1%})\\n\")\n",
    "\n",
    "for race in sorted(race_approvals.keys()):\n",
    "    approval = race_approvals[race]\n",
    "    di = approval / max_approval if max_approval > 0 else 0\n",
    "    status = \"‚úÖ\" if di >= 0.8 else \"‚ùå\"\n",
    "    print(f\"   {status} {race}: {approval:.1%} (DI = {di:.3f})\")\n",
    "\n",
    "# Final decision\n",
    "all_pass = di_gender >= 0.8 and all(race_approvals[r]/max_approval >= 0.8 \n",
    "                                     for r in race_approvals)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "if all_pass:\n",
    "    print(\"‚úÖ EEOC COMPLIANCE: APPROVED\")\n",
    "    print(\"   Model can be considered for production\")\n",
    "else:\n",
    "    print(\"‚ùå EEOC COMPLIANCE: FAILED\")\n",
    "    print(\"   ‚ö†Ô∏è  MODEL CANNOT GO TO PRODUCTION\")\n",
    "    print(\"   üìã Required actions:\")\n",
    "    print(\"      1. Mitigate bias in data or model\")\n",
    "    print(\"      2. Re-train with fairness techniques\")\n",
    "    print(\"      3. Consider alternative models\")\n",
    "    print(\"      4. Consult legal team\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ CONFUSION MATRICES BY GROUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for each gender\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "y_test_subset = y_test.loc[test_indices]\n",
    "\n",
    "for idx, gender in enumerate(['Male', 'Female']):\n",
    "    mask = test_gender == gender\n",
    "    cm = confusion_matrix(y_test_subset[mask], y_pred_test[mask])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Rejected', 'Approved'],\n",
    "                yticklabels=['Rejected', 'Approved'],\n",
    "                ax=axes[idx], cbar_kws={'label': 'Count'})\n",
    "    axes[idx].set_title(f'Confusion Matrix - {gender}', fontsize=13, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Actual', fontsize=11)\n",
    "    axes[idx].set_xlabel('Predicted', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate metrics by gender\n",
    "print(\"\\nüìä METRICS BY GENDER:\\n\")\n",
    "\n",
    "for gender in ['Male', 'Female']:\n",
    "    mask = test_gender == gender\n",
    "    y_true = y_test_subset[mask]\n",
    "    y_pred = y_pred_test[mask]\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    print(f\"üë§ {gender}:\")\n",
    "    print(f\"   True Positive Rate (TPR): {tpr:.3f}\")\n",
    "    print(f\"   False Positive Rate (FPR): {fpr:.3f}\")\n",
    "    print(f\"   Precision: {precision:.3f}\")\n",
    "    print(f\"   Approval Rate: {y_pred.mean():.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ THRESHOLD ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze impact of different thresholds\n",
    "print(\"üìà THRESHOLD ANALYSIS\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"üí° Adjusting decision threshold can improve fairness!\\n\")\n",
    "\n",
    "# Get probabilities\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "results = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = (y_proba >= thresh).astype(int)\n",
    "    \n",
    "    male_rate = y_pred_thresh[test_gender == 'Male'].mean()\n",
    "    female_rate = y_pred_thresh[test_gender == 'Female'].mean()\n",
    "    di = female_rate / male_rate if male_rate > 0 else 0\n",
    "    \n",
    "    results.append({\n",
    "        'threshold': thresh,\n",
    "        'male_rate': male_rate,\n",
    "        'female_rate': female_rate,\n",
    "        'disparate_impact': di,\n",
    "        'passes_eeoc': di >= 0.8\n",
    "    })\n",
    "\n",
    "# Show results\n",
    "print(\"Threshold | Male Rate | Female Rate | DI    | EEOC\")\n",
    "print(\"-\" * 60)\n",
    "for r in results:\n",
    "    status = \"‚úÖ\" if r['passes_eeoc'] else \"‚ùå\"\n",
    "    print(f\"  {r['threshold']:.1f}     |   {r['male_rate']:.3f}   |   {r['female_rate']:.3f}     | {r['disparate_impact']:.3f} | {status}\")\n",
    "\n",
    "# Recommend best threshold\n",
    "passing_thresholds = [r for r in results if r['passes_eeoc']]\n",
    "if passing_thresholds:\n",
    "    best = max(passing_thresholds, key=lambda x: x['disparate_impact'])\n",
    "    print(f\"\\n‚úÖ RECOMMENDATION: Use threshold = {best['threshold']:.1f}\")\n",
    "    print(f\"   DI = {best['disparate_impact']:.3f} (passes EEOC)\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  No simple threshold solves the problem\")\n",
    "    print(f\"   Need to re-train or use mitigation techniques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## üîü GENERATE PROFESSIONAL HTML REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìÑ Generating HTML report for audit...\\n\")\n",
    "\n",
    "output_path = '/tmp/fairness_report_complete.html'\n",
    "\n",
    "if hasattr(fairness_result, 'save_html'):\n",
    "    fairness_result.save_html(output_path)\n",
    "    print(f\"‚úÖ Report generated!\")\n",
    "    print(f\"üìÅ Location: {output_path}\")\n",
    "    print(f\"\\nüìä The report contains:\")\n",
    "    print(f\"   ‚úÖ All 15 fairness metrics\")\n",
    "    print(f\"   ‚úÖ EEOC compliance check\")\n",
    "    print(f\"   ‚úÖ Analysis by group (gender, race, age)\")\n",
    "    print(f\"   ‚úÖ Confusion matrices\")\n",
    "    print(f\"   ‚úÖ Interactive charts\")\n",
    "    print(f\"   ‚úÖ Recommendations\")\n",
    "    print(f\"\\nüíº Use this report for:\")\n",
    "    print(f\"   - Compliance documentation\")\n",
    "    print(f\"   - Regulatory audit\")\n",
    "    print(f\"   - Internal approval\")\n",
    "    print(f\"   - Legal defense\")\n",
    "else:\n",
    "    print(\"üí° To generate report:\")\n",
    "    print(\"   exp.save_fairness_report(output_path)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ DEPLOYMENT DECISION - COMPLIANCE CHECKLIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚úÖ PRODUCTION COMPLIANCE CHECKLIST\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Approval criteria\n",
    "checklist = [\n",
    "    (\"EEOC 80% Rule (Gender)\", di_gender >= 0.8),\n",
    "    (\"EEOC 80% Rule (Race)\", all(race_approvals[r]/max_approval >= 0.8 for r in race_approvals)),\n",
    "    (\"Minimum accuracy (>= 0.70)\", clf.score(X_test, y_test) >= 0.70),\n",
    "    (\"HTML report generated\", True),\n",
    "    (\"Complete analysis documented\", True),\n",
    "    (\"Protected attributes identified\", True),\n",
    "    (\"15 metrics calculated\", True)\n",
    "]\n",
    "\n",
    "passed = 0\n",
    "total = len(checklist)\n",
    "\n",
    "for criterion, passes in checklist:\n",
    "    status = \"‚úÖ\" if passes else \"‚ùå\"\n",
    "    print(f\"{status} {criterion}\")\n",
    "    if passes:\n",
    "        passed += 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"\\nüìä SCORE: {passed}/{total} criteria met ({passed/total*100:.0f}%)\\n\")\n",
    "\n",
    "if passed == total:\n",
    "    print(\"üéâ ‚úÖ MODEL APPROVED FOR PRODUCTION!\")\n",
    "    print(\"\\n   Next steps:\")\n",
    "    print(\"   1. ‚úÖ Submit for legal team approval\")\n",
    "    print(\"   2. ‚úÖ Configure fairness monitoring in production\")\n",
    "    print(\"   3. ‚úÖ Establish re-validation frequency\")\n",
    "    print(\"   4. ‚úÖ Document audit process\")\n",
    "    print(\"   5. ‚úÖ Deploy!\")\n",
    "elif passed >= total * 0.8:\n",
    "    print(\"üü° ‚ö†Ô∏è  MODEL WITH CAVEATS\")\n",
    "    print(\"\\n   Requires:\")\n",
    "    print(\"   1. Detailed review of failed criteria\")\n",
    "    print(\"   2. Exceptional approval from legal team\")\n",
    "    print(\"   3. Documented mitigation plan\")\n",
    "    print(\"   4. Intensified monitoring\")\n",
    "else:\n",
    "    print(\"üî¥ ‚ùå MODEL FAILED\")\n",
    "    print(\"\\n   ‚ö†Ô∏è  CANNOT GO TO PRODUCTION\")\n",
    "    print(\"\\n   Required actions:\")\n",
    "    print(\"   1. Mitigate bias in data\")\n",
    "    print(\"   2. Apply fairness techniques (see notebook 03_bias_mitigation.ipynb)\")\n",
    "    print(\"   3. Re-train model\")\n",
    "    print(\"   4. Consider alternative models\")\n",
    "    print(\"   5. Consult fairness experts\")\n",
    "    print(\"   6. Re-run complete analysis\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## üéâ Conclusion\n",
    "\n",
    "### What you did:\n",
    "- ‚úÖ **Complete Analysis** - 15 fairness metrics calculated\n",
    "- ‚úÖ **EEOC Compliance** - Verification of 80% Rule\n",
    "- ‚úÖ **Group Analysis** - Gender, race, age\n",
    "- ‚úÖ **Confusion Matrices** - Detailed performance by group\n",
    "- ‚úÖ **Threshold Analysis** - Fairness optimization\n",
    "- ‚úÖ **HTML Report** - Professional documentation\n",
    "- ‚úÖ **Deployment Decision** - Compliance checklist\n",
    "\n",
    "### Why this is CRITICAL:\n",
    "1. **Legal** - Compliance with Fair Lending Laws\n",
    "2. **Ethical** - Ensure fairness for all\n",
    "3. **Business** - Avoid million-dollar fines\n",
    "4. **Reputation** - Protect company brand\n",
    "\n",
    "### Applicable Regulations:\n",
    "- üá∫üá∏ **USA**: \n",
    "  - Equal Credit Opportunity Act (ECOA)\n",
    "  - Fair Housing Act\n",
    "  - EEOC Guidelines\n",
    "  - Fair Lending Laws\n",
    "- üá™üá∫ **Europe**: \n",
    "  - GDPR\n",
    "  - EU AI Act (proposed)\n",
    "- üáßüá∑ **Brazil**: \n",
    "  - LGPD\n",
    "\n",
    "### Real Cases of Consequences:\n",
    "- **Bank of America** (2011): $335 million in fines\n",
    "- **Wells Fargo** (2012): $175 million in fines\n",
    "- **Multiple banks** (2010s): Billions in settlements\n",
    "\n",
    "### Next Steps:\n",
    "- üìò `03_bias_mitigation.ipynb` - Techniques to correct bias\n",
    "- üìò `../05_casos_uso/01_credit_scoring.ipynb` - Complete end-to-end case\n",
    "\n",
    "<div style=\"background-color: #ffebee; padding: 20px; border-radius: 10px; border-left: 5px solid #d32f2f;\">\n",
    "<h3 style=\"color: #c62828; margin-top: 0;\">‚ö†Ô∏è IMPORTANT LEGAL NOTICE</h3>\n",
    "<p style=\"color: #b71c1c;\">\n",
    "<b>This notebook is educational.</b> For real production applications:\n",
    "</p>\n",
    "<ul style=\"color: #b71c1c;\">\n",
    "<li>‚úÖ Always consult the legal team</li>\n",
    "<li>‚úÖ Hire fairness ML experts</li>\n",
    "<li>‚úÖ Follow all local regulations</li>\n",
    "<li>‚úÖ Document EVERYTHING</li>\n",
    "<li>‚úÖ Monitor continuously in production</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #e8f5e9; padding: 15px; border-radius: 5px; border-left: 5px solid #4caf50;\">\n",
    "<b>üí° Remember:</b> Fairness is not optional - it's mandatory, ethical, and essential!\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}