# ‚öñÔ∏è Fairness e Compliance

Aprenda a detectar, quantificar e mitigar bias em modelos de Machine Learning.

<div style="background-color: #fff3e0; padding: 15px; border-radius: 5px;">
<b>‚ö†Ô∏è CR√çTICO:</b> Para aplica√ß√µes que afetam pessoas (credit, hiring, healthcare, justice), an√°lise de fairness √© OBRIGAT√ìRIA por lei!
</div>

---

## üìì Notebooks desta Pasta

| # | Notebook | Tempo | Descri√ß√£o | Prioridade |
|---|----------|-------|-----------|------------|
| 1 | `01_introducao_fairness.ipynb` | 20 min | Conceitos, atributos protegidos, m√©tricas | üî¥ ALTA |
| 2 | `02_analise_completa_fairness.ipynb` ‚≠ê‚≠ê | 35 min | **CR√çTICO** - 15 m√©tricas + EEOC compliance | üî¥ ALTA |
| 3 | `03_mitigacao_bias.ipynb` | 25 min | T√©cnicas de corre√ß√£o de bias | üü° M√âDIA |

**Tempo Total**: ~80 minutos

---

## üéØ Ordem Recomendada

### Para Todos (OBRIGAT√ìRIO se trabalha com modelos em produ√ß√£o)

1. **COMECE AQUI:** `01_introducao_fairness.ipynb`
   - Por que fairness importa
   - Casos reais de bias (Amazon, COMPAS, Apple Card)
   - Atributos protegidos (gender, race, age)
   - 15 m√©tricas de fairness
   - Primeiro teste de fairness

2. **CR√çTICO:** `02_analise_completa_fairness.ipynb` ‚≠ê‚≠ê
   - **NOTEBOOK MAIS IMPORTANTE DA PASTA**
   - An√°lise completa com 15 m√©tricas
   - EEOC 80% Rule compliance
   - An√°lise por grupo (gender, race, age)
   - Confusion matrices por grupo
   - Threshold analysis
   - Relat√≥rio HTML profissional
   - Decis√£o de deploy com checklist legal

3. **Se Detectou Bias:** `03_mitigacao_bias.ipynb`
   - T√©cnicas Pre/In/Post-processing
   - Reweighting
   - Threshold optimization
   - Compara√ß√£o Before vs After
   - Trade-offs (accuracy vs fairness)

---

## üìñ O que Voc√™ Vai Aprender

### Notebook 1: Introdu√ß√£o a Fairness
- ‚úÖ **Casos Reais de Bias**
  - Amazon (recrutamento)
  - COMPAS (justi√ßa criminal)
  - Apple Card (cr√©dito)
  - Reconhecimento facial
- ‚úÖ **Atributos Protegidos**
  - Gender, race, age, religion, etc.
  - Regula√ß√µes (EEOC, GDPR, LGPD)
- ‚úÖ **15 M√©tricas de Fairness**
  - Demographic Parity
  - Equal Opportunity
  - Equalized Odds
  - Disparate Impact (‚≠ê EEOC)
  - ... e mais 11
- ‚úÖ **Auto-detec√ß√£o** de atributos sens√≠veis

### Notebook 2: An√°lise Completa ‚≠ê‚≠ê (CR√çTICO)
- ‚úÖ **Cen√°rio Real**: Credit Scoring
- ‚úÖ **15 M√©tricas Calculadas**
- ‚úÖ **EEOC 80% Rule** - Compliance legal
- ‚úÖ **An√°lise por Grupo**
  - Gender (Male vs Female)
  - Race (White, Black, Hispanic, Asian, Other)
  - Age groups
- ‚úÖ **Confusion Matrices por Grupo**
- ‚úÖ **Threshold Analysis** - Otimizar fairness
- ‚úÖ **Relat√≥rio HTML** para auditoria
- ‚úÖ **Checklist de Deploy** - Decis√£o legal

### Notebook 3: Mitiga√ß√£o de Bias
- ‚úÖ **3 Tipos de Mitiga√ß√£o**
  - Pre-processing (dados)
  - In-processing (algoritmo)
  - Post-processing (predi√ß√µes)
- ‚úÖ **T√©cnicas Pr√°ticas**
  - Reweighting
  - Threshold optimization
  - Fairness constraints
- ‚úÖ **Compara√ß√£o Before vs After**
- ‚úÖ **Trade-offs** (accuracy vs fairness)
- ‚úÖ **Bibliotecas Avan√ßadas** (Fairlearn, AIF360)

---

## üéì Pr√©-requisitos

### Conhecimento
- Completar `01_introducao/` (recomendado)
- Entender m√©tricas de classifica√ß√£o (TPR, FPR, Precision, Recall)
- **Importante**: No√ß√µes b√°sicas de √©tica em ML

### Instala√ß√£o
```bash
pip install deepbridge jupyter pandas numpy matplotlib seaborn scikit-learn

# Opcional (para t√©cnicas avan√ßadas)
pip install fairlearn aif360
```

---

## üöÄ Como Executar

```bash
# 1. Navegar at√© a pasta
cd /home/guhaase/projetos/DeepBridge/examples/notebooks/04_fairness

# 2. Iniciar Jupyter
jupyter notebook

# 3. Abrir o primeiro notebook
# 01_introducao_fairness.ipynb
```

---

## üí° Principais Conceitos

### EEOC 80% Rule (Four-Fifths Rule) ‚≠ê

```python
# Regra fundamental de compliance
Disparate Impact = P(≈∂=1 | Unprivileged) / P(≈∂=1 | Privileged)

# ‚úÖ PASSA: DI >= 0.80
# ‚ùå FALHA: DI < 0.80

# Exemplo
male_approval_rate = 0.50  # 50%
female_approval_rate = 0.35  # 35%
di = female_approval_rate / male_approval_rate  # 0.70
# Resultado: ‚ùå FALHA (< 0.80)
```

### Executar An√°lise de Fairness

```python
from deepbridge import DBDataset, Experiment

# Criar Experiment COM protected_attributes
exp = Experiment(
    dataset=dataset,
    experiment_type='binary_classification',
    protected_attributes=['gender', 'race'],  # ‚Üê CR√çTICO!
    random_state=42
)

# Executar an√°lise completa
fairness_result = exp.run_fairness_tests(config='full')

# Verificar EEOC compliance
passes_eeoc = fairness_result.passes_eeoc_compliance()
print(f"EEOC Compliance: {'‚úÖ PASSA' if passes_eeoc else '‚ùå FALHA'}")

# Gerar relat√≥rio HTML
exp.save_fairness_report('fairness_report.html')
```

---

## üìä As 15 M√©tricas de Fairness

| # | M√©trica | O que Mede | Import√¢ncia |
|---|---------|------------|-------------|
| 1 | Demographic Parity Difference | Taxa de predi√ß√µes positivas igual | Alta |
| 2 | Demographic Parity Ratio | Ratio de taxas de predi√ß√µes | Alta |
| 3 | Equal Opportunity Difference | TPR igual entre grupos | Alta |
| 4 | Equalized Odds Difference | TPR e FPR iguais | Alta |
| 5 | **Disparate Impact** ‚≠ê | **EEOC 80% Rule** | **CR√çTICA** |
| 6 | Statistical Parity Difference | Similar a Demographic Parity | M√©dia |
| 7 | Average Odds Difference | M√©dia de TPR e FPR | M√©dia |
| 8 | Theil Index | Desigualdade geral | M√©dia |
| 9 | False Positive Rate Difference | FPR diferen√ßa | Alta |
| 10 | False Negative Rate Difference | FNR diferen√ßa | Alta |
| 11 | Precision Difference | Precis√£o diferen√ßa | M√©dia |
| 12 | Recall Difference | Recall diferen√ßa | M√©dia |
| 13 | F1 Score Difference | F1 diferen√ßa | M√©dia |
| 14 | Accuracy Difference | Accuracy diferen√ßa | Baixa |
| 15 | Selection Rate | Taxa de sele√ß√£o | M√©dia |

---

## ‚öñÔ∏è Regula√ß√µes e Compliance

### üá∫üá∏ Estados Unidos
- **EEOC** (Equal Employment Opportunity Commission)
- **Fair Lending Laws**
- **Equal Credit Opportunity Act (ECOA)**
- **Fair Housing Act**
- **Civil Rights Act of 1964**

**Penalidades**: Multas de milh√µes a bilh√µes de d√≥lares

### üá™üá∫ Uni√£o Europeia
- **GDPR** (General Data Protection Regulation)
- **EU AI Act** (proposto)
- **Right to Explanation**

**Penalidades**: At√© 4% da receita global anual

### üáßüá∑ Brasil
- **LGPD** (Lei Geral de Prote√ß√£o de Dados)
- Artigos sobre decis√µes automatizadas

**Penalidades**: At√© R$ 50 milh√µes por infra√ß√£o

---

## üö® Casos Reais de Consequ√™ncias

### Multas e Settlements (EUA)
- **Bank of America** (2011): **$335 milh√µes**
- **Wells Fargo** (2012): **$175 milh√µes**
- **Countrywide Financial** (2011): **$335 milh√µes**
- **Multiple banks** (2010s): **Bilh√µes em total**

### Danos √† Reputa√ß√£o
- **Amazon** (2018): Sistema de recrutamento descontinuado
- **COMPAS** (2016): Investiga√ß√µes e processos
- **Apple Card** (2019): Investiga√ß√£o regulat√≥ria
- **Reconhecimento Facial**: Morat√≥rias e banimentos

---

## üéØ Decis√£o: Qual Notebook Usar?

| Sua Situa√ß√£o | Notebook Recomendado |
|--------------|---------------------|
| Nunca analisou fairness antes | `01_introducao_fairness` |
| Modelo vai para produ√ß√£o (critical!) | `02_analise_completa` ‚≠ê‚≠ê |
| Detectou bias e precisa corrigir | `03_mitigacao_bias` |
| Aplica√ß√£o regulada (credit, hiring, healthcare) | **TODOS (obrigat√≥rio)** |
| Auditoria ou compliance | `02_analise_completa` + relat√≥rios |

---

## ‚úÖ Checklist de Fairness para Produ√ß√£o

Antes de fazer deploy de QUALQUER modelo que afeta pessoas:

- [ ] ‚úÖ Identificar atributos protegidos
- [ ] ‚úÖ Executar an√°lise completa (15 m√©tricas)
- [ ] ‚úÖ Verificar EEOC 80% Rule
- [ ] ‚úÖ Analisar confusion matrices por grupo
- [ ] ‚úÖ Gerar relat√≥rio HTML para documenta√ß√£o
- [ ] ‚úÖ Consultar time jur√≠dico
- [ ] ‚úÖ Se bias detectado: aplicar mitiga√ß√£o
- [ ] ‚úÖ Re-validar ap√≥s mitiga√ß√£o
- [ ] ‚úÖ Configurar monitoramento cont√≠nuo em produ√ß√£o
- [ ] ‚úÖ Estabelecer frequ√™ncia de re-valida√ß√£o

**Se QUALQUER item falhar: N√ÉO FAZER DEPLOY!**

---

## üéØ Pr√≥ximos Passos

Depois de dominar fairness, continue para:

üìÅ **05_casos_uso/01_credit_scoring.ipynb** ‚≠ê‚≠ê‚≠ê
- Caso real completo end-to-end
- Credit Scoring com compliance total
- Workflow completo de valida√ß√£o

üìÅ **03_testes_validacao/**
- Combinar fairness com robustez
- Combinar fairness com incerteza

---

## üí° Dicas Cr√≠ticas

### 1. Fairness √© OBRIGAT√ìRIO, n√£o opcional
```python
# ‚ùå NUNCA fa√ßa isso em produ√ß√£o:
model.fit(X, y)
# Deploy sem an√°lise de fairness

# ‚úÖ SEMPRE fa√ßa isso:
model.fit(X, y)
exp = Experiment(dataset, protected_attributes=['gender', 'race'])
fairness_result = exp.run_fairness_tests(config='full')
if fairness_result.passes_eeoc_compliance():
    # OK para considerar deploy
else:
    # STOP - aplicar mitiga√ß√£o
```

### 2. Use config='full' para Produ√ß√£o
- `quick`: Explora√ß√£o inicial
- `medium`: Desenvolvimento
- `full`: **Valida√ß√£o final obrigat√≥ria**

### 3. Sempre Salve Relat√≥rios
```python
# Documenta√ß√£o para auditoria e compliance
exp.save_fairness_report('fairness_report_YYYY-MM-DD.html')
```

### 4. Monitoramento Cont√≠nuo
Fairness pode degradar ao longo do tempo!
- Re-validar mensalmente (m√≠nimo)
- Monitorar em produ√ß√£o
- Alertas autom√°ticos se DI < 0.80

---

## üìö Recursos Adicionais

### Bibliotecas de Fairness
- **Fairlearn** (Microsoft): https://fairlearn.org
- **AIF360** (IBM): https://aif360.mybluemix.net
- **Themis-ml**: https://github.com/cosmicBboy/themis-ml

### Leituras Recomendadas
- ProPublica - "Machine Bias" (COMPAS)
- Kate Crawford - "Atlas of AI"
- Cathy O'Neil - "Weapons of Math Destruction"
- Solon Barocas et al. - "Fairness and Machine Learning"

### Cursos
- Google - "Machine Learning Fairness"
- Coursera - "AI For Everyone" (Andrew Ng)

---

## üìû Precisa de Ajuda?

- üìñ [Documenta√ß√£o DeepBridge](../../planejamento_doc/1-CORE/)
- üíª [C√≥digo Fonte](https://github.com/DeepBridge-Validation/DeepBridge)
- ‚ùì [Issues](https://github.com/DeepBridge-Validation/DeepBridge/issues)

**Para quest√µes legais**: Sempre consulte advogados especializados em compliance de ML

---

<div style="background-color: #ffebee; padding: 20px; border-radius: 10px;">
<h3 style="color: #c62828;">‚ö†Ô∏è AVISO LEGAL</h3>
<p style="color: #b71c1c;">
Estes notebooks s√£o educacionais. Para aplica√ß√µes reais em produ√ß√£o que afetam pessoas:
</p>
<ul style="color: #b71c1c;">
<li><b>SEMPRE</b> consulte time jur√≠dico</li>
<li><b>SEMPRE</b> contrate especialistas em fairness ML</li>
<li><b>SEMPRE</b> siga todas as regula√ß√µes aplic√°veis</li>
<li><b>SEMPRE</b> documente TUDO</li>
<li><b>SEMPRE</b> monitore continuamente</li>
</ul>
<p style="color: #b71c1c;">
<b>O uso inadequado de modelos com bias pode resultar em:</b><br>
- Multas milion√°rias<br>
- Processos judiciais<br>
- Danos irrevers√≠veis √† reputa√ß√£o<br>
- Perda de licen√ßa operacional<br>
- Discrimina√ß√£o ilegal de pessoas
</p>
</div>

---

**√öltima Atualiza√ß√£o**: 04 de Novembro de 2025
**Status**: ‚úÖ Fase 2 Completa (3/3 notebooks)
**Import√¢ncia**: ‚≠ê‚≠ê CR√çTICA para modelos em produ√ß√£o
