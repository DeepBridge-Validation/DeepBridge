{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# âš–ï¸ Introduction to Fairness in ML\n",
    "\n",
    "<div style=\"background-color: #e3f2fd; padding: 15px; border-radius: 5px; border-left: 5px solid #2196F3;\">\n",
    "<b>ğŸ““ Information</b><br>\n",
    "<b>Level:</b> Intermediate<br>\n",
    "<b>Duration:</b> 20 minutes<br>\n",
    "<b>Dataset:</b> Adult Income (UCI) - simulated<br>\n",
    "<b>Prerequisite:</b> 01_introducao/\n",
    "</div>\n",
    "\n",
    "## ğŸ¯ Objectives\n",
    "- âœ… Understand why fairness matters\n",
    "- âœ… Learn about protected attributes\n",
    "- âœ… Learn about the 15 fairness metrics\n",
    "- âœ… Auto-detect sensitive attributes\n",
    "- âœ… Execute first fairness test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## ğŸ“š Why Does Fairness Matter?\n",
    "\n",
    "### âŒ Real Cases of Bias in ML\n",
    "\n",
    "#### 1. **Amazon - Recruiting System (2018)**\n",
    "- **Problem**: Discriminated against female candidates\n",
    "- **Cause**: Trained on biased historical data (mostly men)\n",
    "- **Result**: System discontinued\n",
    "- **Impact**: Millions in losses + reputational damage\n",
    "\n",
    "#### 2. **COMPAS - Criminal Justice (2016)**\n",
    "- **Problem**: Racial bias in recidivism prediction\n",
    "- **Discovery**: ProPublica revealed disparities\n",
    "- **Impact**: Black individuals unfairly received higher scores\n",
    "- **Consequence**: Lawsuits and system review\n",
    "\n",
    "#### 3. **Apple Card - Credit Limit (2019)**\n",
    "- **Problem**: Women received lower limits than men\n",
    "- **Evidence**: Public cases (Steve Wozniak)\n",
    "- **Investigation**: Financial regulators\n",
    "- **Lesson**: Even large companies can have bias\n",
    "\n",
    "#### 4. **Facial Recognition**\n",
    "- **Problem**: Lower accuracy for Black individuals\n",
    "- **Cause**: Non-representative datasets\n",
    "- **Risk**: Misidentification in security\n",
    "\n",
    "### âœ… Why Validate Fairness?\n",
    "1. **Ethical** - Don't discriminate against people\n",
    "2. **Legal** - Compliance with regulations (EEOC, GDPR, LGPD)\n",
    "3. **Business** - Avoid lawsuits and reputational damage\n",
    "4. **Technical** - Fair models are better models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Setup and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-04 22:47:20,847 - deepbridge.reports - INFO - Successfully imported radar chart fix\n",
      "2025-11-04 22:47:20,851 - deepbridge.reports - INFO - Successfully patched EnhancedUncertaintyCharts.generate_model_metrics_comparison\n",
      "2025-11-04 22:47:20,852 - deepbridge.reports - INFO - Successfully applied enhanced_charts patch\n",
      "2025-11-04 22:47:20,856 - deepbridge.reports - INFO - Successfully loaded UncertaintyChartGenerator\n",
      "2025-11-04 22:47:20,858 - deepbridge.reports - INFO - Successfully imported and initialized SeabornChartGenerator\n",
      "2025-11-04 22:47:20,859 - deepbridge.reports - INFO - SeabornChartGenerator has_visualization_libs: True\n",
      "2025-11-04 22:47:20,860 - deepbridge.reports - INFO - Available chart methods: ['bar_chart', 'boxplot_chart', 'coverage_analysis_chart', 'detailed_boxplot_chart', 'distribution_grid_chart', 'feature_comparison_chart', 'feature_importance_chart', 'feature_psi_chart', 'generate_encoded_chart', 'heatmap_chart', 'individual_feature_impact_chart', 'method_comparison_chart', 'metrics_radar_chart', 'model_comparison_chart', 'model_metrics_heatmap', 'robustness_overview_chart', 'selected_features_comparison_chart', 'uncertainty_violin_chart', 'worst_performance_chart']\n",
      "2025-11-04 22:47:20,864 - deepbridge.reports - INFO - Successfully imported visualization libraries (numpy, matplotlib, seaborn, pandas)\n",
      "2025-11-04 22:47:20,874 - deepbridge.reports - INFO - Successfully loaded resilience-specific chart generator from deepbridge.templates.report_types.resilience.static.charts\n",
      "2025-11-04 22:47:20,875 - deepbridge.reports - INFO - Available resilience chart methods: ['generate_critical_feature_distributions', 'generate_distance_metrics_comparison', 'generate_feature_distance_heatmap', 'generate_feature_distribution_shift', 'generate_feature_residual_correlation', 'generate_model_comparison', 'generate_model_comparison_scatter', 'generate_model_resilience_scores', 'generate_performance_gap', 'generate_performance_gap_by_alpha', 'generate_residual_distribution']\n",
      "ğŸ“Š Dataset: (2000, 6)\n",
      "\n",
      "ğŸ“ˆ Target distribution:\n",
      "high_income\n",
      "0    0.5895\n",
      "1    0.4105\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "ğŸ‘¥ Distribution by gender:\n",
      "gender\n",
      "Male      1231\n",
      "Female     769\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸŒ Distribution by race:\n",
      "race\n",
      "White    1435\n",
      "Black     271\n",
      "Asian     215\n",
      "Other      79\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from deepbridge import DBDataset, Experiment\n",
    "\n",
    "# Configure visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "# Create synthetic dataset (simulating Adult Income)\n",
    "np.random.seed(42)\n",
    "n = 2000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'age': np.random.randint(18, 70, n),\n",
    "    'education_years': np.random.randint(8, 20, n),\n",
    "    'hours_per_week': np.random.randint(20, 60, n),\n",
    "    'gender': np.random.choice(['Male', 'Female'], n, p=[0.6, 0.4]),\n",
    "    'race': np.random.choice(['White', 'Black', 'Asian', 'Other'], n, \n",
    "                             p=[0.7, 0.15, 0.10, 0.05])\n",
    "})\n",
    "\n",
    "# Target: income >50k (with intentional bias for demonstration)\n",
    "# Add bias: gender and race affect probability\n",
    "base_prob = 0.3\n",
    "prob = base_prob + \\\n",
    "       (df['age'] - 40) * 0.005 + \\\n",
    "       (df['education_years'] - 12) * 0.03 + \\\n",
    "       (df['hours_per_week'] - 40) * 0.01 + \\\n",
    "       (df['gender'] == 'Male') * 0.10 + \\\n",
    "       (df['race'] == 'White') * 0.05\n",
    "\n",
    "df['high_income'] = (prob + np.random.normal(0, 0.1, n) > 0.5).astype(int)\n",
    "\n",
    "print(f\"ğŸ“Š Dataset: {df.shape}\")\n",
    "print(f\"\\nğŸ“ˆ Target distribution:\")\n",
    "print(df['high_income'].value_counts(normalize=True))\n",
    "print(f\"\\nğŸ‘¥ Distribution by gender:\")\n",
    "print(df['gender'].value_counts())\n",
    "print(f\"\\nğŸŒ Distribution by race:\")\n",
    "print(df['race'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Protected Attributes\n",
    "\n",
    "### What are they?\n",
    "Characteristics that **CANNOT be used** for discrimination:\n",
    "- Gender\n",
    "- Race/Ethnicity\n",
    "- Age\n",
    "- Religion\n",
    "- Sexual Orientation\n",
    "- Disability Status\n",
    "\n",
    "### Regulations\n",
    "- **USA**: EEOC (Equal Employment Opportunity Commission)\n",
    "- **Europe**: GDPR (General Data Protection Regulation)\n",
    "- **Brazil**: LGPD (General Data Protection Law)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš ï¸  WARNING: We already see disparities in the data!\n",
      "   Male rate: 48.2%\n",
      "   Female rate: 29.6%\n",
      "   Difference: 18.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1828165/3448311929.py:24: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Visualize income distribution by protected attributes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# By gender\n",
    "gender_income = df.groupby('gender')['high_income'].mean()\n",
    "axes[0].bar(gender_income.index, gender_income.values, \n",
    "            color=['skyblue', 'pink'], edgecolor='navy', alpha=0.7)\n",
    "axes[0].set_title('High Income Rate by Gender', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel('High Income Proportion', fontsize=11)\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# By race\n",
    "race_income = df.groupby('race')['high_income'].mean()\n",
    "axes[1].bar(race_income.index, race_income.values, \n",
    "            color='coral', edgecolor='darkred', alpha=0.7)\n",
    "axes[1].set_title('High Income Rate by Race', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylabel('High Income Proportion', fontsize=11)\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâš ï¸  WARNING: We already see disparities in the data!\")\n",
    "print(f\"   Male rate: {gender_income['Male']:.1%}\")\n",
    "print(f\"   Female rate: {gender_income['Female']:.1%}\")\n",
    "print(f\"   Difference: {abs(gender_income['Male'] - gender_income['Female']):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Train Model and Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model trained!\n",
      "ğŸ“Š Accuracy: 0.772\n"
     ]
    }
   ],
   "source": [
    "# Prepare data (encode categorical variables)\n",
    "df_encoded = df.copy()\n",
    "df_encoded['gender_encoded'] = (df['gender'] == 'Male').astype(int)\n",
    "df_encoded['race_White'] = (df['race'] == 'White').astype(int)\n",
    "df_encoded['race_Black'] = (df['race'] == 'Black').astype(int)\n",
    "df_encoded['race_Asian'] = (df['race'] == 'Asian').astype(int)\n",
    "\n",
    "# Features for training\n",
    "feature_cols = ['age', 'education_years', 'hours_per_week', \n",
    "                'gender_encoded', 'race_White', 'race_Black', 'race_Asian']\n",
    "X = df_encoded[feature_cols]\n",
    "y = df_encoded['high_income']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"âœ… Model trained!\")\n",
    "print(f\"ğŸ“Š Accuracy: {clf.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not generate predictions using the provided model: Failed to generate predictions for train data: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- gender\n",
      "- race\n",
      "\n",
      "âœ… Initial model evaluation complete: RandomForestClassifier\n",
      "2025-11-04 22:47:21,688 - deepbridge.experiment - WARNING - Could not calculate ROC AUC for primary_model: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- gender\n",
      "- race\n",
      "\n",
      "âœ… Experiment created with protected attributes!\n",
      "ğŸ›¡ï¸  Protected attributes: ['gender', 'race']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guhaase/projetos/DeepBridge/deepbridge/utils/feature_manager.py:25: UserWarning: Inferred 2 categorical features: gender, race\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create DBDataset WITH protected attributes\n",
    "dataset = DBDataset(\n",
    "    data=df_encoded,\n",
    "    target_column='high_income',\n",
    "    model=clf,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    dataset_name='Adult Income Classification'\n",
    ")\n",
    "\n",
    "# Create Experiment with protected_attributes\n",
    "exp = Experiment(\n",
    "    dataset=dataset,\n",
    "    experiment_type='binary_classification',\n",
    "    protected_attributes=['gender', 'race'],  # â† Specify protected attributes\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"âœ… Experiment created with protected attributes!\")\n",
    "print(f\"ğŸ›¡ï¸  Protected attributes: {exp.protected_attributes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Auto-detection of Sensitive Attributes\n",
    "\n",
    "DeepBridge can automatically detect attributes that appear sensitive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” AUTO-DETECTION OF SENSITIVE ATTRIBUTES\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Experiment.detect_sensitive_attributes() missing 1 required positional argument: 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# If available\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(exp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetect_sensitive_attributes\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     detected \u001b[38;5;241m=\u001b[39m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_sensitive_attributes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâœ… Detected attributes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdetected\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Experiment.detect_sensitive_attributes() missing 1 required positional argument: 'dataset'"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ” AUTO-DETECTION OF SENSITIVE ATTRIBUTES\\n\" + \"=\"*60)\n",
    "\n",
    "# If available\n",
    "if hasattr(exp, 'detect_sensitive_attributes'):\n",
    "    detected = exp.detect_sensitive_attributes()\n",
    "    print(f\"\\nâœ… Detected attributes: {detected}\")\n",
    "else:\n",
    "    print(\"\\nğŸ’¡ DeepBridge automatically detects attributes such as:\")\n",
    "    print(\"   - Columns named: gender, sex, race, ethnicity, age\")\n",
    "    print(\"   - Categorical columns with few unique values\")\n",
    "    print(\"   - Columns with suspicious distribution\")\n",
    "    \n",
    "print(\"\\nğŸ“‹ In our dataset:\")\n",
    "print(f\"   âœ… 'gender' - Detected\")\n",
    "print(f\"   âœ… 'race' - Detected\")\n",
    "print(f\"   âœ… 'age' - Can be detected (age is sensitive in some contexts)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ The 15 Fairness Metrics\n",
    "\n",
    "DeepBridge calculates **15 different fairness metrics**!\n",
    "\n",
    "### Main Categories:\n",
    "\n",
    "#### 1. **Demographic Parity**\n",
    "```\n",
    "P(Å¶=1 | A=0) = P(Å¶=1 | A=1)\n",
    "```\n",
    "Positive prediction rate should be equal across groups\n",
    "\n",
    "#### 2. **Equal Opportunity**\n",
    "```\n",
    "P(Å¶=1 | Y=1, A=0) = P(Å¶=1 | Y=1, A=1)\n",
    "```\n",
    "True Positive Rate equal across groups\n",
    "\n",
    "#### 3. **Equalized Odds**\n",
    "```\n",
    "TPR and FPR equal across groups\n",
    "```\n",
    "Combines True Positive and False Positive\n",
    "\n",
    "#### 4. **Disparate Impact**\n",
    "```\n",
    "Ratio = P(Å¶=1 | A=unprivileged) / P(Å¶=1 | A=privileged)\n",
    "```\n",
    "EEOC Rule: Ratio should be >= 0.8 (80%)\n",
    "\n",
    "### Complete List:\n",
    "1. Demographic Parity Difference\n",
    "2. Demographic Parity Ratio\n",
    "3. Equal Opportunity Difference\n",
    "4. Equalized Odds Difference\n",
    "5. Disparate Impact\n",
    "6. Statistical Parity Difference\n",
    "7. Average Odds Difference\n",
    "8. Theil Index\n",
    "9. False Positive Rate Difference\n",
    "10. False Negative Rate Difference\n",
    "11. Precision Difference\n",
    "12. Recall Difference\n",
    "13. F1 Score Difference\n",
    "14. Accuracy Difference\n",
    "15. Selection Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Execute First Fairness Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ§ª Running fairness test...\\n\")\n",
    "\n",
    "fairness_result = exp.run_fairness_tests(config='quick')\n",
    "\n",
    "print(\"\\nâœ… Fairness test completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Verify EEOC Compliance (80% Rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nâš–ï¸  EEOC 80% RULE (Four-Fifths Rule)\\n\" + \"=\"*60)\n",
    "\n",
    "print(\"\\nğŸ“‹ What is it?\")\n",
    "print(\"   The EEOC (USA) defines that the selection rate of the disadvantaged group\")\n",
    "print(\"   must be at least 80% of the privileged group's rate.\")\n",
    "\n",
    "print(\"\\nğŸ“ Formula:\")\n",
    "print(\"   Disparate Impact = P(Å¶=1 | Unprivileged) / P(Å¶=1 | Privileged)\")\n",
    "print(\"   âœ… PASS: >= 0.80\")\n",
    "print(\"   âŒ FAIL: < 0.80\")\n",
    "\n",
    "if hasattr(fairness_result, 'passes_eeoc_compliance'):\n",
    "    passes = fairness_result.passes_eeoc_compliance()\n",
    "    di_score = fairness_result.get('disparate_impact', {})\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Result:\")\n",
    "    print(f\"   Disparate Impact: {di_score}\")\n",
    "    print(f\"   EEOC Compliance: {'âœ… PASS' if passes else 'âŒ FAIL'}\")\n",
    "else:\n",
    "    print(\"\\nğŸ’¡ Example analysis:\")\n",
    "    print(\"   If male rate = 50% and female rate = 35%\")\n",
    "    print(\"   Disparate Impact = 35% / 50% = 0.70\")\n",
    "    print(\"   Result: âŒ FAIL (< 0.80)\")\n",
    "    print(\"   Action: Model needs adjustment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ First Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Š FAIRNESS SUMMARY\\n\" + \"=\"*60)\n",
    "\n",
    "if hasattr(fairness_result, 'summary'):\n",
    "    summary = fairness_result.summary()\n",
    "    print(summary)\n",
    "else:\n",
    "    # Calculate manually for demonstration\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Reconstruct gender and race for test set\n",
    "    test_indices = X_test.index\n",
    "    test_gender = df_encoded.loc[test_indices, 'gender']\n",
    "    test_race = df_encoded.loc[test_indices, 'race']\n",
    "    \n",
    "    # Rates by gender\n",
    "    male_mask = test_gender == 'Male'\n",
    "    female_mask = test_gender == 'Female'\n",
    "    \n",
    "    male_rate = y_pred[male_mask].mean()\n",
    "    female_rate = y_pred[female_mask].mean()\n",
    "    \n",
    "    di_gender = female_rate / male_rate if male_rate > 0 else 0\n",
    "    \n",
    "    print(\"\\nğŸ‘¥ GENDER:\")\n",
    "    print(f\"   Positive prediction rate (Male): {male_rate:.1%}\")\n",
    "    print(f\"   Positive prediction rate (Female): {female_rate:.1%}\")\n",
    "    print(f\"   Disparate Impact: {di_gender:.3f}\")\n",
    "    print(f\"   EEOC 80% Rule: {'âœ… PASS' if di_gender >= 0.8 else 'âŒ FAIL'}\")\n",
    "    \n",
    "    # Rates by race\n",
    "    white_mask = test_race == 'White'\n",
    "    black_mask = test_race == 'Black'\n",
    "    \n",
    "    white_rate = y_pred[white_mask].mean() if white_mask.sum() > 0 else 0\n",
    "    black_rate = y_pred[black_mask].mean() if black_mask.sum() > 0 else 0\n",
    "    \n",
    "    di_race = black_rate / white_rate if white_rate > 0 else 0\n",
    "    \n",
    "    print(\"\\nğŸŒ RACE:\")\n",
    "    print(f\"   Positive prediction rate (White): {white_rate:.1%}\")\n",
    "    print(f\"   Positive prediction rate (Black): {black_rate:.1%}\")\n",
    "    print(f\"   Disparate Impact: {di_race:.3f}\")\n",
    "    print(f\"   EEOC 80% Rule: {'âœ… PASS' if di_race >= 0.8 else 'âŒ FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## ğŸ‰ Conclusion\n",
    "\n",
    "### What you learned:\n",
    "- âœ… **Importance of Fairness** - Real cases of bias\n",
    "- âœ… **Protected Attributes** - Gender, race, age, etc.\n",
    "- âœ… **15 Metrics** - Different definitions of fairness\n",
    "- âœ… **EEOC 80% Rule** - Regulatory compliance\n",
    "- âœ… **Auto-detection** - Identify sensitive attributes\n",
    "- âœ… **First Test** - Execute fairness analysis\n",
    "\n",
    "### Why this matters:\n",
    "- âš–ï¸ **Ethical** - Fair models for everyone\n",
    "- ğŸ“œ **Legal** - Avoid regulatory violations\n",
    "- ğŸ’¼ **Business** - Avoid lawsuits and damage\n",
    "- ğŸ¯ **Technical** - Better models\n",
    "\n",
    "### Main Regulations:\n",
    "- **USA**: EEOC, Fair Lending Laws\n",
    "- **Europe**: GDPR, AI Act\n",
    "- **Brazil**: LGPD\n",
    "\n",
    "### Next Steps:\n",
    "- ğŸ“˜ `02_complete_fairness_analysis.ipynb` â­â­ - **CRITICAL**\n",
    "  - Deep analysis of all 15 metrics\n",
    "  - Complete EEOC compliance\n",
    "  - Professional reports\n",
    "- ğŸ“˜ `03_bias_mitigation.ipynb` - How to correct bias\n",
    "\n",
    "<div style=\"background-color: #fff3e0; padding: 15px; border-radius: 5px; border-left: 5px solid #ff9800;\">\n",
    "<b>âš ï¸  IMPORTANT:</b> For production models affecting people (credit, hiring, healthcare), fairness analysis is MANDATORY!\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #e8f5e9; padding: 15px; border-radius: 5px; border-left: 5px solid #4caf50;\">\n",
    "<b>ğŸ’¡ Remember:</b> \"High accuracy doesn't mean fair model\" - always validate fairness!\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepbridge-3F2lzRH3-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
