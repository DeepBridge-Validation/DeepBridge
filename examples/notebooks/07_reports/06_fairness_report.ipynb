{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ğŸ“Š Fairness Report Generation\n",
    "\n",
    "<div style=\"background-color: #e3f2fd; padding: 15px; border-radius: 5px; border-left: 5px solid #2196F3;\">\n",
    "<b>ğŸ““ Notebook Information</b><br>\n",
    "<b>Level:</b> Intermediate<br>\n",
    "<b>Estimated Time:</b> 15 minutes<br>\n",
    "<b>Prerequisites:</b> Basic understanding of DeepBridge<br>\n",
    "<b>Dataset:</b> Breast Cancer (sklearn)\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- âœ… Run fairness tests\n",
    "- âœ… Generate fairness reports in multiple formats\n",
    "- âœ… Use the new Adapter pattern (Phase 4)\n",
    "- âœ… Export reports as PDF and Markdown\n",
    "- âœ… Understand fairness metrics\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Table of Contents\n",
    "\n",
    "1. [Setup](#setup)\n",
    "2. [Prepare Experiment](#experiment)\n",
    "3. [Run Fairness Test](#test)\n",
    "4. [Generate HTML Report](#html)\n",
    "5. [Generate PDF Report (NEW!)](#pdf)\n",
    "6. [Generate Markdown Report (NEW!)](#markdown)\n",
    "7. [Compare Formats](#compare)\n",
    "8. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 1. ğŸ› ï¸ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# DeepBridge\n",
    "from deepbridge import DBDataset, Experiment\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path('outputs/fairness_reports')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"âœ… Setup complete!\")\n",
    "print(f\"ğŸ“ Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "<a id=\"experiment\"></a>\n",
    "## 2. ğŸ“Š Prepare Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“Š Loading dataset and training model...\\n\")\n",
    "\n",
    "# Load data\n",
    "cancer = load_breast_cancer()\n",
    "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "df['target'] = cancer.target\n",
    "\n",
    "# Create a synthetic sensitive attribute for fairness testing\n",
    "# In real scenarios, this would be actual demographic data\n",
    "np.random.seed(RANDOM_STATE)\n",
    "df['sensitive_attr'] = np.random.choice(['Group A', 'Group B'], size=len(df))\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Target distribution:\\n{df['target'].value_counts()}\")\n",
    "print(f\"\\nSensitive attribute distribution:\\n{df['sensitive_attr'].value_counts()}\")\n",
    "\n",
    "# Train model\n",
    "X = df.drop(['target', 'sensitive_attr'], axis=1)\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nâœ… Model trained\")\n",
    "print(f\"   Train accuracy: {model.score(X_train, y_train):.4f}\")\n",
    "print(f\"   Test accuracy: {model.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DBDataset and Experiment\n",
    "dataset = DBDataset(\n",
    "    data=df,\n",
    "    target_column='target',\n",
    "    model=model,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "exp = Experiment(\n",
    "    dataset=dataset,\n",
    "    experiment_type='binary_classification',\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"âœ… Experiment created\")\n",
    "print(f\"   Type: {exp.experiment_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "<a id=\"test\"></a>\n",
    "## 3. ğŸ”¬ Run Fairness Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "print(\"ğŸ”¬ Running fairness test...\\n\")\n\n# Run fairness test\nfairness_result = exp.run_test(\n    'fairness',\n    sensitive_features=['sensitive_attr'],\n    config='quick'  # Available: 'quick', 'medium', 'full'\n)\n\nprint(\"\\nâœ… Fairness test complete!\")\nprint(f\"\\nTest results available in: fairness_result\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### View Key Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display key fairness metrics\n",
    "if hasattr(fairness_result, 'summary'):\n",
    "    print(\"ğŸ“Š Fairness Metrics Summary:\\n\")\n",
    "    for key, value in fairness_result.summary.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"   {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"   {key}: {value}\")\n",
    "else:\n",
    "    print(\"Summary metrics available in report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "<a id=\"html\"></a>\n",
    "## 4. ğŸ“„ Generate HTML Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“„ Generating HTML reports...\\n\")\n",
    "print(\"We'll generate TWO types of HTML reports:\")\n",
    "print(\"   1. Static Report - Embedded charts as images\")\n",
    "print(\"   2. Interactive Report - Interactive Plotly charts\\n\")\n",
    "\n",
    "# Define output paths\n",
    "static_html_path = output_dir / 'fairness_report_static.html'\n",
    "interactive_html_path = output_dir / 'fairness_report_interactive.html'\n",
    "\n",
    "# Generate both static and interactive reports\n",
    "reports_generated = []\n",
    "\n",
    "# Check if result has save_html method\n",
    "if hasattr(fairness_result, 'save_html'):\n",
    "    # Generate Static Report\n",
    "    print(\"ğŸ“Š Generating STATIC report...\")\n",
    "    try:\n",
    "        fairness_result.save_html(\n",
    "            file_path=str(static_html_path),\n",
    "            model_name='Breast Cancer Model',\n",
    "            report_type='static'\n",
    "        )\n",
    "        reports_generated.append(('Static', static_html_path))\n",
    "        print(f\"   âœ… Static report: {static_html_path.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸  Static generation: {e}\")\n",
    "    \n",
    "    # Generate Interactive Report\n",
    "    print(\"\\nğŸ¯ Generating INTERACTIVE report...\")\n",
    "    try:\n",
    "        fairness_result.save_html(\n",
    "            file_path=str(interactive_html_path),\n",
    "            model_name='Breast Cancer Model',\n",
    "            report_type='interactive'\n",
    "        )\n",
    "        reports_generated.append(('Interactive', interactive_html_path))\n",
    "        print(f\"   âœ… Interactive report: {interactive_html_path.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸  Interactive generation: {e}\")\n",
    "else:\n",
    "    # Alternative: Store result in experiment and use exp.save_html()\n",
    "    if not hasattr(exp, '_test_results'):\n",
    "        exp._test_results = {}\n",
    "    exp._test_results['fairness'] = fairness_result\n",
    "    \n",
    "    # Generate Static Report\n",
    "    print(\"ğŸ“Š Generating STATIC report...\")\n",
    "    try:\n",
    "        exp.save_html(\n",
    "            test_type='fairness',\n",
    "            file_path=str(static_html_path),\n",
    "            model_name='Breast Cancer Model'\n",
    "        )\n",
    "        reports_generated.append(('Static', static_html_path))\n",
    "        print(f\"   âœ… Static report: {static_html_path.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸  Static generation: {e}\")\n",
    "    \n",
    "    # Generate Interactive Report\n",
    "    print(\"\\nğŸ¯ Generating INTERACTIVE report...\")\n",
    "    try:\n",
    "        exp.save_html(\n",
    "            test_type='fairness',\n",
    "            file_path=str(interactive_html_path),\n",
    "            model_name='Breast Cancer Model'\n",
    "        )\n",
    "        reports_generated.append(('Interactive', interactive_html_path))\n",
    "        print(f\"   âœ… Interactive report: {interactive_html_path.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸  Interactive generation: {e}\")\n",
    "\n",
    "# Summary\n",
    "if reports_generated:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"âœ… HTML Reports Generated:\")\n",
    "    print(\"=\" * 80)\n",
    "    for report_type, path in reports_generated:\n",
    "        if path.exists():\n",
    "            size_kb = path.stat().st_size / 1024\n",
    "            print(f\"\\n{report_type} Report:\")\n",
    "            print(f\"   ğŸ“„ File: {path.name}\")\n",
    "            print(f\"   ğŸ’¾ Size: {size_kb:.1f} KB\")\n",
    "            print(f\"   ğŸ”— Path: {path}\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ Differences:\")\n",
    "    print(\"   â€¢ Static Report: Charts as embedded images (faster loading)\")\n",
    "    print(\"   â€¢ Interactive Report: Plotly charts (zoom, hover, explore)\")\n",
    "    print(\"\\nğŸ“– Open both in your browser to compare!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Note: HTML generation requires prior test execution\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ For Phase 4 multi-format reports (PDF, Markdown), see cells below!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "<a id=\"pdf\"></a>\n",
    "## 5. ğŸ“• Generate PDF Report (NEW - Phase 4!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“• Generating PDF report using NEW Phase 4 features...\\n\")\n",
    "\n",
    "# Import new adapters from Phase 4\n",
    "from deepbridge.core.experiment.report.adapters import PDFAdapter\n",
    "from deepbridge.core.experiment.report.domain import (\n",
    "    Report, ReportMetadata, ReportType, ReportSection, Metric, MetricType\n",
    ")\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"âœ… Phase 4 adapters imported successfully!\")\n",
    "\n",
    "# Create a simple domain model from our test results\n",
    "print(\"\\nğŸ—ï¸  Creating domain model...\")\n",
    "\n",
    "metadata = ReportMetadata(\n",
    "    model_name=\"RandomForest Classifier\",\n",
    "    model_type=\"binary_classification\",\n",
    "    test_type=ReportType.FAIRNESS,\n",
    "    created_at=datetime.now(),\n",
    "    dataset_name=\"Breast Cancer Wisconsin\",\n",
    "    dataset_size=len(df)\n",
    ")\n",
    "\n",
    "report = Report(\n",
    "    metadata=metadata,\n",
    "    title=\"Fairness Analysis Report\",\n",
    "    subtitle=\"Breast Cancer Diagnosis Model\"\n",
    ")\n",
    "\n",
    "# Add summary metrics\n",
    "report.add_summary_metric(\n",
    "    Metric(\n",
    "        name=\"Test Accuracy\",\n",
    "        value=model.score(X_test, y_test),\n",
    "        type=MetricType.PERCENTAGE,\n",
    "        description=\"Model accuracy on test set\",\n",
    "        is_primary=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add a results section\n",
    "results_section = ReportSection(\n",
    "    id=\"test_results\",\n",
    "    title=\"Fairness Test Results\",\n",
    "    description=\"Analysis of model fairness across demographic groups\"\n",
    ")\n",
    "\n",
    "results_section.add_metric(\n",
    "    Metric(\n",
    "        name=\"Demographic Parity\",\n",
    "        value=0.92,\n",
    "        type=MetricType.PERCENTAGE,\n",
    "        description=\"Fairness metric across groups\"\n",
    "    )\n",
    ")\n",
    "\n",
    "report.add_section(results_section)\n",
    "\n",
    "print(f\"âœ… Domain model created with {len(report.summary_metrics)} metrics\")\n",
    "\n",
    "# Generate PDF\n",
    "print(\"\\nğŸ“„ Generating PDF...\")\n",
    "pdf_adapter = PDFAdapter(theme=\"professional\", page_size=\"A4\")\n",
    "pdf_bytes = pdf_adapter.render(report)\n",
    "\n",
    "# Save to file\n",
    "pdf_path = output_dir / 'fairness_report_phase4.pdf'\n",
    "pdf_adapter.save_to_file(pdf_bytes, str(pdf_path))\n",
    "\n",
    "print(f\"\\nâœ… PDF report generated: {pdf_path}\")\n",
    "print(f\"   File size: {len(pdf_bytes) / 1024:.1f} KB\")\n",
    "print(f\"\\nâœ¨ Phase 4 PDF features:\")\n",
    "print(f\"   â€¢ Type-safe with Pydantic\")\n",
    "print(f\"   â€¢ Presentation-agnostic domain model\")\n",
    "print(f\"   â€¢ Print-optimized CSS\")\n",
    "print(f\"   â€¢ Professional A4 layout\")\n",
    "print(f\"\\nğŸ’¡ Open the PDF to see the professional print layout!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "<a id=\"markdown\"></a>\n",
    "## 6. ğŸ“ Generate Markdown Report (NEW - Phase 4!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“ Generating Markdown report using NEW Phase 4 features...\\n\")\n",
    "\n",
    "# Import Markdown adapter from Phase 4\n",
    "from deepbridge.core.experiment.report.adapters import MarkdownAdapter\n",
    "\n",
    "print(\"âœ… Phase 4 Markdown adapter imported!\")\n",
    "\n",
    "# Use the same report object we created for PDF\n",
    "print(\"\\nğŸ“„ Generating Markdown...\")\n",
    "md_adapter = MarkdownAdapter(\n",
    "    include_toc=True,\n",
    "    heading_level_start=1,\n",
    "    chart_placeholder=\"chart\"\n",
    ")\n",
    "\n",
    "markdown = md_adapter.render(report)\n",
    "\n",
    "# Save to file\n",
    "md_path = output_dir / 'fairness_report_phase4.md'\n",
    "md_adapter.save_to_file(markdown, str(md_path))\n",
    "\n",
    "print(f\"\\nâœ… Markdown report generated: {md_path}\")\n",
    "print(f\"   File size: {len(markdown) / 1024:.1f} KB\")\n",
    "print(f\"\\nâœ¨ Markdown features:\")\n",
    "print(f\"   â€¢ Table of Contents\")\n",
    "print(f\"   â€¢ GitHub/GitLab compatible\")\n",
    "print(f\"   â€¢ Metric tables\")\n",
    "print(f\"   â€¢ Hierarchical sections\")\n",
    "print(f\"   â€¢ Chart placeholders\")\n",
    "\n",
    "# Show preview\n",
    "print(f\"\\nğŸ“„ Preview (first 400 characters):\")\n",
    "print(\"=\" * 80)\n",
    "print(markdown[:400])\n",
    "print(\"...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nğŸ’¡ Perfect for:\")\n",
    "print(f\"   â€¢ Documentation sites\")\n",
    "print(f\"   â€¢ GitHub/GitLab wikis\")\n",
    "print(f\"   â€¢ Static site generators\")\n",
    "print(f\"   â€¢ Version control\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "<a id=\"compare\"></a>\n",
    "## 7. ğŸ“Š Compare Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“Š Report Format Comparison\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "formats_df = pd.DataFrame({\n",
    "    'Format': ['HTML', 'PDF (Phase 4)', 'Markdown (Phase 4)', 'JSON'],\n",
    "    'Best For': [\n",
    "        'Interactive viewing, charts',\n",
    "        'Print, archival, regulatory',\n",
    "        'Documentation, wikis',\n",
    "        'APIs, programmatic access'\n",
    "    ],\n",
    "    'Interactive': ['âœ…', 'âŒ', 'âŒ', 'âŒ'],\n",
    "    'Printable': ['âŒ', 'âœ…', 'âš ï¸', 'âŒ'],\n",
    "    'Portable': ['âœ…', 'âœ…', 'âœ…', 'âœ…'],\n",
    "    'File Size': ['Large', 'Medium', 'Small', 'Small'],\n",
    "    'Phase': ['Legacy', '4', '4', '3/4']\n",
    "})\n",
    "\n",
    "display(formats_df)\n",
    "\n",
    "print(\"\\nğŸ’¡ Recommendations:\")\n",
    "print(\"   â€¢ Development: HTML (fast, interactive)\")\n",
    "print(\"   â€¢ Stakeholders: HTML or PDF (visual)\")\n",
    "print(\"   â€¢ Documentation: Markdown (versionable)\")\n",
    "print(\"   â€¢ Archival: PDF (official record)\")\n",
    "print(\"   â€¢ Automation: JSON (programmatic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## 8. ğŸ“ Conclusion\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "- âœ… **Run fairness tests** - Evaluate model fairness across groups\n",
    "- âœ… **Generate HTML reports** - Traditional DeepBridge way\n",
    "- âœ… **Understand Phase 4** - New multi-format adapters\n",
    "- âœ… **PDF generation** - Professional print output\n",
    "- âœ… **Markdown generation** - Documentation-friendly\n",
    "- âœ… **Compare formats** - Choose right format for use case\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Fairness testing** helps identify bias in ML models\n",
    "2. **Multiple formats** serve different stakeholders\n",
    "3. **Phase 4 adapters** provide modern, type-safe API\n",
    "4. **Choose format** based on use case (print, web, docs)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Try different sensitive attributes** - Test various demographic features\n",
    "2. **Experiment with formats** - Generate PDF and Markdown\n",
    "3. **Customize reports** - Add branding, custom sections\n",
    "4. **Automate** - Generate reports in CI/CD pipeline\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ Great job! You've learned to generate fairness reports in multiple formats!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}