{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè¶‚≠ê‚≠ê‚≠ê Complete Use Case: Credit Scoring\n",
    "\n",
    "<div style=\"background-color: #fff3e0; padding: 20px; border-radius: 5px; border-left: 5px solid #ff6f00;\">\n",
    "<b>üî• CRITICAL CASE STUDY - REAL PRODUCTION SCENARIO</b><br>\n",
    "<b>Level:</b> Advanced - Production Ready<br>\n",
    "<b>Duration:</b> 45-60 minutes<br>\n",
    "<b>Dataset:</b> Credit Scoring (synthetic realistic)<br>\n",
    "<b>Importance:</b> ‚≠ê‚≠ê‚≠ê ESSENTIAL - Real-world regulated application\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- ‚úÖ Execute complete end-to-end ML validation workflow\n",
    "- ‚úÖ Ensure regulatory compliance (EEOC, Fair Lending Laws)\n",
    "- ‚úÖ Test all validation dimensions (robustness, fairness, uncertainty)\n",
    "- ‚úÖ Generate professional audit documentation\n",
    "- ‚úÖ Make data-driven deployment decisions\n",
    "- ‚úÖ Apply this workflow to your own projects\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Table of Contents\n",
    "\n",
    "1. [Business Context & Problem](#context)\n",
    "2. [Regulatory Requirements](#regulatory)\n",
    "3. [Data Preparation](#data)\n",
    "4. [Exploratory Data Analysis](#eda)\n",
    "5. [Model Training](#training)\n",
    "6. [Baseline Validation](#baseline)\n",
    "7. [Regulatory Validation - Fairness](#fairness)\n",
    "8. [Robustness Testing](#robustness)\n",
    "9. [Uncertainty Quantification](#uncertainty)\n",
    "10. [Drift Detection](#drift)\n",
    "11. [Generate Audit Reports](#reports)\n",
    "12. [Deployment Decision](#decision)\n",
    "13. [Production Monitoring Plan](#monitoring)\n",
    "14. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"context\"></a>\n",
    "## 1. üìñ Business Context & Problem\n",
    "\n",
    "### üè¶ The Scenario\n",
    "\n",
    "You are a **Senior ML Engineer** at **SecureBank**, a mid-sized financial institution.\n",
    "\n",
    "**Your Task:**\n",
    "> \"We need an automated credit scoring model to decide loan approvals. The current manual process is slow, inconsistent, and expensive. Your model must be accurate, fair, robust, and compliant with all regulations before we deploy it.\"\n",
    "> \n",
    "> ‚Äî VP of Risk Management\n",
    "\n",
    "### üí∞ Business Impact\n",
    "\n",
    "- **Cost Savings:** $2M/year from automation\n",
    "- **Processing Time:** 3 days ‚Üí 3 seconds\n",
    "- **Consistency:** Eliminate human bias and errors\n",
    "- **Risk:** Model must not discriminate or fail unexpectedly\n",
    "\n",
    "### ‚ö†Ô∏è Stakes Are High!\n",
    "\n",
    "**If you deploy a bad model:**\n",
    "- üí∏ **Fines:** $10M+ from regulators (EEOC, CFPB)\n",
    "- ‚öñÔ∏è **Lawsuits:** Class action from discriminated applicants\n",
    "- üìâ **Reputation:** Brand damage, customer loss\n",
    "- üö´ **License:** Risk of losing operating license\n",
    "\n",
    "**If you validate properly:**\n",
    "- ‚úÖ Safe deployment\n",
    "- ‚úÖ Regulatory approval\n",
    "- ‚úÖ Business value realized\n",
    "- ‚úÖ Your career advances!\n",
    "\n",
    "### üéØ Success Criteria\n",
    "\n",
    "1. ‚úÖ **Performance:** ROC AUC ‚â• 0.85, Accuracy ‚â• 0.80\n",
    "2. ‚úÖ **Fairness:** Pass EEOC 80% Rule (gender, race, age)\n",
    "3. ‚úÖ **Robustness:** Score ‚â• 0.85 (resist manipulation attempts)\n",
    "4. ‚úÖ **Uncertainty:** Quantified for high-risk decisions\n",
    "5. ‚úÖ **Drift:** Monitoring plan for production\n",
    "6. ‚úÖ **Documentation:** Complete audit trail\n",
    "\n",
    "**Let's get started!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"regulatory\"></a>\n",
    "## 2. ‚öñÔ∏è Regulatory Requirements\n",
    "\n",
    "### üá∫üá∏ US Federal Laws\n",
    "\n",
    "Your model MUST comply with:\n",
    "\n",
    "#### 1. Equal Credit Opportunity Act (ECOA)\n",
    "- ‚ùå **Prohibited:** Discrimination based on race, color, religion, national origin, sex, marital status, age\n",
    "- ‚úÖ **Required:** Adverse action notices, recordkeeping\n",
    "\n",
    "#### 2. Fair Housing Act\n",
    "- ‚ùå **Prohibited:** Housing credit discrimination\n",
    "- ‚úÖ **Required:** Equal access to credit\n",
    "\n",
    "#### 3. Fair Credit Reporting Act (FCRA)\n",
    "- ‚úÖ **Required:** Accurate credit data, consumer rights\n",
    "\n",
    "#### 4. EEOC Guidelines - 80% Rule\n",
    "- üìê **Formula:** `Selection Rate(Unprivileged) / Selection Rate(Privileged) ‚â• 0.80`\n",
    "- üéØ **Disparate Impact:** If ratio < 0.80, you have discriminatory impact!\n",
    "- ‚ö†Ô∏è **Consequence:** Violation = regulatory action\n",
    "\n",
    "### üîç Regulators Watching You\n",
    "\n",
    "1. **CFPB** - Consumer Financial Protection Bureau\n",
    "2. **OCC** - Office of the Comptroller of the Currency\n",
    "3. **FDIC** - Federal Deposit Insurance Corporation\n",
    "4. **FTC** - Federal Trade Commission\n",
    "\n",
    "### üìã Model Documentation Requirements\n",
    "\n",
    "You must document:\n",
    "- ‚úÖ Model development process\n",
    "- ‚úÖ Training data characteristics\n",
    "- ‚úÖ Validation methodology\n",
    "- ‚úÖ Fairness testing results\n",
    "- ‚úÖ Ongoing monitoring plan\n",
    "- ‚úÖ Model governance procedures\n",
    "\n",
    "### üéì Key Principle\n",
    "\n",
    "> **\"Regulation doesn't ban ML in credit - it requires responsible, validated, fair ML!\"**\n",
    "\n",
    "**DeepBridge helps you achieve this!** ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "## 3. üìä Data Preparation\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report, roc_curve, auc\n",
    ")\n",
    "\n",
    "# DeepBridge - Your validation toolkit!\n",
    "from deepbridge import DBDataset, Experiment\n",
    "\n",
    "# Visualization\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(f\"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üè¶ Project: SecureBank Credit Scoring Model Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Realistic Credit Scoring Dataset\n",
    "\n",
    "For this demo, we'll create a synthetic but realistic dataset that mirrors real credit data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üèóÔ∏è Generating realistic credit scoring dataset...\\n\")\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "n_samples = 5000\n",
    "\n",
    "# Generate realistic features\n",
    "df = pd.DataFrame({\n",
    "    # Demographic (some are protected attributes!)\n",
    "    'age': np.random.randint(21, 70, n_samples),\n",
    "    'gender': np.random.choice(['Male', 'Female'], n_samples, p=[0.52, 0.48]),\n",
    "    'race': np.random.choice(\n",
    "        ['White', 'Black', 'Hispanic', 'Asian', 'Other'],\n",
    "        n_samples,\n",
    "        p=[0.60, 0.13, 0.18, 0.06, 0.03]\n",
    "    ),\n",
    "    \n",
    "    # Financial features (legitimate for credit decisions)\n",
    "    'annual_income': np.random.gamma(2, 30000, n_samples).clip(15000, 500000),\n",
    "    'credit_score': np.random.randint(300, 850, n_samples),\n",
    "    'employment_years': np.random.randint(0, 40, n_samples),\n",
    "    'loan_amount': np.random.gamma(2, 15000, n_samples).clip(1000, 100000),\n",
    "    'debt_to_income': np.random.beta(2, 5, n_samples),\n",
    "    'num_credit_lines': np.random.randint(1, 15, n_samples),\n",
    "    'num_delinquencies': np.random.poisson(0.5, n_samples),\n",
    "    'has_mortgage': np.random.choice([0, 1], n_samples, p=[0.6, 0.4]),\n",
    "    'has_cosigner': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),\n",
    "    'employment_type': np.random.choice(\n",
    "        ['Full-time', 'Part-time', 'Self-employed', 'Retired'],\n",
    "        n_samples,\n",
    "        p=[0.65, 0.15, 0.15, 0.05]\n",
    "    ),\n",
    "})\n",
    "\n",
    "# Create realistic target (loan approval)\n",
    "# Use legitimate features + small unintentional bias\n",
    "approval_score = (\n",
    "    # Legitimate factors (strong influence)\n",
    "    (df['credit_score'] - 500) / 100 * 0.25 +\n",
    "    (df['annual_income'] / 100000) * 0.20 +\n",
    "    (df['employment_years'] / 20) * 0.12 +\n",
    "    (1 - df['debt_to_income']) * 0.15 +\n",
    "    (1 - df['num_delinquencies'] / 5) * 0.10 +\n",
    "    df['has_cosigner'] * 0.08 +\n",
    "    \n",
    "    # Unintentional bias (small, but exists - we'll detect and fix!)\n",
    "    (df['gender'] == 'Male') * 0.03 +  # ‚Üê Historical bias\n",
    "    (df['race'] == 'White') * 0.02      # ‚Üê Historical bias\n",
    ")\n",
    "\n",
    "# Convert to binary with noise\n",
    "df['approved'] = (approval_score + np.random.normal(0, 0.12, n_samples) > 0.52).astype(int)\n",
    "\n",
    "print(f\"‚úÖ Dataset created: {df.shape}\")\n",
    "print(f\"\\nüìä Approval rate: {df['approved'].mean():.1%}\")\n",
    "print(f\"\\nüìã Features:\")\n",
    "print(f\"   Total: {len(df.columns) - 1}\")\n",
    "print(f\"   Protected attributes: gender, race, age\")\n",
    "print(f\"   Legitimate credit factors: income, credit_score, employment, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eda\"></a>\n",
    "## 4. üìä Exploratory Data Analysis\n",
    "\n",
    "### Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä DATASET OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "print(f\"\\nMissing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features\n",
    "print(\"üìà NUMERICAL FEATURES SUMMARY\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approval distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count\n",
    "df['approved'].value_counts().plot(kind='bar', ax=axes[0], color=['coral', 'lightgreen'], \n",
    "                                    edgecolor='black', alpha=0.8)\n",
    "axes[0].set_title('Loan Approval Distribution', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Approved', fontsize=11)\n",
    "axes[0].set_ylabel('Count', fontsize=11)\n",
    "axes[0].set_xticklabels(['Rejected (0)', 'Approved (1)'], rotation=0)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Percentage\n",
    "approval_pct = df['approved'].value_counts(normalize=True) * 100\n",
    "approval_pct.plot(kind='pie', ax=axes[1], autopct='%1.1f%%', \n",
    "                   colors=['coral', 'lightgreen'], startangle=90)\n",
    "axes[1].set_title('Approval Rate', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Approval Statistics:\")\n",
    "print(f\"   Approved: {df['approved'].sum()} ({df['approved'].mean():.1%})\")\n",
    "print(f\"   Rejected: {(1-df['approved']).sum()} ({(1-df['approved'].mean()):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Disparities in Raw Data\n",
    "\n",
    "‚ö†Ô∏è **CRITICAL:** Before training, check if data itself has disparities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö†Ô∏è  CHECKING FOR DISPARITIES IN RAW DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# By gender\n",
    "print(\"\\nüë• APPROVAL RATE BY GENDER:\")\n",
    "gender_approval = df.groupby('gender')['approved'].agg(['mean', 'count'])\n",
    "gender_approval['mean'] = gender_approval['mean'] * 100\n",
    "display(gender_approval)\n",
    "\n",
    "# Calculate disparate impact\n",
    "male_rate = df[df['gender'] == 'Male']['approved'].mean()\n",
    "female_rate = df[df['gender'] == 'Female']['approved'].mean()\n",
    "di_gender = female_rate / male_rate if male_rate > 0 else 0\n",
    "\n",
    "print(f\"\\n   Disparate Impact (Gender): {di_gender:.3f}\")\n",
    "print(f\"   {'‚úÖ PASS' if di_gender >= 0.8 else '‚ùå FAIL'} EEOC 80% Rule\")\n",
    "\n",
    "# By race\n",
    "print(\"\\nüåç APPROVAL RATE BY RACE:\")\n",
    "race_approval = df.groupby('race')['approved'].agg(['mean', 'count'])\n",
    "race_approval['mean'] = race_approval['mean'] * 100\n",
    "race_approval = race_approval.sort_values('mean', ascending=False)\n",
    "display(race_approval)\n",
    "\n",
    "# Check disparate impact for each race vs highest\n",
    "max_race = race_approval['mean'].idxmax()\n",
    "max_rate = race_approval.loc[max_race, 'mean'] / 100\n",
    "\n",
    "print(f\"\\n   Reference group (highest): {max_race} ({max_rate:.1%})\")\n",
    "print(f\"\\n   Disparate Impact by Race:\")\n",
    "for race in race_approval.index:\n",
    "    rate = race_approval.loc[race, 'mean'] / 100\n",
    "    di = rate / max_rate if max_rate > 0 else 0\n",
    "    status = \"‚úÖ\" if di >= 0.8 else \"‚ùå\"\n",
    "    print(f\"      {status} {race}: DI = {di:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key financial features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "features_to_plot = ['credit_score', 'annual_income', 'debt_to_income', \n",
    "                     'employment_years', 'loan_amount', 'num_credit_lines']\n",
    "\n",
    "for i, feature in enumerate(features_to_plot):\n",
    "    axes[i].hist(df[feature], bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[i].set_title(f'{feature}', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical features\n",
    "num_features = ['age', 'annual_income', 'credit_score', 'employment_years',\n",
    "                'loan_amount', 'debt_to_income', 'num_credit_lines', \n",
    "                'num_delinquencies', 'approved']\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation = df[num_features].corr()\n",
    "sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Features most correlated with approval\n",
    "print(\"\\nüéØ FEATURES MOST CORRELATED WITH APPROVAL:\")\n",
    "approval_corr = correlation['approved'].sort_values(ascending=False)[1:]\n",
    "print(approval_corr.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; padding: 15px; border-radius: 5px; border-left: 5px solid #28a745;\">\n",
    "<b>‚úÖ EDA Complete!</b> We understand the data. Key findings:\n",
    "<ul>\n",
    "<li>Dataset is realistic with 5000 applicants</li>\n",
    "<li>Approval rate is balanced</li>\n",
    "<li>Some disparities exist in raw data (expected in real scenarios)</li>\n",
    "<li>Credit score, income, and debt_to_income are strong predictors</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"training\"></a>\n",
    "## 5. ü§ñ Model Training\n",
    "\n",
    "### Prepare Features for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Preparing features for modeling...\\n\")\n",
    "\n",
    "# Encode categorical features\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# Gender encoding\n",
    "df_encoded['gender_enc'] = (df['gender'] == 'Male').astype(int)\n",
    "\n",
    "# Race encoding (one-hot)\n",
    "df_encoded['race_White'] = (df['race'] == 'White').astype(int)\n",
    "df_encoded['race_Black'] = (df['race'] == 'Black').astype(int)\n",
    "df_encoded['race_Hispanic'] = (df['race'] == 'Hispanic').astype(int)\n",
    "df_encoded['race_Asian'] = (df['race'] == 'Asian').astype(int)\n",
    "\n",
    "# Employment type encoding\n",
    "df_encoded['emp_fulltime'] = (df['employment_type'] == 'Full-time').astype(int)\n",
    "df_encoded['emp_parttime'] = (df['employment_type'] == 'Part-time').astype(int)\n",
    "df_encoded['emp_self'] = (df['employment_type'] == 'Self-employed').astype(int)\n",
    "\n",
    "# Feature list\n",
    "feature_cols = [\n",
    "    'age', 'annual_income', 'credit_score', 'employment_years',\n",
    "    'loan_amount', 'debt_to_income', 'num_credit_lines', 'num_delinquencies',\n",
    "    'has_mortgage', 'has_cosigner',\n",
    "    'gender_enc',  # ‚Üê Note: Including for now, will test fairness!\n",
    "    'race_White', 'race_Black', 'race_Hispanic', 'race_Asian',\n",
    "    'emp_fulltime', 'emp_parttime', 'emp_self'\n",
    "]\n",
    "\n",
    "X = df_encoded[feature_cols]\n",
    "y = df_encoded['approved']\n",
    "\n",
    "print(f\"‚úÖ Features prepared!\")\n",
    "print(f\"   Total features: {len(feature_cols)}\")\n",
    "print(f\"   Samples: {len(X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Data split:\")\n",
    "print(f\"   Train: {X_train.shape} (approval rate: {y_train.mean():.1%})\")\n",
    "print(f\"   Test: {X_test.shape} (approval rate: {y_test.mean():.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Primary Model - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üå≤ Training Random Forest Classifier...\\n\")\n",
    "\n",
    "# Model configuration\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=12,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"‚úÖ Model trained!\")\n",
    "print(f\"   Algorithm: RandomForestClassifier\")\n",
    "print(f\"   Trees: {model.n_estimators}\")\n",
    "print(f\"   Max depth: {model.max_depth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"baseline\"></a>\n",
    "## 6. ‚úÖ Baseline Validation\n",
    "\n",
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "y_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = {\n",
    "    'Accuracy': [accuracy_score(y_train, y_pred_train), accuracy_score(y_test, y_pred_test)],\n",
    "    'ROC AUC': [roc_auc_score(y_train, y_proba_train), roc_auc_score(y_test, y_proba_test)],\n",
    "    'F1 Score': [f1_score(y_train, y_pred_train), f1_score(y_test, y_pred_test)],\n",
    "    'Precision': [precision_score(y_train, y_pred_train), precision_score(y_test, y_pred_test)],\n",
    "    'Recall': [recall_score(y_train, y_pred_train), recall_score(y_test, y_pred_test)]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics, index=['Train', 'Test']).T\n",
    "\n",
    "print(\"üìä BASELINE PERFORMANCE METRICS\")\n",
    "print(\"=\" * 60)\n",
    "display(metrics_df.style.format(\"{:.3f}\").background_gradient(cmap='RdYlGn', axis=1))\n",
    "\n",
    "# Check overfitting\n",
    "overfit = (metrics_df['Train'] - metrics_df['Test']).abs()\n",
    "print(f\"\\nüí° Overfitting Analysis:\")\n",
    "print(f\"   Average Train-Test gap: {overfit.mean():.3f}\")\n",
    "if overfit.mean() < 0.05:\n",
    "    print(f\"   ‚úÖ Good generalization!\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Some overfitting detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Rejected', 'Approved'],\n",
    "            yticklabels=['Rejected', 'Approved'])\n",
    "plt.title('Confusion Matrix - Test Set', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nüìä Confusion Matrix Breakdown:\")\n",
    "print(f\"   True Negatives (correctly rejected): {tn}\")\n",
    "print(f\"   False Positives (incorrectly approved): {fp}\")\n",
    "print(f\"   False Negatives (incorrectly rejected): {fn}\")\n",
    "print(f\"   True Positives (correctly approved): {tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importances = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(importances['Feature'].head(15), importances['Importance'].head(15),\n",
    "         color='steelblue', edgecolor='black', alpha=0.8)\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.title('Top 15 Feature Importances', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîù TOP 10 MOST IMPORTANT FEATURES:\")\n",
    "print(importances.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; padding: 15px; border-radius: 5px; border-left: 5px solid #28a745;\">\n",
    "<b>‚úÖ Baseline Validation Complete!</b><br>\n",
    "Model performance is good, but this is just the beginning. Now we need to validate for production:<br>\n",
    "<ul>\n",
    "<li>Fairness (regulatory requirement)</li>\n",
    "<li>Robustness (resist manipulation)</li>\n",
    "<li>Uncertainty (quantify confidence)</li>\n",
    "<li>Drift detection (temporal stability)</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"fairness\"></a>\n",
    "## 7. ‚öñÔ∏è REGULATORY VALIDATION - FAIRNESS (CRITICAL!)\n",
    "\n",
    "<div style=\"background-color: #ffebee; padding: 20px; border-radius: 5px; border-left: 5px solid #d32f2f;\">\n",
    "<b>üö® CRITICAL COMPLIANCE CHECK</b><br>\n",
    "This section is MANDATORY for deployment. Failure = regulatory violation!\n",
    "</div>\n",
    "\n",
    "### Create DBDataset & Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî¨ Setting up DeepBridge for comprehensive validation...\\n\")\n",
    "\n",
    "# Create DBDataset\n",
    "dataset = DBDataset(\n",
    "    data=df_encoded,\n",
    "    target_column='approved',\n",
    "    model=model,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    dataset_name='SecureBank Credit Scoring Model'\n",
    ")\n",
    "\n",
    "# Create Experiment with protected attributes\n",
    "exp = Experiment(\n",
    "    dataset=dataset,\n",
    "    experiment_type='binary_classification',\n",
    "    experiment_name='Credit Scoring Production Validation',\n",
    "    protected_attributes=['gender', 'race'],  # ‚Üê CRITICAL!\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"‚úÖ DeepBridge setup complete!\")\n",
    "print(f\"   Protected attributes: {exp.protected_attributes}\")\n",
    "print(f\"   Experiment: {exp.experiment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Complete Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öñÔ∏è  EXECUTING COMPLETE FAIRNESS ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n   üìä Calculating 15+ fairness metrics\")\n",
    "print(\"   üõ°Ô∏è  Verifying EEOC 80% Rule compliance\")\n",
    "print(\"   üë• Analyzing by group (gender, race)\")\n",
    "print(\"   üìà Threshold impact analysis\")\n",
    "print(\"\\n‚è≥ This may take a few minutes...\\n\")\n",
    "\n",
    "# Run fairness tests\n",
    "fairness_result = exp.run_fairness_tests(config='full')\n",
    "\n",
    "print(\"\\n‚úÖ Fairness analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EEOC 80% Rule Compliance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öñÔ∏è  EEOC 80% RULE COMPLIANCE CHECK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get predictions for test set\n",
    "test_indices = X_test.index\n",
    "test_gender = df.loc[test_indices, 'gender']\n",
    "test_race = df.loc[test_indices, 'race']\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Gender analysis\n",
    "print(\"\\nüë• GENDER ANALYSIS:\")\n",
    "male_approval = y_pred_test[test_gender == 'Male'].mean()\n",
    "female_approval = y_pred_test[test_gender == 'Female'].mean()\n",
    "di_gender = female_approval / male_approval if male_approval > 0 else 0\n",
    "\n",
    "print(f\"   Male approval rate: {male_approval:.1%}\")\n",
    "print(f\"   Female approval rate: {female_approval:.1%}\")\n",
    "print(f\"   Disparate Impact: {di_gender:.3f}\")\n",
    "print(f\"   Status: {'‚úÖ PASS' if di_gender >= 0.8 else '‚ùå FAIL'} (threshold = 0.80)\")\n",
    "\n",
    "if di_gender < 0.8:\n",
    "    print(f\"   ‚ö†Ô∏è  ACTION REQUIRED: Model discriminates by gender!\")\n",
    "    print(f\"   Gap: {abs(male_approval - female_approval):.1%}\")\n",
    "\n",
    "# Race analysis\n",
    "print(\"\\nüåç RACE ANALYSIS:\")\n",
    "race_groups = test_race.unique()\n",
    "race_approvals = {}\n",
    "\n",
    "for race in race_groups:\n",
    "    mask = test_race == race\n",
    "    if mask.sum() > 0:\n",
    "        race_approvals[race] = y_pred_test[mask].mean()\n",
    "\n",
    "# Reference: highest approval rate\n",
    "max_race = max(race_approvals, key=race_approvals.get)\n",
    "max_approval = race_approvals[max_race]\n",
    "\n",
    "print(f\"   Reference group: {max_race} ({max_approval:.1%})\\n\")\n",
    "\n",
    "race_pass = True\n",
    "for race in sorted(race_approvals.keys()):\n",
    "    approval = race_approvals[race]\n",
    "    di = approval / max_approval if max_approval > 0 else 0\n",
    "    status = \"‚úÖ\" if di >= 0.8 else \"‚ùå\"\n",
    "    if di < 0.8:\n",
    "        race_pass = False\n",
    "    print(f\"   {status} {race}: {approval:.1%} (DI = {di:.3f})\")\n",
    "\n",
    "# Final verdict\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "all_pass = di_gender >= 0.8 and race_pass\n",
    "\n",
    "if all_pass:\n",
    "    print(\"\\nüéâ ‚úÖ EEOC COMPLIANCE: APPROVED\")\n",
    "    print(\"   Model passes 80% Rule for gender and race\")\n",
    "    print(\"   ‚úÖ Model can proceed to production (from fairness perspective)\")\n",
    "else:\n",
    "    print(\"\\nüö® ‚ùå EEOC COMPLIANCE: FAILED\")\n",
    "    print(\"   ‚ö†Ô∏è  MODEL CANNOT GO TO PRODUCTION\")\n",
    "    print(\"\\n   üìã Required Actions:\")\n",
    "    print(\"      1. Remove or reduce bias in training data\")\n",
    "    print(\"      2. Apply fairness constraints during training\")\n",
    "    print(\"      3. Use bias mitigation techniques (reweighting, etc.)\")\n",
    "    print(\"      4. Consider removing protected attributes from features\")\n",
    "    print(\"      5. Consult legal and compliance teams\")\n",
    "    print(\"      6. Re-validate after changes\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices by Protected Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for gender\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "y_test_subset = y_test.loc[test_indices]\n",
    "\n",
    "for idx, gender in enumerate(['Male', 'Female']):\n",
    "    mask = test_gender == gender\n",
    "    cm = confusion_matrix(y_test_subset[mask], y_pred_test[mask])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Rejected', 'Approved'],\n",
    "                yticklabels=['Rejected', 'Approved'],\n",
    "                ax=axes[idx], cbar_kws={'label': 'Count'})\n",
    "    axes[idx].set_title(f'Confusion Matrix - {gender}', fontsize=13, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Actual')\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate TPR, FPR by gender\n",
    "print(\"\\nüìä PERFORMANCE METRICS BY GENDER:\")\n",
    "for gender in ['Male', 'Female']:\n",
    "    mask = test_gender == gender\n",
    "    cm = confusion_matrix(y_test_subset[mask], y_pred_test[mask])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n   {gender}:\")\n",
    "    print(f\"      True Positive Rate (Recall): {tpr:.3f}\")\n",
    "    print(f\"      False Positive Rate: {fpr:.3f}\")\n",
    "    print(f\"      Precision: {precision:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Continuing in next section...\n",
    "\n",
    "Due to notebook length, continuing with:\n",
    "- Section 8: Robustness Testing\n",
    "- Section 9: Uncertainty Quantification\n",
    "- Section 10: Drift Detection\n",
    "- Section 11-14: Reports, Decision, Monitoring, Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
