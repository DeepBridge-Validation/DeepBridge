{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ğŸ§ª Introduction to Validation Tests\n",
    "\n",
    "<div style=\"background-color: #e3f2fd; padding: 15px; border-radius: 5px; border-left: 5px solid #2196F3;\">\n",
    "<b>ğŸ““ Information</b><br>\n",
    "<b>Level:</b> Intermediate<br>\n",
    "<b>Time:</b> 15 minutes<br>\n",
    "<b>Dataset:</b> Breast Cancer (sklearn)<br>\n",
    "<b>Prerequisite:</b> 01_introduction/\n",
    "</div>\n",
    "\n",
    "## ğŸ¯ Objectives\n",
    "- âœ… Understand the 5 types of validation tests\n",
    "- âœ… Run all tests simultaneously\n",
    "- âœ… Compare quick/medium/full configurations\n",
    "- âœ… Interpret aggregated results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## ğŸ“š Why Validate Models?\n",
    "\n",
    "### âŒ Real Cases of Failures\n",
    "- **Amazon Recruiting System (2018)**: Discriminated against women\n",
    "- **COMPAS (Criminal Justice)**: Racial bias in recidivism prediction\n",
    "- **COVID-19 Models**: Failed after temporal drift\n",
    "\n",
    "### âœ… Benefits of Validation\n",
    "1. ğŸ›¡ï¸ **Robustness** - Resist perturbations\n",
    "2. ğŸ“Š **Confidence** - Quantify uncertainty\n",
    "3. ğŸ”„ **Resilience** - Detect drift\n",
    "4. âš™ï¸ **Optimization** - Hyperparameter importance\n",
    "5. âš–ï¸ **Fairness** - Ensure fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Setup and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from deepbridge import DBDataset, Experiment\n",
    "\n",
    "# Load data\n",
    "cancer = load_breast_cancer()\n",
    "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "df['target'] = cancer.target\n",
    "\n",
    "print(f\"ğŸ“Š Dataset: {df.shape}\")\n",
    "print(f\"ğŸ¯ Classes: {cancer.target_names}\")\n",
    "print(f\"ğŸ“ Features: {len(cancer.feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train RandomForest\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"âœ… Model trained!\")\n",
    "print(f\"ğŸ“Š Test accuracy: {clf.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Create DBDataset and Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBDataset with model\n",
    "dataset = DBDataset(\n",
    "    data=df,\n",
    "    target_column='target',\n",
    "    model=clf,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    dataset_name='Breast Cancer Classification'\n",
    ")\n",
    "\n",
    "# Experiment\n",
    "exp = Experiment(\n",
    "    dataset=dataset,\n",
    "    experiment_type='binary_classification',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"âœ… DBDataset and Experiment created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ The 5 Types of Tests\n",
    "\n",
    "### ğŸ›¡ï¸ 1. Robustness\n",
    "**What it tests**: Resistance to data perturbations\n",
    "**Why it matters**: Real-world data is noisy\n",
    "**Methods**: Gaussian noise, dropout, scaling\n",
    "\n",
    "### ğŸ“Š 2. Uncertainty\n",
    "**What it tests**: Uncertainty quantification in predictions\n",
    "**Why it matters**: Critical decisions need confidence\n",
    "**Method**: CRQR (Conformalized Quantile Regression)\n",
    "\n",
    "### ğŸ”„ 3. Resilience\n",
    "**What it tests**: Drift detection (data changes)\n",
    "**Why it matters**: Data changes over time\n",
    "**Types**: Covariate, Label, Concept, Temporal drift\n",
    "\n",
    "### âš™ï¸ 4. Hyperparameter\n",
    "**What it tests**: Importance of each hyperparameter\n",
    "**Why it matters**: Understand model sensitivity\n",
    "**Method**: Optuna (Bayesian optimization)\n",
    "\n",
    "### âš–ï¸ 5. Fairness\n",
    "**What it tests**: Bias in protected attributes\n",
    "**Why it matters**: Compliance and ethics\n",
    "**Metrics**: 15 metrics + EEOC compliance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Run ALL Tests\n",
    "\n",
    "<div style=\"background-color: #fff3e0; padding: 15px; border-radius: 5px; border-left: 5px solid #ff9800;\">\n",
    "<b>â±ï¸ Note:</b> This test may take a few minutes!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all tests with config='quick'\n",
    "print(\"ğŸ§ª Running all tests...\\n\")\n",
    "\n",
    "results = exp.run_tests(config_name='quick')\n",
    "\n",
    "print(\"\\nâœ… All tests completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results summary\n",
    "print(\"ğŸ“Š TEST SUMMARY\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "for test_name, test_result in results.items():\n",
    "    print(f\"\\nğŸ§ª {test_name.upper()}:\")\n",
    "    if hasattr(test_result, 'summary'):\n",
    "        summary = test_result.summary()\n",
    "        print(summary)\n",
    "    else:\n",
    "        print(f\"  Status: âœ… Executed\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Configurations: quick vs medium vs full\n",
    "\n",
    "### ğŸš€ quick (Default)\n",
    "- **Time**: Seconds to a few minutes\n",
    "- **Use**: Fast development, iteration\n",
    "- **Coverage**: Basic\n",
    "\n",
    "### âš¡ medium\n",
    "- **Time**: Minutes\n",
    "- **Use**: Intermediate validation\n",
    "- **Coverage**: Moderate\n",
    "\n",
    "### ğŸ”¬ full\n",
    "- **Time**: May take a while\n",
    "- **Use**: Final validation before production\n",
    "- **Coverage**: Complete and exhaustive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare execution time\n",
    "import time\n",
    "\n",
    "configs = ['quick']\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"\\nğŸ§ª Testing config='{config}'...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # Run only robustness for comparison\n",
    "    result = exp.run_test('robustness', config_name=config)\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    print(f\"â±ï¸  Time: {elapsed:.2f}s\")\n",
    "    print(f\"ğŸ“Š Robustness Score: {result.get('robustness_score', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ Run Individual Tests\n",
    "\n",
    "You can also run tests individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Only robustness\n",
    "rob_result = exp.run_test('robustness', config_name='quick')\n",
    "print(\"âœ… Robustness test completed!\")\n",
    "print(f\"ğŸ“Š Score: {rob_result.get('robustness_score', 'N/A')}\")\n",
    "\n",
    "# Other available tests:\n",
    "# exp.run_test('uncertainty', config_name='quick')\n",
    "# exp.run_test('resilience', config_name='quick')\n",
    "# exp.run_test('hyperparameter', config_name='quick')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## ğŸ‰ Conclusion\n",
    "\n",
    "### What you learned:\n",
    "- âœ… **5 types of tests**: Robustness, Uncertainty, Resilience, Hyperparameter, Fairness\n",
    "- âœ… **Run all together**: `run_tests()`\n",
    "- âœ… **Configurations**: quick/medium/full\n",
    "- âœ… **Individual tests**: `run_test(name)`\n",
    "\n",
    "### Why this matters:\n",
    "- ğŸ›¡ï¸ **Confidence** - Model validated across multiple dimensions\n",
    "- âš¡ **Speed** - Validation that would take weeks done in minutes\n",
    "- ğŸ“Š **Complete** - Professional coverage\n",
    "- âœ… **Production** - Ready for deployment with confidence\n",
    "\n",
    "### Next Steps:\n",
    "- ğŸ“˜ `02_complete_robustness.ipynb` - Deep dive into robustness\n",
    "- ğŸ“˜ `03_uncertainty.ipynb` - Uncertainty quantification\n",
    "- ğŸ“˜ `04_resilience_drift.ipynb` - Drift detection\n",
    "\n",
    "<div style=\"background-color: #e8f5e9; padding: 15px; border-radius: 5px; border-left: 5px solid #4caf50;\">\n",
    "<b>ğŸ’¡ Tip:</b> In production, run <code>config='full'</code> before final deployment!\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
