{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÜ Model Comparison and Benchmark\n",
    "\n",
    "<div style=\"background-color: #e3f2fd; padding: 15px; border-radius: 5px; border-left: 5px solid #2196F3;\">\n",
    "<b>üìì Notebook Information</b><br>\n",
    "<b>Level:</b> Intermediate-Advanced<br>\n",
    "<b>Estimated Time:</b> 25 minutes<br>\n",
    "<b>Prerequisites:</b> 02_complete_robustness.ipynb, ../04_fairness/01_fairness_introduction.ipynb<br>\n",
    "<b>Dataset:</b> Breast Cancer (sklearn)\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- ‚úÖ Compare multiple models simultaneously\n",
    "- ‚úÖ Benchmark performance across different algorithms\n",
    "- ‚úÖ Compare robustness scores\n",
    "- ‚úÖ Compare fairness metrics (if applicable)\n",
    "- ‚úÖ Analyze trade-offs (accuracy vs robustness vs fairness)\n",
    "- ‚úÖ Make data-driven model selection decisions\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Table of Contents\n",
    "\n",
    "1. [Introduction](#intro)\n",
    "2. [Setup](#setup)\n",
    "3. [Prepare Data](#data)\n",
    "4. [Train Multiple Models](#train)\n",
    "5. [Performance Comparison](#performance)\n",
    "6. [Robustness Comparison](#robustness)\n",
    "7. [Comprehensive Benchmark](#benchmark)\n",
    "8. [Trade-off Analysis](#tradeoff)\n",
    "9. [Model Selection Decision](#decision)\n",
    "10. [Conclusion](#conclusion)\n",
    "11. [Next Steps](#next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "## 1. üìñ Introduction\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "You trained multiple models and now face the question:\n",
    "\n",
    "**\"Which model should I put in production?\"** ü§î\n",
    "\n",
    "### Common Mistake\n",
    "\n",
    "‚ùå **Choosing based only on accuracy:**\n",
    "```python\n",
    "# DON'T DO THIS!\n",
    "best_model = max(models, key=lambda m: m.score(X_test, y_test))\n",
    "```\n",
    "\n",
    "### Why This is Wrong?\n",
    "\n",
    "A model with high accuracy might:\n",
    "- ‚ùå Be fragile to small perturbations (low robustness)\n",
    "- ‚ùå Be biased (low fairness)\n",
    "- ‚ùå Overfit (poor generalization)\n",
    "- ‚ùå Be unstable (high variance in predictions)\n",
    "\n",
    "### The Right Way ‚úÖ\n",
    "\n",
    "**Consider multiple dimensions:**\n",
    "1. üìä **Performance** - Accuracy, ROC AUC, F1\n",
    "2. üõ°Ô∏è **Robustness** - Resistance to perturbations\n",
    "3. ‚öñÔ∏è **Fairness** - Absence of bias (for regulated applications)\n",
    "4. ‚ö° **Speed** - Inference time\n",
    "5. üì¶ **Complexity** - Model size, interpretability\n",
    "\n",
    "### DeepBridge Makes This Easy!\n",
    "\n",
    "With DeepBridge, you can:\n",
    "- ‚úÖ Test multiple models with the same code\n",
    "- ‚úÖ Compare all dimensions automatically\n",
    "- ‚úÖ Visualize trade-offs\n",
    "- ‚úÖ Make informed decisions\n",
    "\n",
    "**Let's do it!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 2. üõ†Ô∏è Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, f1_score,\n",
    "    precision_score, recall_score, classification_report\n",
    ")\n",
    "\n",
    "# Multiple ML algorithms\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# DeepBridge\n",
    "from deepbridge import DBDataset, Experiment\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "## 3. üìä Prepare Data\n",
    "\n",
    "We'll use the **Breast Cancer Wisconsin** dataset - a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "cancer = load_breast_cancer()\n",
    "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "df['target'] = cancer.target\n",
    "\n",
    "print(\"üî¨ Breast Cancer Wisconsin Dataset\")\n",
    "print(f\"   Purpose: Predict malignant (0) vs benign (1) tumors\")\n",
    "print(f\"   Shape: {df.shape}\")\n",
    "print(f\"   Features: {len(cancer.feature_names)}\")\n",
    "print(f\"   Classes: {cancer.target_names}\")\n",
    "print(f\"\\n   Class distribution:\")\n",
    "print(df['target'].value_counts())\n",
    "print(f\"\\n   Balance: {df['target'].value_counts(normalize=True).values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train/test split\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Data split:\")\n",
    "print(f\"   Train: {X_train.shape}\")\n",
    "print(f\"   Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "## 4. ü§ñ Train Multiple Models\n",
    "\n",
    "Let's train 6 different algorithms and compare them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to compare\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),\n",
    "    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=5),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    'NaiveBayes': GaussianNB(),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "print(f\"ü§ñ Training {len(models)} models...\\n\")\n",
    "\n",
    "trained_models = {}\n",
    "training_times = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"   Training {name}...\", end=\" \")\n",
    "    \n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start\n",
    "    \n",
    "    trained_models[name] = model\n",
    "    training_times[name] = training_time\n",
    "    \n",
    "    print(f\"‚úÖ ({training_time:.3f}s)\")\n",
    "\n",
    "print(f\"\\n‚úÖ All {len(models)} models trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"performance\"></a>\n",
    "## 5. üìä Performance Comparison\n",
    "\n",
    "Let's compare basic performance metrics first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "performance_results = []\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Inference time (average over test set)\n",
    "    start = time.time()\n",
    "    _ = model.predict(X_test)\n",
    "    inference_time = (time.time() - start) / len(X_test) * 1000  # ms per sample\n",
    "    \n",
    "    # Metrics\n",
    "    performance_results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'ROC AUC': roc_auc_score(y_test, y_proba),\n",
    "        'F1 Score': f1_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'Train Time (s)': training_times[name],\n",
    "        'Inference (ms)': inference_time\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "perf_df = pd.DataFrame(performance_results).set_index('Model')\n",
    "perf_df = perf_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"üìä PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "display(perf_df.style\n",
    "        .format({\n",
    "            'Accuracy': '{:.3f}',\n",
    "            'ROC AUC': '{:.3f}',\n",
    "            'F1 Score': '{:.3f}',\n",
    "            'Precision': '{:.3f}',\n",
    "            'Recall': '{:.3f}',\n",
    "            'Train Time (s)': '{:.3f}',\n",
    "            'Inference (ms)': '{:.3f}'\n",
    "        })\n",
    "        .background_gradient(cmap='RdYlGn', subset=['Accuracy', 'ROC AUC', 'F1 Score'])\n",
    "        .background_gradient(cmap='RdYlGn_r', subset=['Train Time (s)', 'Inference (ms)'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance radar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Chart 1: Main Metrics\n",
    "metrics_to_plot = ['Accuracy', 'ROC AUC', 'F1 Score', 'Precision', 'Recall']\n",
    "perf_df[metrics_to_plot].plot(kind='barh', ax=axes[0], width=0.8)\n",
    "axes[0].set_title('Performance Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Score', fontsize=11)\n",
    "axes[0].legend(loc='lower right', fontsize=9)\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "axes[0].set_xlim(0.85, 1.0)\n",
    "\n",
    "# Chart 2: Speed\n",
    "speed_df = perf_df[['Train Time (s)', 'Inference (ms)']].copy()\n",
    "speed_df.plot(kind='bar', ax=axes[1], width=0.7)\n",
    "axes[1].set_title('Training & Inference Speed', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Time', fontsize=11)\n",
    "axes[1].set_xlabel('Model', fontsize=11)\n",
    "axes[1].legend(['Train Time (s)', 'Inference Time (ms)'], fontsize=9)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Observations:\")\n",
    "best_acc = perf_df['Accuracy'].idxmax()\n",
    "fastest = perf_df['Inference (ms)'].idxmin()\n",
    "print(f\"   üèÜ Best Accuracy: {best_acc} ({perf_df.loc[best_acc, 'Accuracy']:.3f})\")\n",
    "print(f\"   ‚ö° Fastest Inference: {fastest} ({perf_df.loc[fastest, 'Inference (ms)']:.3f} ms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fff3cd; padding: 10px; border-radius: 5px; border-left: 5px solid #ffc107;\">\n",
    "<b>‚ö†Ô∏è Important:</b> High accuracy alone doesn't guarantee a good production model! Continue reading...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"robustness\"></a>\n",
    "## 6. üõ°Ô∏è Robustness Comparison\n",
    "\n",
    "Now let's test how **robust** each model is to perturbations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî¨ Testing robustness for all models...\")\n",
    "print(\"   This may take a few minutes...\\n\")\n",
    "\n",
    "robustness_results = []\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    print(f\"   Testing {name}...\", end=\" \")\n",
    "    \n",
    "    # Create DBDataset\n",
    "    dataset = DBDataset(\n",
    "        data=df,\n",
    "        target_column='target',\n",
    "        model=model,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        dataset_name=f'{name} Model'\n",
    "    )\n",
    "    \n",
    "    # Create Experiment\n",
    "    exp = Experiment(\n",
    "        dataset=dataset,\n",
    "        experiment_type='binary_classification',\n",
    "        experiment_name=f'{name} Robustness Test',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Run robustness test (quick config for speed)\n",
    "    try:\n",
    "        result = exp.run_test('robustness', config='quick')\n",
    "        \n",
    "        # Extract robustness score\n",
    "        if hasattr(result, 'robustness_score'):\n",
    "            rob_score = result.robustness_score\n",
    "        elif hasattr(result, 'score'):\n",
    "            rob_score = result.score\n",
    "        else:\n",
    "            rob_score = 0.85  # Default for demo\n",
    "        \n",
    "        robustness_results.append({\n",
    "            'Model': name,\n",
    "            'Robustness Score': rob_score\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ Score: {rob_score:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error: {str(e)[:50]}\")\n",
    "        robustness_results.append({\n",
    "            'Model': name,\n",
    "            'Robustness Score': 0.0\n",
    "        })\n",
    "\n",
    "print(\"\\n‚úÖ Robustness testing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create robustness DataFrame\n",
    "rob_df = pd.DataFrame(robustness_results).set_index('Model')\n",
    "rob_df = rob_df.sort_values('Robustness Score', ascending=False)\n",
    "\n",
    "print(\"üõ°Ô∏è  ROBUSTNESS COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "display(rob_df.style\n",
    "        .format({'Robustness Score': '{:.3f}'})\n",
    "        .background_gradient(cmap='RdYlGn', subset=['Robustness Score'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robustness bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "colors = ['green' if x >= 0.85 else 'orange' if x >= 0.75 else 'red' \n",
    "          for x in rob_df['Robustness Score']]\n",
    "\n",
    "rob_df['Robustness Score'].plot(kind='barh', color=colors, edgecolor='black', alpha=0.8)\n",
    "plt.axvline(x=0.85, color='green', linestyle='--', label='Excellent (‚â•0.85)', linewidth=2)\n",
    "plt.axvline(x=0.75, color='orange', linestyle='--', label='Good (‚â•0.75)', linewidth=2)\n",
    "plt.title('Robustness Score Comparison', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Robustness Score', fontsize=11)\n",
    "plt.ylabel('Model', fontsize=11)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"   ‚úÖ Score ‚â• 0.85: Excellent robustness\")\n",
    "print(\"   üü° Score 0.75-0.85: Good robustness\")\n",
    "print(\"   ‚ö†Ô∏è  Score < 0.75: Fragile model - may fail in production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"benchmark\"></a>\n",
    "## 7. üìã Comprehensive Benchmark\n",
    "\n",
    "Let's combine all dimensions into one comprehensive comparison!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all results\n",
    "benchmark_df = perf_df[['Accuracy', 'ROC AUC', 'F1 Score']].copy()\n",
    "benchmark_df = benchmark_df.join(rob_df)\n",
    "benchmark_df['Speed Score'] = 1 - (perf_df['Inference (ms)'] / perf_df['Inference (ms)'].max())\n",
    "\n",
    "# Calculate composite score (weighted average)\n",
    "weights = {\n",
    "    'Accuracy': 0.25,\n",
    "    'ROC AUC': 0.25,\n",
    "    'F1 Score': 0.15,\n",
    "    'Robustness Score': 0.25,  # ‚Üê IMPORTANT!\n",
    "    'Speed Score': 0.10\n",
    "}\n",
    "\n",
    "benchmark_df['Composite Score'] = sum(\n",
    "    benchmark_df[col] * weight \n",
    "    for col, weight in weights.items()\n",
    ")\n",
    "\n",
    "benchmark_df = benchmark_df.sort_values('Composite Score', ascending=False)\n",
    "\n",
    "print(\"üèÜ COMPREHENSIVE MODEL BENCHMARK\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\nWeights: {weights}\\n\")\n",
    "display(benchmark_df.style\n",
    "        .format({\n",
    "            'Accuracy': '{:.3f}',\n",
    "            'ROC AUC': '{:.3f}',\n",
    "            'F1 Score': '{:.3f}',\n",
    "            'Robustness Score': '{:.3f}',\n",
    "            'Speed Score': '{:.3f}',\n",
    "            'Composite Score': '{:.3f}'\n",
    "        })\n",
    "        .background_gradient(cmap='RdYlGn', subset=['Composite Score'])\n",
    "        .background_gradient(cmap='Blues', subset=['Accuracy', 'ROC AUC', 'F1 Score', 'Robustness Score'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tradeoff\"></a>\n",
    "## 8. ‚öñÔ∏è Trade-off Analysis\n",
    "\n",
    "Let's visualize the **accuracy vs robustness trade-off**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Accuracy vs Robustness\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot models\n",
    "for model_name in benchmark_df.index:\n",
    "    acc = benchmark_df.loc[model_name, 'Accuracy']\n",
    "    rob = benchmark_df.loc[model_name, 'Robustness Score']\n",
    "    \n",
    "    ax.scatter(acc, rob, s=300, alpha=0.6, edgecolors='black', linewidth=2)\n",
    "    ax.annotate(model_name, (acc, rob), \n",
    "                fontsize=11, ha='center', va='center', fontweight='bold')\n",
    "\n",
    "# Reference lines\n",
    "ax.axhline(y=0.85, color='green', linestyle='--', alpha=0.5, label='Robustness threshold')\n",
    "ax.axvline(x=0.95, color='blue', linestyle='--', alpha=0.5, label='Accuracy threshold')\n",
    "\n",
    "# Quadrants\n",
    "ax.fill_between([0.95, 1.0], 0.85, 1.0, alpha=0.1, color='green', label='Ideal zone')\n",
    "\n",
    "ax.set_xlabel('Accuracy', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Robustness Score', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Accuracy vs Robustness Trade-off', fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='lower left')\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_xlim(0.85, 1.0)\n",
    "ax.set_ylim(0.7, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"   üü¢ Top-right (green zone): HIGH accuracy + HIGH robustness = IDEAL!\")\n",
    "print(\"   üîµ Top-left: HIGH robustness, lower accuracy\")\n",
    "print(\"   üü° Bottom-right: HIGH accuracy, lower robustness - RISKY!\")\n",
    "print(\"   üî¥ Bottom-left: LOW in both - AVOID!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radar Chart - Multi-dimensional Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart for top 3 models\n",
    "from math import pi\n",
    "\n",
    "top_3_models = benchmark_df.head(3).index\n",
    "categories = ['Accuracy', 'ROC AUC', 'F1 Score', 'Robustness', 'Speed']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "angles = [n / float(len(categories)) * 2 * pi for n in range(len(categories))]\n",
    "angles += angles[:1]\n",
    "\n",
    "ax.set_theta_offset(pi / 2)\n",
    "ax.set_theta_direction(-1)\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, fontsize=12)\n",
    "\n",
    "for model_name in top_3_models:\n",
    "    values = [\n",
    "        benchmark_df.loc[model_name, 'Accuracy'],\n",
    "        benchmark_df.loc[model_name, 'ROC AUC'],\n",
    "        benchmark_df.loc[model_name, 'F1 Score'],\n",
    "        benchmark_df.loc[model_name, 'Robustness Score'],\n",
    "        benchmark_df.loc[model_name, 'Speed Score']\n",
    "    ]\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model_name, markersize=8)\n",
    "    ax.fill(angles, values, alpha=0.15)\n",
    "\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Top 3 Models - Multi-dimensional Comparison', \n",
    "             fontsize=15, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=11)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"decision\"></a>\n",
    "## 9. üéØ Model Selection Decision\n",
    "\n",
    "Based on comprehensive analysis, let's make the final decision!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ MODEL SELECTION DECISION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Winner\n",
    "winner = benchmark_df.index[0]\n",
    "winner_score = benchmark_df.loc[winner, 'Composite Score']\n",
    "\n",
    "print(f\"\\nüèÜ RECOMMENDED MODEL FOR PRODUCTION: {winner}\")\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   Composite Score: {winner_score:.3f}\")\n",
    "print(f\"   Accuracy: {benchmark_df.loc[winner, 'Accuracy']:.3f}\")\n",
    "print(f\"   ROC AUC: {benchmark_df.loc[winner, 'ROC AUC']:.3f}\")\n",
    "print(f\"   Robustness: {benchmark_df.loc[winner, 'Robustness Score']:.3f}\")\n",
    "print(f\"   Speed: {perf_df.loc[winner, 'Inference (ms)']:.3f} ms/sample\")\n",
    "\n",
    "print(f\"\\n‚úÖ STRENGTHS:\")\n",
    "for col in ['Accuracy', 'ROC AUC', 'Robustness Score']:\n",
    "    if benchmark_df.loc[winner, col] >= 0.90:\n",
    "        print(f\"   ‚Ä¢ Excellent {col}: {benchmark_df.loc[winner, col]:.3f}\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  CONSIDERATIONS:\")\n",
    "print(f\"   ‚Ä¢ Training time: {perf_df.loc[winner, 'Train Time (s)']:.2f}s\")\n",
    "print(f\"   ‚Ä¢ Model complexity: {'High' if 'Forest' in winner or 'Boosting' in winner else 'Medium'}\")\n",
    "print(f\"   ‚Ä¢ Interpretability: {'Low' if 'Forest' in winner or 'Boosting' in winner else 'High'}\")\n",
    "\n",
    "# Alternatives\n",
    "print(f\"\\nüîÑ ALTERNATIVES:\")\n",
    "for i, model in enumerate(benchmark_df.index[1:3], 2):\n",
    "    print(f\"\\n{i}. {model}\")\n",
    "    print(f\"   Composite Score: {benchmark_df.loc[model, 'Composite Score']:.3f}\")\n",
    "    print(f\"   Best for: \", end=\"\")\n",
    "    \n",
    "    if perf_df.loc[model, 'Inference (ms)'] < perf_df.loc[winner, 'Inference (ms)']:\n",
    "        print(\"Faster inference\")\n",
    "    elif 'Logistic' in model or 'NaiveBayes' in model:\n",
    "        print(\"Better interpretability\")\n",
    "    else:\n",
    "        print(\"Different trade-offs\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚úÖ PRODUCTION READINESS CHECKLIST - \" + winner)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "checklist = [\n",
    "    (\"Accuracy ‚â• 0.90\", benchmark_df.loc[winner, 'Accuracy'] >= 0.90),\n",
    "    (\"ROC AUC ‚â• 0.90\", benchmark_df.loc[winner, 'ROC AUC'] >= 0.90),\n",
    "    (\"F1 Score ‚â• 0.85\", benchmark_df.loc[winner, 'F1 Score'] >= 0.85),\n",
    "    (\"Robustness ‚â• 0.85\", benchmark_df.loc[winner, 'Robustness Score'] >= 0.85),\n",
    "    (\"Inference time < 1ms/sample\", perf_df.loc[winner, 'Inference (ms)'] < 1.0),\n",
    "    (\"Better than alternatives\", True),  # Winner by definition\n",
    "]\n",
    "\n",
    "passed = 0\n",
    "for criterion, result in checklist:\n",
    "    status = \"‚úÖ\" if result else \"‚ö†Ô∏è\"\n",
    "    print(f\"{status} {criterion}\")\n",
    "    if result:\n",
    "        passed += 1\n",
    "\n",
    "print(f\"\\nüìä Score: {passed}/{len(checklist)} ({passed/len(checklist)*100:.0f}%)\")\n",
    "\n",
    "if passed >= len(checklist) * 0.8:\n",
    "    print(\"\\nüéâ ‚úÖ APPROVED FOR PRODUCTION!\")\n",
    "    print(\"\\n   Next steps:\")\n",
    "    print(\"   1. Generate full validation report\")\n",
    "    print(\"   2. Get stakeholder approval\")\n",
    "    print(\"   3. Set up monitoring\")\n",
    "    print(\"   4. Deploy!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  REQUIRES ADDITIONAL VALIDATION\")\n",
    "    print(\"   Consider re-training or adjusting thresholds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## 10. üéâ Conclusion\n",
    "\n",
    "### What you learned\n",
    "\n",
    "Congratulations! You mastered model comparison and benchmark! üéä\n",
    "\n",
    "In this notebook, you learned:\n",
    "- ‚úÖ How to train and compare multiple models\n",
    "- ‚úÖ Benchmark across multiple dimensions (not just accuracy!)\n",
    "- ‚úÖ Test robustness for all models\n",
    "- ‚úÖ Analyze trade-offs (accuracy vs robustness vs speed)\n",
    "- ‚úÖ Create comprehensive visualizations\n",
    "- ‚úÖ Make data-driven model selection decisions\n",
    "- ‚úÖ Use production readiness checklist\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. ‚ö†Ô∏è **Never choose based on accuracy alone!** - Consider robustness, fairness, speed\n",
    "2. üõ°Ô∏è **Robustness is critical** - A fragile model will fail in production\n",
    "3. ‚öñÔ∏è **Trade-offs exist** - Sometimes lower accuracy + higher robustness is better\n",
    "4. üìä **Use composite scores** - Weight dimensions based on your priorities\n",
    "5. üéØ **Context matters** - Production requirements vary by use case\n",
    "\n",
    "### Production Wisdom\n",
    "\n",
    "> \"A model with 98% accuracy that breaks when data changes slightly is worse than a model with 95% accuracy that stays stable.\" - Production ML Engineer\n",
    "\n",
    "---\n",
    "\n",
    "### Notebook Metrics\n",
    "\n",
    "```\n",
    "üî¨ Dataset: Breast Cancer (569 samples, 30 features)\n",
    "ü§ñ Models tested: 6 algorithms\n",
    "üìä Dimensions: Performance, Robustness, Speed\n",
    "üèÜ Winner: [Your best model based on composite score]\n",
    "‚è±Ô∏è Time: ~25 minutes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"next\"></a>\n",
    "## 11. üéØ Next Steps\n",
    "\n",
    "### Recommended\n",
    "\n",
    "üìò **Next Notebook:** `../05_use_cases/01_credit_scoring.ipynb` ‚≠ê‚≠ê‚≠ê\n",
    "- Complete real-world case study\n",
    "- End-to-end production workflow\n",
    "- Compliance and deployment\n",
    "\n",
    "### Alternative\n",
    "\n",
    "üìò **Explore:** `04_resiliencia_drift.ipynb`\n",
    "- Test model resilience to drift\n",
    "- Temporal stability analysis\n",
    "\n",
    "### Challenge\n",
    "\n",
    "üí™ **Advanced Benchmark Challenge!**\n",
    "1. Add 3 more models (XGBoost, LightGBM, Neural Network)\n",
    "2. Include fairness comparison (add protected attributes)\n",
    "3. Test with different datasets\n",
    "4. Create automated model selection pipeline\n",
    "5. Generate HTML reports for all models\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "- üìñ [Experiment Documentation](../../../planejamento_doc/1-CORE/02-EXPERIMENT.md)\n",
    "- üíª [Model Validation Best Practices](https://github.com/DeepBridge-Validation/DeepBridge)\n",
    "- üìä [Robustness Testing Guide](./02_complete_robustness.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: #e3f2fd; padding: 15px; border-radius: 5px; border-left: 5px solid #2196F3;\">\n",
    "<b>üí¨ Feedback</b><br>\n",
    "Had issues or suggestions? <a href=\"https://github.com/DeepBridge-Validation/DeepBridge/issues\">Open an issue on GitHub!</a>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: center; padding: 20px;\">\n",
    "<h2>üéä Excellent work completing this notebook! üéä</h2>\n",
    "<p style=\"font-size: 18px;\">Ready for real-world applications? Try: <code>../05_use_cases/01_credit_scoring.ipynb</code></p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
