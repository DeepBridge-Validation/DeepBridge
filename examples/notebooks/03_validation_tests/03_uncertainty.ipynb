{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# üìä Uncertainty Quantification\n",
    "\n",
    "<div style=\"background-color: #e3f2fd; padding: 15px; border-radius: 5px; border-left: 5px solid #2196F3;\">\n",
    "<b>üìì Information</b><br>\n",
    "<b>Level:</b> Intermediate/Advanced<br>\n",
    "<b>Time:</b> 20 minutes<br>\n",
    "<b>Dataset:</b> California Housing (sklearn)<br>\n",
    "<b>Prerequisite:</b> 01_tests_introduction.ipynb\n",
    "</div>\n",
    "\n",
    "## üéØ Objectives\n",
    "- ‚úÖ Understand why uncertainty matters\n",
    "- ‚úÖ Learn about CRQR (Conformalized Quantile Regression)\n",
    "- ‚úÖ Generate confidence intervals\n",
    "- ‚úÖ Analyze probability calibration\n",
    "- ‚úÖ Make uncertainty-based decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## üìö Why Does Uncertainty Matter?\n",
    "\n",
    "### Critical Contexts\n",
    "\n",
    "#### üè• Medicine\n",
    "- **Problem**: Cancer diagnosis\n",
    "- **Solution**: \"90% confidence it's malignant\" vs \"not sure\"\n",
    "- **Impact**: Save lives with informed decisions\n",
    "\n",
    "#### üí∞ Finance\n",
    "- **Problem**: Credit approval\n",
    "- **Solution**: Quantify default risk\n",
    "- **Impact**: More informed risk decisions\n",
    "\n",
    "#### üöó Autonomous Vehicles\n",
    "- **Problem**: Pedestrian detection\n",
    "- **Solution**: Know when not confident\n",
    "- **Impact**: Increased safety\n",
    "\n",
    "### The Problem with Traditional Models\n",
    "```python\n",
    "# Traditional prediction\n",
    "prediction = model.predict(X)  # [0.92]\n",
    "# But how confident are we?\n",
    "# 92% ¬± 1%? or 92% ¬± 20%?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup - Regression Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from deepbridge import DBDataset, Experiment\n",
    "\n",
    "# Configure visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load data (house prices)\n",
    "housing = fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "df['target'] = housing.target  # Price in $100k\n",
    "\n",
    "# Use subset for speed\n",
    "df = df.sample(n=5000, random_state=42)\n",
    "\n",
    "print(f\"üìä Dataset: {df.shape}\")\n",
    "print(f\"üè† Target: House prices (in $100k)\")\n",
    "print(f\"\\nüìà Target statistics:\")\n",
    "print(df['target'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Train Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train RandomForest Regressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"‚úÖ Model trained!\")\n",
    "print(f\"üìä MAE: ${mae*100:.2f}k\")\n",
    "print(f\"üìä R¬≤: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DBDataset(\n",
    "    data=df,\n",
    "    target_column='target',\n",
    "    model=model,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    dataset_name='California Housing Prices'\n",
    ")\n",
    "\n",
    "exp = Experiment(\n",
    "    dataset=dataset,\n",
    "    experiment_type='regression',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Experiment created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Run Uncertainty Test\n",
    "\n",
    "<div style=\"background-color: #fff3e0; padding: 15px; border-radius: 5px; border-left: 5px solid #ff9800;\">\n",
    "<b>‚ÑπÔ∏è CRQR:</b> Conformalized Quantile Regression is an advanced technique for generating calibrated prediction intervals.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß™ Running uncertainty quantification test...\\n\")\n",
    "\n",
    "uncertainty_result = exp.run_test('uncertainty', config_name='quick')\n",
    "\n",
    "print(\"\\n‚úÖ Uncertainty test completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Confidence Intervals\n",
    "\n",
    "Now we have confidence intervals for each prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract intervals (if available)\n",
    "if 'prediction_intervals' in uncertainty_result:\n",
    "    intervals = uncertainty_result['prediction_intervals']\n",
    "    \n",
    "    # Example: first 5 predictions\n",
    "    print(\"\\nüìä CONFIDENCE INTERVALS (first 5 predictions):\\n\" + \"=\"*70)\n",
    "    \n",
    "    for i in range(min(5, len(intervals))):\n",
    "        lower = intervals[i]['lower_bound']\n",
    "        upper = intervals[i]['upper_bound']\n",
    "        prediction = intervals[i]['prediction']\n",
    "        actual = y_test.iloc[i] if i < len(y_test) else None\n",
    "        \n",
    "        print(f\"\\nüè† House {i+1}:\")\n",
    "        print(f\"   Prediction: ${prediction*100:.2f}k\")\n",
    "        print(f\"   Interval: [${lower*100:.2f}k, ${upper*100:.2f}k]\")\n",
    "        print(f\"   Width: ${(upper-lower)*100:.2f}k\")\n",
    "        if actual is not None:\n",
    "            print(f\"   Actual Value: ${actual*100:.2f}k\")\n",
    "            contains = lower <= actual <= upper\n",
    "            print(f\"   Contains actual? {'‚úÖ' if contains else '‚ùå'}\")\n",
    "else:\n",
    "    print(\"\\nüí° Prediction intervals:\\n\")\n",
    "    print(\"For each prediction, we have an interval [lower, upper]:\")\n",
    "    print(\"\\nExample:\")\n",
    "    print(\"  Prediction: $250k\")\n",
    "    print(\"  95% Interval: [$200k, $300k]\")\n",
    "    print(\"  Interpretation: 95% confidence that the actual value is within this interval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Coverage Analysis\n",
    "\n",
    "**Coverage** = % of actual values that fall within predicted intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'coverage' in uncertainty_result:\n",
    "    coverage = uncertainty_result['coverage']\n",
    "    target_coverage = uncertainty_result.get('target_coverage', 0.95)\n",
    "    \n",
    "    print(f\"\\nüìä COVERAGE ANALYSIS\\n\" + \"=\"*60)\n",
    "    print(f\"\\nüéØ Target Coverage: {target_coverage*100:.0f}%\")\n",
    "    print(f\"‚úÖ Achieved Coverage: {coverage*100:.1f}%\")\n",
    "    \n",
    "    # Evaluate quality\n",
    "    if abs(coverage - target_coverage) < 0.05:\n",
    "        print(\"\\nüü¢ EXCELLENT - Coverage is calibrated!\")\n",
    "    elif abs(coverage - target_coverage) < 0.10:\n",
    "        print(\"\\nüü° GOOD - Acceptable coverage\")\n",
    "    else:\n",
    "        print(\"\\nüî¥ WARNING - Coverage is miscalibrated\")\n",
    "        \n",
    "    print(f\"\\nüí° Interpretation:\")\n",
    "    print(f\"   {coverage*100:.1f}% of predictions contain the actual value in the interval\")\n",
    "else:\n",
    "    print(\"\\nüìä Expected coverage: ~95%\")\n",
    "    print(\"   Means that 95% of actual values should fall within the intervals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Visualize Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interval visualization\n",
    "n_samples = 30  # Show 30 samples\n",
    "\n",
    "# Simulate intervals if not available\n",
    "if 'prediction_intervals' not in uncertainty_result:\n",
    "    # Simulate for demonstration\n",
    "    predictions = y_pred[:n_samples]\n",
    "    std_pred = np.std(y_test[:n_samples] - predictions)\n",
    "    lower = predictions - 1.96 * std_pred\n",
    "    upper = predictions + 1.96 * std_pred\n",
    "    actual = y_test.iloc[:n_samples].values\n",
    "else:\n",
    "    intervals = uncertainty_result['prediction_intervals'][:n_samples]\n",
    "    predictions = np.array([i['prediction'] for i in intervals])\n",
    "    lower = np.array([i['lower_bound'] for i in intervals])\n",
    "    upper = np.array([i['upper_bound'] for i in intervals])\n",
    "    actual = y_test.iloc[:n_samples].values\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "x = np.arange(n_samples)\n",
    "\n",
    "# Confidence intervals\n",
    "plt.fill_between(x, lower, upper, alpha=0.3, color='skyblue', \n",
    "                 label='95% Confidence Interval')\n",
    "\n",
    "# Predictions\n",
    "plt.plot(x, predictions, 'o-', color='blue', label='Prediction', \n",
    "         markersize=6, linewidth=2)\n",
    "\n",
    "# Actual values\n",
    "plt.scatter(x, actual, color='red', marker='x', s=100, \n",
    "            label='Actual Value', zorder=5, linewidth=2)\n",
    "\n",
    "plt.xlabel('Sample', fontsize=12)\n",
    "plt.ylabel('Price ($100k)', fontsize=12)\n",
    "plt.title('Confidence Intervals vs Actual Values', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count how many actual values fall within intervals\n",
    "contains = np.sum((actual >= lower) & (actual <= upper))\n",
    "coverage_calc = contains / n_samples\n",
    "\n",
    "print(f\"\\nüìä In the {n_samples} samples shown:\")\n",
    "print(f\"   ‚úÖ {contains}/{n_samples} actual values within interval\")\n",
    "print(f\"   üìä Coverage: {coverage_calc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Probability Calibration\n",
    "\n",
    "For classification problems, we can also evaluate calibration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä PROBABILITY CALIBRATION\\n\" + \"=\"*60)\n",
    "\n",
    "print(\"\\nüí° What is calibration?\")\n",
    "print(\"   If the model says '70% chance', it should be right ~70% of the time\")\n",
    "print(\"\\n‚úÖ Calibrated model:\")\n",
    "print(\"   - Prob = 0.9 ‚Üí 90% accuracy\")\n",
    "print(\"   - Prob = 0.7 ‚Üí 70% accuracy\")\n",
    "print(\"   - Prob = 0.5 ‚Üí 50% accuracy\")\n",
    "\n",
    "print(\"\\n‚ùå Miscalibrated model:\")\n",
    "print(\"   - Prob = 0.9 ‚Üí 60% accuracy (overconfident)\")\n",
    "print(\"   - Prob = 0.5 ‚Üí 80% accuracy (underconfident)\")\n",
    "\n",
    "if 'calibration_score' in uncertainty_result:\n",
    "    cal_score = uncertainty_result['calibration_score']\n",
    "    print(f\"\\nüìä Calibration Score: {cal_score:.3f}\")\n",
    "    \n",
    "    if cal_score > 0.9:\n",
    "        print(\"üü¢ Excellent calibration!\")\n",
    "    elif cal_score > 0.7:\n",
    "        print(\"üü° Acceptable calibration\")\n",
    "    else:\n",
    "        print(\"üî¥ Calibration needs improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Uncertainty-Based Decisions\n",
    "\n",
    "How to use uncertainty in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüíº PRACTICAL USE EXAMPLES\\n\" + \"=\"*60)\n",
    "\n",
    "print(\"\\nüè• MEDICINE - Cancer Diagnosis\")\n",
    "print(\"   Prediction: 85% chance of cancer\")\n",
    "print(\"   Interval: [60%, 95%]\")\n",
    "print(\"   Action: ‚úÖ Perform biopsy (high uncertainty in critical decision)\")\n",
    "\n",
    "print(\"\\nüí∞ FINANCE - Credit Approval\")\n",
    "print(\"   Prediction: 40% chance of default\")\n",
    "print(\"   Interval: [35%, 45%]\")\n",
    "print(\"   Action: ‚úÖ Approve with adjusted rate (low uncertainty)\")\n",
    "\n",
    "print(\"\\nüöó AUTONOMOUS VEHICLES - Pedestrian Detection\")\n",
    "print(\"   Prediction: 70% chance it's a pedestrian\")\n",
    "print(\"   Interval: [30%, 90%]\")\n",
    "print(\"   Action: ‚ö†Ô∏è  Brake preventively (high uncertainty = caution)\")\n",
    "\n",
    "print(\"\\nüè† OUR EXAMPLE - House Price\")\n",
    "sample_idx = 0\n",
    "if 'prediction_intervals' in uncertainty_result and len(uncertainty_result['prediction_intervals']) > 0:\n",
    "    interval = uncertainty_result['prediction_intervals'][sample_idx]\n",
    "    pred = interval['prediction']\n",
    "    low = interval['lower_bound']\n",
    "    up = interval['upper_bound']\n",
    "    width = up - low\n",
    "else:\n",
    "    pred = y_pred[sample_idx]\n",
    "    std = np.std(y_test - y_pred[:len(y_test)])\n",
    "    low = pred - 1.96 * std\n",
    "    up = pred + 1.96 * std\n",
    "    width = up - low\n",
    "\n",
    "print(f\"   Prediction: ${pred*100:.2f}k\")\n",
    "print(f\"   Interval: [${low*100:.2f}k, ${up*100:.2f}k]\")\n",
    "print(f\"   Width: ${width*100:.2f}k\")\n",
    "\n",
    "if width < 1.0:  # $100k\n",
    "    print(f\"   Action: ‚úÖ High confidence - can use prediction directly\")\n",
    "else:\n",
    "    print(f\"   Action: ‚ö†Ô∏è  High uncertainty - consider range of values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## üîü Comparison: With vs Without Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä WITHOUT UNCERTAINTY QUANTIFICATION:\\n\" + \"=\"*60)\n",
    "print(\"‚ùå Prediction: $250k\")\n",
    "print(\"‚ùå Problem: We don't know how reliable it is!\")\n",
    "print(\"‚ùå Risk: Decisions without confidence context\")\n",
    "\n",
    "print(\"\\nüìä WITH UNCERTAINTY QUANTIFICATION:\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Prediction: $250k\")\n",
    "print(\"‚úÖ 95% Interval: [$200k, $300k]\")\n",
    "print(\"‚úÖ Benefit: We know the margin of error!\")\n",
    "print(\"‚úÖ Decisions: More informed and safe\")\n",
    "\n",
    "print(\"\\nüí° IMPACT:\")\n",
    "print(\"   üè• Medicine: Avoid wrong diagnoses\")\n",
    "print(\"   üí∞ Finance: Manage risk appropriately\")\n",
    "print(\"   üöó Autonomy: Prioritize safety in uncertain situations\")\n",
    "print(\"   üè† Real Estate: Negotiate with realistic value range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## üéâ Conclusion\n",
    "\n",
    "### What you learned:\n",
    "- ‚úÖ **Importance of Uncertainty** - Essential for critical decisions\n",
    "- ‚úÖ **CRQR** - State-of-the-art technique for calibrated intervals\n",
    "- ‚úÖ **Confidence Intervals** - Quantify margin of error\n",
    "- ‚úÖ **Coverage** - Validate interval calibration\n",
    "- ‚úÖ **Practical Applications** - How to use in different domains\n",
    "\n",
    "### Uncertainty Checklist for Production:\n",
    "- [ ] ‚úÖ Coverage close to target (e.g., 95%)\n",
    "- [ ] ‚úÖ Calibrated intervals\n",
    "- [ ] ‚úÖ Interval width acceptable for business\n",
    "- [ ] ‚úÖ Uncertainty-based decision rules defined\n",
    "- [ ] ‚úÖ Calibration monitoring in production\n",
    "\n",
    "### When to Use Uncertainty Quantification:\n",
    "- üè• **Always** in medical applications\n",
    "- üí∞ **Always** in financial decisions\n",
    "- üöó **Always** in safety systems\n",
    "- üìä **Recommended** in any critical application\n",
    "\n",
    "### Next Steps:\n",
    "- üìò `04_resilience_drift.ipynb` - Detect data changes\n",
    "- üìò `../04_fairness/` - Fairness and bias analysis\n",
    "\n",
    "<div style=\"background-color: #e8f5e9; padding: 15px; border-radius: 5px; border-left: 5px solid #4caf50;\">\n",
    "<b>üí° Remember:</b> \"Being wrong with confidence is worse than being uncertain\" - always quantify uncertainty!\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
