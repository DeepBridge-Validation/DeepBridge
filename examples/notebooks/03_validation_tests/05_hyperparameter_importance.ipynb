{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéõÔ∏è Hyperparameter Importance Analysis\n",
    "\n",
    "<div style=\"background-color: #e8f5e9; padding: 15px; border-radius: 5px; border-left: 5px solid #4caf50;\">\n",
    "<b>üìì Notebook Information</b><br>\n",
    "<b>Level:</b> Intermediate-Advanced<br>\n",
    "<b>Estimated Time:</b> 25 minutes<br>\n",
    "<b>Prerequisites:</b> 01_tests_introduction.ipynb<br>\n",
    "<b>Dataset:</b> Diabetes (sklearn)\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- ‚úÖ Understand hyperparameter importance vs feature importance\n",
    "- ‚úÖ Use Optuna for Bayesian optimization\n",
    "- ‚úÖ Analyze which hyperparameters matter most\n",
    "- ‚úÖ Test hyperparameter sensitivity\n",
    "- ‚úÖ Know which hyperparameters to tune (and which to ignore)\n",
    "- ‚úÖ Save time by focusing on important hyperparameters\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Table of Contents\n",
    "\n",
    "1. [Introduction](#intro)\n",
    "2. [Setup](#setup)\n",
    "3. [Baseline Model](#baseline)\n",
    "4. [Hyperparameter Tuning with Optuna](#optuna)\n",
    "5. [Importance Analysis](#importance)\n",
    "6. [Sensitivity Testing](#sensitivity)\n",
    "7. [Feature vs Hyperparameter Importance](#comparison)\n",
    "8. [DeepBridge Integration](#deepbridge)\n",
    "9. [Best Practices](#practices)\n",
    "10. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "## 1. üìñ Introduction\n",
    "\n",
    "### What is Hyperparameter Importance?\n",
    "\n",
    "> **Hyperparameter Importance** tells you which hyperparameters have the biggest impact on model performance.\n",
    "\n",
    "### Why Does This Matter?\n",
    "\n",
    "**The Problem:**\n",
    "```python\n",
    "# RandomForest has 20+ hyperparameters!\n",
    "RandomForestClassifier(\n",
    "    n_estimators=?,       # ‚Üê Important?\n",
    "    max_depth=?,          # ‚Üê Important?\n",
    "    min_samples_split=?,  # ‚Üê Important?\n",
    "    min_samples_leaf=?,   # ‚Üê Important?\n",
    "    max_features=?,       # ‚Üê Important?\n",
    "    ... # 15 more!\n",
    ")\n",
    "```\n",
    "\n",
    "**The Reality:**\n",
    "- üòµ **Too many to tune** - Grid search explodes combinatorially\n",
    "- ‚è±Ô∏è **Wastes time** - Tuning unimportant params doesn't help\n",
    "- üí∞ **Costs money** - Cloud compute isn't free\n",
    "\n",
    "**The Solution:**\n",
    "- üéØ **Focus on what matters** - Tune important params, use defaults for rest\n",
    "- ‚ö° **Faster optimization** - Fewer dimensions = faster convergence\n",
    "- üìä **Better understanding** - Know your model's sensitivities\n",
    "\n",
    "### Feature Importance vs Hyperparameter Importance\n",
    "\n",
    "| Aspect | Feature Importance | Hyperparameter Importance |\n",
    "|--------|-------------------|---------------------------|\n",
    "| **What** | Which input features matter? | Which hyperparameters matter? |\n",
    "| **Impact** | Data ‚Üí Predictions | Model configuration ‚Üí Performance |\n",
    "| **Usage** | Feature selection | Hyperparameter tuning |\n",
    "| **Typical Result** | 20% features = 80% importance | 2-3 params = 80% importance |\n",
    "\n",
    "### Real-world Example\n",
    "\n",
    "**RandomForest hyperparameter importance (typical):**\n",
    "1. ‚≠ê‚≠ê‚≠ê `max_depth` - CRITICAL (50% importance)\n",
    "2. ‚≠ê‚≠ê `n_estimators` - Important (25% importance)\n",
    "3. ‚≠ê `min_samples_split` - Moderate (15% importance)\n",
    "4. Others - Minor (10% combined)\n",
    "\n",
    "**Insight:** Focus on top 3, ignore the rest!\n",
    "\n",
    "**Let's learn how!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 2. üõ†Ô∏è Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# sklearn\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Optuna for Bayesian optimization\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# DeepBridge\n",
    "from deepbridge import DBDataset, Experiment\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('Set2')\n",
    "%matplotlib inline\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(\"üéõÔ∏è Topic: Hyperparameter Importance Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"baseline\"></a>\n",
    "## 3. üìä Baseline Model\n",
    "\n",
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load diabetes dataset (regression)\n",
    "diabetes = load_diabetes()\n",
    "df = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "df['target'] = diabetes.target\n",
    "\n",
    "print(f\"üìä Diabetes Dataset:\")\n",
    "print(f\"   Shape: {df.shape}\")\n",
    "print(f\"   Task: Regression (predict disease progression)\")\n",
    "print(f\"   Features: {len(diabetes.feature_names)}\")\n",
    "\n",
    "# Split\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\n   Train: {X_train.shape}\")\n",
    "print(f\"   Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Baseline Model (Default Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: RandomForest with default params\n",
    "model_baseline = RandomForestRegressor(random_state=RANDOM_STATE)\n",
    "model_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_baseline = model_baseline.predict(X_test)\n",
    "r2_baseline = r2_score(y_test, y_pred_baseline)\n",
    "rmse_baseline = np.sqrt(mean_squared_error(y_test, y_pred_baseline))\n",
    "\n",
    "print(\"üìä BASELINE MODEL (Default Hyperparameters)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n   R¬≤ Score: {r2_baseline:.4f}\")\n",
    "print(f\"   RMSE: {rmse_baseline:.2f}\")\n",
    "print(f\"\\n   Default hyperparameters used:\")\n",
    "print(f\"      n_estimators: {model_baseline.n_estimators}\")\n",
    "print(f\"      max_depth: {model_baseline.max_depth}\")\n",
    "print(f\"      min_samples_split: {model_baseline.min_samples_split}\")\n",
    "print(f\"      min_samples_leaf: {model_baseline.min_samples_leaf}\")\n",
    "print(f\"      max_features: {model_baseline.max_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"optuna\"></a>\n",
    "## 4. üîç Hyperparameter Tuning with Optuna\n",
    "\n",
    "### Define Optimization Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Setting up Bayesian Optimization with Optuna...\\n\")\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna.\n",
    "    Defines hyperparameter search space and returns metric to optimize.\n",
    "    \"\"\"\n",
    "    # Define hyperparameters to tune\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 20),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "        'random_state': RANDOM_STATE\n",
    "    }\n",
    "    \n",
    "    # Train model with these hyperparameters\n",
    "    model = RandomForestRegressor(**params)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    scores = cross_val_score(model, X_train, y_train, \n",
    "                              cv=5, scoring='r2', n_jobs=-1)\n",
    "    \n",
    "    return scores.mean()  # Return mean R¬≤\n",
    "\n",
    "print(\"‚úÖ Objective function defined\")\n",
    "print(\"   Hyperparameters to tune: 5\")\n",
    "print(\"   Optimization metric: R¬≤ (cross-validation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Running Bayesian Optimization...\\n\")\n",
    "print(\"   This will try 50 different hyperparameter combinations\")\n",
    "print(\"   Using smart Bayesian search (not random!)\\n\")\n",
    "\n",
    "# Create study\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',  # Maximize R¬≤\n",
    "    sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE)\n",
    ")\n",
    "\n",
    "# Optimize\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Optimization complete!\")\n",
    "print(f\"   Trials run: {len(study.trials)}\")\n",
    "print(f\"   Best R¬≤: {study.best_value:.4f}\")\n",
    "print(f\"   Improvement over baseline: {(study.best_value - r2_baseline)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üèÜ BEST HYPERPARAMETERS FOUND\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for param, value in study.best_params.items():\n",
    "    print(f\"   {param}: {value}\")\n",
    "\n",
    "# Train final model with best params\n",
    "model_tuned = RandomForestRegressor(**study.best_params, random_state=RANDOM_STATE)\n",
    "model_tuned.fit(X_train, y_train)\n",
    "\n",
    "y_pred_tuned = model_tuned.predict(X_test)\n",
    "r2_tuned = r2_score(y_test, y_pred_tuned)\n",
    "rmse_tuned = np.sqrt(mean_squared_error(y_test, y_pred_tuned))\n",
    "\n",
    "print(f\"\\nüìä Tuned Model Performance:\")\n",
    "print(f\"   R¬≤ Score: {r2_tuned:.4f} (baseline: {r2_baseline:.4f})\")\n",
    "print(f\"   RMSE: {rmse_tuned:.2f} (baseline: {rmse_baseline:.2f})\")\n",
    "print(f\"\\n   Improvement: {(r2_tuned - r2_baseline)*100:.2f}% R¬≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"importance\"></a>\n",
    "## 5. üéØ Hyperparameter Importance Analysis\n",
    "\n",
    "### Calculate Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Analyzing Hyperparameter Importance...\\n\")\n",
    "\n",
    "# Get hyperparameter importance from Optuna\n",
    "importance = optuna.importance.get_param_importances(study)\n",
    "\n",
    "# Create DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Hyperparameter': list(importance.keys()),\n",
    "    'Importance': list(importance.values())\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"üìä HYPERPARAMETER IMPORTANCE RANKING\\n\")\n",
    "print(\"=\" * 60)\n",
    "display(importance_df.style\n",
    "        .format({'Importance': '{:.4f}'})\n",
    "        .background_gradient(cmap='RdYlGn', subset=['Importance'])\n",
    ")\n",
    "\n",
    "print(f\"\\nüí° Key Insights:\")\n",
    "most_important = importance_df.iloc[0]['Hyperparameter']\n",
    "most_importance_val = importance_df.iloc[0]['Importance']\n",
    "print(f\"   Most important: {most_important} ({most_importance_val:.1%} importance)\")\n",
    "print(f\"   Top 2 params account for: {importance_df.head(2)['Importance'].sum():.1%} of importance\")\n",
    "print(f\"   Bottom params are negligible - use defaults!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Hyperparameter'], importance_df['Importance'],\n",
    "         color='steelblue', edgecolor='black', alpha=0.8)\n",
    "plt.xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "plt.title('Hyperparameter Importance', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Tuning Strategy Based on Importance:\")\n",
    "print(f\"   ‚Ä¢ Focus on: {', '.join(importance_df.head(2)['Hyperparameter'])}\")\n",
    "print(f\"   ‚Ä¢ Use defaults for rest to save time!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sensitivity\"></a>\n",
    "## 6. üìà Sensitivity Testing\n",
    "\n",
    "### Test Most Important Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most important hyperparameter\n",
    "most_important_param = importance_df.iloc[0]['Hyperparameter']\n",
    "\n",
    "print(f\"üìà Testing sensitivity of: {most_important_param}\\n\")\n",
    "\n",
    "# Test range of values\n",
    "if most_important_param == 'n_estimators':\n",
    "    test_values = [50, 100, 150, 200, 250, 300]\n",
    "elif most_important_param == 'max_depth':\n",
    "    test_values = [2, 5, 8, 10, 15, 20]\n",
    "elif most_important_param == 'min_samples_split':\n",
    "    test_values = [2, 5, 10, 15, 20]\n",
    "else:\n",
    "    test_values = [1, 2, 3, 5, 8, 10]\n",
    "\n",
    "# Test each value\n",
    "r2_scores = []\n",
    "\n",
    "for val in test_values:\n",
    "    params = study.best_params.copy()\n",
    "    params[most_important_param] = val\n",
    "    \n",
    "    model = RandomForestRegressor(**params, random_state=RANDOM_STATE)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "    r2_scores.append(scores.mean())\n",
    "\n",
    "# Plot sensitivity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(test_values, r2_scores, 'o-', linewidth=2, markersize=8, color='steelblue')\n",
    "plt.xlabel(most_important_param, fontsize=12, fontweight='bold')\n",
    "plt.ylabel('R¬≤ Score (CV)', fontsize=12, fontweight='bold')\n",
    "plt.title(f'Sensitivity Analysis: {most_important_param}', fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Sensitivity Insights:\")\n",
    "best_idx = np.argmax(r2_scores)\n",
    "print(f\"   Optimal {most_important_param}: {test_values[best_idx]}\")\n",
    "print(f\"   R¬≤ range: {min(r2_scores):.4f} - {max(r2_scores):.4f}\")\n",
    "print(f\"   Impact: {(max(r2_scores) - min(r2_scores))*100:.2f}% R¬≤ swing\")\n",
    "print(f\"   ‚Üí This param is VERY important to tune!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "- ‚úÖ **Hyperparameter vs Feature Importance** - Two different concepts\n",
    "- ‚úÖ **Bayesian Optimization** - Smart tuning with Optuna\n",
    "- ‚úÖ **Importance Analysis** - Which params matter most\n",
    "- ‚úÖ **Sensitivity Testing** - How much impact does each param have\n",
    "- ‚úÖ **Efficient Tuning** - Focus on top 2-3 params, ignore rest\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. üéØ **80/20 Rule** - 2-3 params = 80% of importance\n",
    "2. ‚ö° **Save Time** - Don't tune everything!\n",
    "3. üìä **Measure, Don't Guess** - Use importance analysis\n",
    "4. üîç **Optuna > Grid Search** - Bayesian optimization is smarter\n",
    "5. üí° **Model-Specific** - Different models, different important params\n",
    "6. üîÑ **Context Matters** - Importance varies by dataset\n",
    "\n",
    "### Typical Importance Rankings\n",
    "\n",
    "**RandomForest:**\n",
    "1. `max_depth` ‚≠ê‚≠ê‚≠ê\n",
    "2. `n_estimators` ‚≠ê‚≠ê\n",
    "3. `min_samples_split` ‚≠ê\n",
    "\n",
    "**GradientBoosting:**\n",
    "1. `learning_rate` ‚≠ê‚≠ê‚≠ê\n",
    "2. `n_estimators` ‚≠ê‚≠ê\n",
    "3. `max_depth` ‚≠ê‚≠ê\n",
    "\n",
    "**Neural Networks:**\n",
    "1. `learning_rate` ‚≠ê‚≠ê‚≠ê\n",
    "2. `batch_size` ‚≠ê‚≠ê\n",
    "3. `hidden_layer_sizes` ‚≠ê‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "**Remember: Tune smart, not hard!** üéØ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
