{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Š Uncertainty Quantification - Classification\n",
    "\n",
    "<div style=\"background-color: #e3f2fd; padding: 15px; border-radius: 5px; border-left: 5px solid #2196F3;\">\n",
    "<b>ğŸ““ Information</b><br>\n",
    "<b>Level:</b> Intermediate/Advanced<br>\n",
    "<b>Time:</b> 20 minutes<br>\n",
    "<b>Dataset:</b> Breast Cancer (sklearn)<br>\n",
    "<b>Prerequisite:</b> 03_uncertainty.ipynb\n",
    "</div>\n",
    "\n",
    "## ğŸ¯ Objectives\n",
    "- âœ… Uncertainty quantification for **classification** problems\n",
    "- âœ… Generate **interactive HTML report**\n",
    "- âœ… Export results to **JSON format**\n",
    "- âœ… Analyze **probability calibration**\n",
    "- âœ… Make **uncertainty-based decisions** in critical scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š Why Uncertainty in Classification?\n",
    "\n",
    "### Critical Contexts\n",
    "\n",
    "#### ğŸ¥ Medicine - Cancer Diagnosis\n",
    "- **Problem**: Binary diagnosis (benign vs malignant)\n",
    "- **Traditional**: Prediction = 0.85 (85% chance malignant)\n",
    "- **With Uncertainty**: Prediction = 0.85 Â± 0.15 (interval: [0.70, 1.00])\n",
    "- **Decision**: High uncertainty â†’ Request additional exams\n",
    "\n",
    "#### ğŸ’³ Finance - Credit Approval\n",
    "- **Problem**: Approve/Reject credit\n",
    "- **Traditional**: Prediction = 0.40 (40% chance of default)\n",
    "- **With Uncertainty**: Prediction = 0.40 Â± 0.05 (interval: [0.35, 0.45])\n",
    "- **Decision**: Low uncertainty â†’ Approve with adjusted rate\n",
    "\n",
    "#### ğŸ”’ Security - Fraud Detection\n",
    "- **Problem**: Detect fraudulent transactions\n",
    "- **Traditional**: Prediction = 0.60 (60% chance fraud)\n",
    "- **With Uncertainty**: Prediction = 0.60 Â± 0.30 (interval: [0.30, 0.90])\n",
    "- **Decision**: High uncertainty â†’ Require 2FA verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Setup - Binary Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-12 20:59:18,925 - deepbridge.reports - DEBUG - Using refactored FairnessDataTransformer\n",
      "2025-11-12 20:59:18,932 - deepbridge.reports - INFO - Successfully imported radar chart fix\n",
      "2025-11-12 20:59:18,934 - deepbridge.reports - INFO - Successfully patched EnhancedUncertaintyCharts.generate_model_metrics_comparison\n",
      "2025-11-12 20:59:18,935 - deepbridge.reports - INFO - Successfully applied enhanced_charts patch\n",
      "2025-11-12 20:59:18,937 - deepbridge.reports - INFO - Successfully loaded UncertaintyChartGenerator\n",
      "2025-11-12 20:59:18,938 - deepbridge.reports - INFO - Successfully imported and initialized SeabornChartGenerator\n",
      "2025-11-12 20:59:18,939 - deepbridge.reports - INFO - SeabornChartGenerator has_visualization_libs: True\n",
      "2025-11-12 20:59:18,940 - deepbridge.reports - INFO - Available chart methods: ['bar_chart', 'boxplot_chart', 'coverage_analysis_chart', 'detailed_boxplot_chart', 'distribution_grid_chart', 'feature_comparison_chart', 'feature_importance_chart', 'feature_psi_chart', 'generate_encoded_chart', 'heatmap_chart', 'individual_feature_impact_chart', 'method_comparison_chart', 'metrics_radar_chart', 'model_comparison_chart', 'model_metrics_heatmap', 'robustness_overview_chart', 'selected_features_comparison_chart', 'uncertainty_violin_chart', 'worst_performance_chart']\n",
      "2025-11-12 20:59:18,943 - deepbridge.reports - INFO - Successfully imported visualization libraries (numpy, matplotlib, seaborn, pandas)\n",
      "2025-11-12 20:59:18,949 - deepbridge.reports - INFO - Successfully loaded resilience-specific chart generator from deepbridge.templates.report_types.resilience.static.charts\n",
      "2025-11-12 20:59:18,950 - deepbridge.reports - INFO - Available resilience chart methods: ['generate_critical_feature_distributions', 'generate_distance_metrics_comparison', 'generate_feature_distance_heatmap', 'generate_feature_distribution_shift', 'generate_feature_residual_correlation', 'generate_model_comparison', 'generate_model_comparison_scatter', 'generate_model_resilience_scores', 'generate_performance_gap', 'generate_performance_gap_by_alpha', 'generate_residual_distribution']\n",
      "ğŸ“Š Dataset: (569, 31)\n",
      "ğŸ¥ Target: Cancer diagnosis (0=malignant, 1=benign)\n",
      "\n",
      "ğŸ“ˆ Class distribution:\n",
      "target\n",
      "1    357\n",
      "0    212\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“Š Class balance:\n",
      "target\n",
      "1    0.627417\n",
      "0    0.372583\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "from deepbridge import DBDataset, Experiment\n",
    "import os\n",
    "\n",
    "# Configure visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load Breast Cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "df['target'] = cancer.target  # 0 = malignant, 1 = benign\n",
    "\n",
    "print(f\"ğŸ“Š Dataset: {df.shape}\")\n",
    "print(f\"ğŸ¥ Target: Cancer diagnosis (0=malignant, 1=benign)\")\n",
    "print(f\"\\nğŸ“ˆ Class distribution:\")\n",
    "print(df['target'].value_counts())\n",
    "print(f\"\\nğŸ“Š Class balance:\")\n",
    "print(df['target'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Train Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model trained!\n",
      "ğŸ“Š Accuracy: 95.61%\n",
      "ğŸ“Š ROC-AUC: 0.994\n",
      "\n",
      "ğŸ“‹ Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Malignant       0.95      0.93      0.94        42\n",
      "      Benign       0.96      0.97      0.97        72\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.95      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train RandomForest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba[:, 1])\n",
    "\n",
    "print(f\"âœ… Model trained!\")\n",
    "print(f\"ğŸ“Š Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"ğŸ“Š ROC-AUC: {auc:.3f}\")\n",
    "print(f\"\\nğŸ“‹ Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Malignant', 'Benign']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Initial model evaluation complete: RandomForestClassifier\n",
      "âœ… Experiment created!\n"
     ]
    }
   ],
   "source": [
    "dataset = DBDataset(\n",
    "    data=df,\n",
    "    target_column='target',\n",
    "    model=model,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    dataset_name='Breast Cancer Classification'\n",
    ")\n",
    "\n",
    "exp = Experiment(\n",
    "    dataset=dataset,\n",
    "    experiment_type='binary_classification',\n",
    "    tests=['uncertainty'],  # Specify uncertainty test\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"âœ… Experiment created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Run Uncertainty Test\n",
    "\n",
    "<div style=\"background-color: #fff3e0; padding: 15px; border-radius: 5px; border-left: 5px solid #ff9800;\">\n",
    "<b>â„¹ï¸ Configuration:</b> Using 'full' config for comprehensive analysis with multiple alpha levels.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Running uncertainty quantification test...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RUN_DEBUG] No CRQR results found to process\n",
      "[RUN_DEBUG] Missing reliability_analysis in results\n",
      "[RUN_DEBUG] Missing marginal_bandwidth in results\n",
      "[RUN_DEBUG] Missing interval_widths in results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Uncertainty Tests Finished!\n",
      "ğŸ‰ Test completed successfully: uncertainty\n",
      "\n",
      "âœ… Uncertainty test completed!\n",
      "\n",
      "ğŸ“Š Result type: <class 'deepbridge.core.experiment.results.ExperimentResult'>\n",
      "ğŸ“Š Uncertainty result type: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ§ª Running uncertainty quantification test...\\n\")\n",
    "\n",
    "# Use run_tests() to store results internally for save_html() and save_json()\n",
    "experiment_result = exp.run_tests(config_name='full')\n",
    "\n",
    "print(\"\\nâœ… Uncertainty test completed!\")\n",
    "print(f\"\\nğŸ“Š Result type: {type(experiment_result)}\")\n",
    "\n",
    "# Access the uncertainty result\n",
    "uncertainty_result = experiment_result.get_result('uncertainty')\n",
    "print(f\"ğŸ“Š Uncertainty result type: {type(uncertainty_result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Generate Interactive HTML Report\n",
    "\n",
    "### Export comprehensive interactive report with:\n",
    "- ğŸ“Š **Coverage Analysis**: Validation of prediction intervals\n",
    "- ğŸ“ˆ **Feature Impact**: Features that most influence uncertainty\n",
    "- ğŸ¯ **Calibration Curves**: Probability calibration analysis\n",
    "- ğŸ“‰ **Trade-off Analysis**: Coverage vs Width balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Generating interactive HTML report...\n",
      "\n",
      "2025-11-12 20:59:36,020 - deepbridge.reports - INFO - Generating SIMPLE uncertainty report to: /home/guhaase/projetos/DeepBridge/examples/notebooks/03_validation_tests/outputs/uncertainty_classification/uncertainty_classification_interactive.html\n",
      "2025-11-12 20:59:36,020 - deepbridge.reports - INFO - Report type: interactive\n",
      "2025-11-12 20:59:36,021 - deepbridge.reports - INFO - Transforming uncertainty data for report (SIMPLE)\n",
      "2025-11-12 20:59:36,022 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] transform() input - results keys: ['primary_model', 'alternative_models', 'config', 'test_predictions', 'test_labels', 'uncertainty_quality_score', 'avg_coverage_error', 'avg_normalized_width', 'alphas', 'feature_importance', 'alpha_levels', 'timestamp', 'model_type', 'dataset', 'predictions', 'metrics', 'initial_results', 'initial_model_evaluation', 'experiment_type']\n",
      "2025-11-12 20:59:36,023 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] Using direct primary_model format\n",
      "2025-11-12 20:59:36,024 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] primary_model keys: ['crqr', 'alphas', 'feature_importance', 'avg_coverage_error', 'avg_normalized_width', 'uncertainty_quality_score', 'plot_data', 'reliability_analysis', 'reliability_charts', 'test_predictions', 'test_labels', 'model_type']\n",
      "2025-11-12 20:59:36,025 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] initial_eval keys: ['config', 'models', 'test_configs']\n",
      "2025-11-12 20:59:36,026 - deepbridge.reports - INFO - [METRICS_DEBUG] Added metrics from initial_model_evaluation to primary_model\n",
      "2025-11-12 20:59:36,027 - deepbridge.reports - INFO - [FEATURE_IMPACT_DEBUG] _transform_feature_impacts called\n",
      "2025-11-12 20:59:36,027 - deepbridge.reports - WARNING - [FEATURE_IMPACT_DEBUG] by_feature data is currently buggy - all features have identical values\n",
      "2025-11-12 20:59:36,028 - deepbridge.reports - INFO - [FEATURE_IMPACT_DEBUG] Using feature importance as workaround\n",
      "2025-11-12 20:59:36,029 - deepbridge.reports - INFO - [FEATURE_IMPACT_DEBUG] Using feature_importance with 5 features\n",
      "2025-11-12 20:59:36,031 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] Base width: 0.8690\n",
      "2025-11-12 20:59:36,032 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] mean radius: importance=0.0633, width_impact=0.0550, coverage_impact=0.0937\n",
      "2025-11-12 20:59:36,033 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] mean texture: importance=1.0000, width_impact=0.8690, coverage_impact=0.0000\n",
      "2025-11-12 20:59:36,033 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] mean perimeter: importance=0.1473, width_impact=0.1280, coverage_impact=0.0853\n",
      "2025-11-12 20:59:36,034 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] mean area: importance=0.2180, width_impact=0.1894, coverage_impact=0.0782\n",
      "2025-11-12 20:59:36,035 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] mean smoothness: importance=0.2531, width_impact=0.2200, coverage_impact=0.0747\n",
      "2025-11-12 20:59:36,035 - deepbridge.reports - INFO - [FEATURE_IMPACT_DEBUG] Created 5 feature impacts using importance\n",
      "2025-11-12 20:59:36,036 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] Top 3 impacts: [{'name': 'mean texture', 'width_impact': 0.8689939268472979, 'coverage_impact': 0.0}, {'name': 'mean smoothness', 'width_impact': 0.21998024359281562, 'coverage_impact': 0.07468564085471784}, {'name': 'mean area', 'width_impact': 0.18943224040698226, 'coverage_impact': 0.07820097073701762}]\n",
      "2025-11-12 20:59:36,037 - deepbridge.reports - INFO - [SUMMARY_DEBUG] base_score extracted: 0.9912\n",
      "2025-11-12 20:59:36,037 - deepbridge.reports - INFO - [SUMMARY_DEBUG] avg_coverage_error (calibration_error): 0.0915\n",
      "2025-11-12 20:59:36,037 - deepbridge.reports - DEBUG - [COVERAGE_DEBUG] Formatted 5 alpha results for table\n",
      "2025-11-12 20:59:36,038 - deepbridge.reports - INFO - [FEATURE_IMPACT_DEBUG] _transform_features called\n",
      "2025-11-12 20:59:36,038 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] initial_eval keys: ['config', 'models', 'test_configs']\n",
      "2025-11-12 20:59:36,039 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] primary_model keys: ['crqr', 'alphas', 'feature_importance', 'avg_coverage_error', 'avg_normalized_width', 'uncertainty_quality_score', 'plot_data', 'reliability_analysis', 'reliability_charts', 'test_predictions', 'test_labels', 'model_type', 'metrics']\n",
      "2025-11-12 20:59:36,039 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] feature_importance from initial_eval: 30 features\n",
      "2025-11-12 20:59:36,040 - deepbridge.reports - INFO - [FEATURE_IMPACT_DEBUG] _prepare_charts called\n",
      "2025-11-12 20:59:36,041 - deepbridge.reports - INFO - [FEATURE_IMPACT_DEBUG] _transform_features called\n",
      "2025-11-12 20:59:36,041 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] initial_eval keys: ['config', 'models', 'test_configs']\n",
      "2025-11-12 20:59:36,042 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] primary_model keys: ['crqr', 'alphas', 'feature_importance', 'avg_coverage_error', 'avg_normalized_width', 'uncertainty_quality_score', 'plot_data', 'reliability_analysis', 'reliability_charts', 'test_predictions', 'test_labels', 'model_type', 'metrics']\n",
      "2025-11-12 20:59:36,042 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] feature_importance from initial_eval: 30 features\n",
      "2025-11-12 20:59:36,043 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] Transformed features: 30 total features\n",
      "2025-11-12 20:59:36,043 - deepbridge.reports - INFO - [FEATURE_IMPACT_DEBUG] _chart_feature_importance called\n",
      "2025-11-12 20:59:36,044 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] features dict keys: ['total', 'importance', 'top_10', 'top_20', 'feature_list']\n",
      "2025-11-12 20:59:36,044 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] features['total']: 30\n",
      "2025-11-12 20:59:36,045 - deepbridge.reports - INFO - [FEATURE_IMPACT_DEBUG] top_features count: 10\n",
      "2025-11-12 20:59:36,045 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] Feature names: ['worst area', 'worst concave points', 'worst radius', 'mean concave points', 'worst perimeter', 'mean perimeter', 'mean radius', 'mean concavity', 'mean area', 'worst concavity']\n",
      "2025-11-12 20:59:36,046 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] Feature importances: [0.14001598566310833, 0.12952961280106504, 0.09769573685350633, 0.0908847484687512, 0.07222637006988931, 0.0695738754013612, 0.06867599362483164, 0.05763806222702219, 0.04917214870032646, 0.03434012642032042]\n",
      "2025-11-12 20:59:36,047 - deepbridge.reports - INFO - [FEATURE_IMPACT_DEBUG] Feature chart created successfully\n",
      "2025-11-12 20:59:36,047 - deepbridge.reports - INFO - [FEATURE_IMPACT_DEBUG] Feature chart data length: 1\n",
      "2025-11-12 20:59:36,048 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] Feature chart has layout: True\n",
      "2025-11-12 20:59:36,049 - deepbridge.reports - DEBUG - [COVERAGE_DEBUG] _chart_coverage_by_alpha called with 5 alphas\n",
      "2025-11-12 20:59:36,049 - deepbridge.reports - DEBUG - [COVERAGE_DEBUG] _chart_coverage_by_alpha: alpha_values=['Î±=0.01', 'Î±=0.05', 'Î±=0.1', 'Î±=0.2', 'Î±=0.3']\n",
      "2025-11-12 20:59:36,050 - deepbridge.reports - DEBUG - [COVERAGE_DEBUG] _chart_coverage_by_alpha: coverages=[0.989010989010989, 0.9956043956043956, 0.9758241758241758, 0.9340659340659341, 0.9010989010989011]\n",
      "2025-11-12 20:59:36,051 - deepbridge.reports - DEBUG - [COVERAGE_DEBUG] _chart_coverage_by_alpha: expected_coverages=[0.99, 0.95, 0.9, 0.8, 0.7]\n",
      "2025-11-12 20:59:36,051 - deepbridge.reports - DEBUG - [COVERAGE_DEBUG] Created coverage chart with 5 alpha levels\n",
      "2025-11-12 20:59:36,052 - deepbridge.reports - INFO - [FEATURE_IMPACT_DEBUG] Charts dictionary keys: ['overview', 'calibration', 'coverage', 'tradeoff', 'boxplot_width', 'width_distribution', 'feature_importance', 'features']\n",
      "2025-11-12 20:59:36,053 - deepbridge.reports - INFO - [FEATURE_IMPACT_DEBUG] charts['features'] exists: True\n",
      "2025-11-12 20:59:36,053 - deepbridge.reports - INFO - [FEATURE_IMPACT_DEBUG] charts['features'] has data: 1 traces\n",
      "2025-11-12 20:59:36,054 - deepbridge.reports - INFO - [WIDTH_DIST_DEBUG] charts['width_distribution'] exists: True\n",
      "2025-11-12 20:59:36,054 - deepbridge.reports - INFO - [WIDTH_DIST_DEBUG] width_distribution has data: 5 traces\n",
      "2025-11-12 20:59:36,055 - deepbridge.reports - INFO - Transformation complete. 5 alpha levels, 30 features, 5 feature impacts\n",
      "2025-11-12 20:59:36,055 - deepbridge.reports - DEBUG - [COVERAGE_DEBUG] alpha_results array has 5 entries\n",
      "2025-11-12 20:59:36,056 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] feature_impacts array has 5 entries\n",
      "2025-11-12 20:59:36,056 - deepbridge.reports - INFO - [FEATURE_IMPACT_DEBUG] report_data keys after transform: ['model_name', 'model_type', 'summary', 'alphas', 'alpha_results', 'features', 'feature_impacts', 'charts', 'metadata']\n",
      "2025-11-12 20:59:36,057 - deepbridge.reports - INFO - [FEATURE_IMPACT_DEBUG] report_data['charts'] keys: ['overview', 'calibration', 'coverage', 'tradeoff', 'boxplot_width', 'width_distribution', 'feature_importance', 'features']\n",
      "2025-11-12 20:59:36,058 - deepbridge.reports - INFO - [FEATURE_IMPACT_DEBUG] 'features' in charts: True\n",
      "2025-11-12 20:59:36,059 - deepbridge.reports - INFO - [FEATURE_IMPACT_DEBUG] features_chart has data: 1 traces\n",
      "2025-11-12 20:59:36,060 - deepbridge.reports - INFO - [FEATURE_IMPACT_DEBUG] features_chart has layout: True\n",
      "2025-11-12 20:59:36,060 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] First trace type: bar\n",
      "2025-11-12 20:59:36,061 - deepbridge.reports - DEBUG - [FEATURE_IMPACT_DEBUG] First trace has y values: True\n",
      "2025-11-12 20:59:36,062 - deepbridge.reports - INFO - Found template at: /home/guhaase/projetos/DeepBridge/deepbridge/templates/report_types/uncertainty/interactive/index.html\n",
      "2025-11-12 20:59:36,062 - deepbridge.reports - DEBUG - Loading template: /home/guhaase/projetos/DeepBridge/deepbridge/templates/report_types/uncertainty/interactive/index.html\n",
      "2025-11-12 20:59:36,080 - deepbridge.reports - INFO - Template loaded for uncertainty/interactive\n",
      "2025-11-12 20:59:36,082 - deepbridge.reports - INFO - CSS compiled successfully using CSSManager for uncertainty: 31096 chars\n",
      "2025-11-12 20:59:36,086 - deepbridge.reports - INFO - [METRICS_DEBUG] base_score from summary: 0.9912\n",
      "2025-11-12 20:59:36,087 - deepbridge.reports - INFO - [METRICS_DEBUG] calibration_error from summary: 0.0915\n",
      "2025-11-12 20:59:36,088 - deepbridge.reports - INFO - [METRICS_DEBUG] uncertainty_score from summary: 0.6376\n",
      "2025-11-12 20:59:36,089 - deepbridge.reports - INFO - [METRICS_DEBUG] avg_coverage from summary: 0.9591\n",
      "2025-11-12 20:59:36,089 - deepbridge.reports - INFO - [FEATURE_IMPACT_DEBUG] Context keys being sent to template: ['report_data', 'report_data_json', 'css_content', 'js_content', 'logo', 'favicon_base64', 'model_name', 'model_type', 'timestamp', 'test_type', 'current_year', 'report_title', 'report_subtitle', 'base_score', 'uncertainty_score', 'total_alphas', 'total_features', 'avg_coverage', 'calibration_error', 'avg_coverage_error', 'avg_width']\n",
      "2025-11-12 20:59:36,090 - deepbridge.reports - INFO - [FEATURE_IMPACT_DEBUG] Context has report_data: True\n",
      "2025-11-12 20:59:36,090 - deepbridge.reports - INFO - [FEATURE_IMPACT_DEBUG] Context has report_data_json: True\n",
      "2025-11-12 20:59:36,091 - deepbridge.reports - INFO - [FEATURE_IMPACT_DEBUG] Context report_data.charts.features exists: True\n",
      "2025-11-12 20:59:36,091 - deepbridge.reports - INFO - [FEATURE_IMPACT_DEBUG] Context report_data.charts.features has data: 1 traces\n",
      "2025-11-12 20:59:36,092 - deepbridge.reports - DEBUG - Rendering template with context keys: ['report_data', 'report_data_json', 'css_content', 'js_content', 'logo', 'favicon_base64', 'model_name', 'model_type', 'timestamp', 'test_type', 'current_year', 'report_title', 'report_subtitle', 'base_score', 'uncertainty_score', 'total_alphas', 'total_features', 'avg_coverage', 'calibration_error', 'avg_coverage_error', 'avg_width']\n",
      "2025-11-12 20:59:36,093 - deepbridge.reports - INFO - Report generated and saved to: /home/guhaase/projetos/DeepBridge/examples/notebooks/03_validation_tests/outputs/uncertainty_classification/uncertainty_classification_interactive.html (type: interactive)\n",
      "2025-11-12 20:59:36,094 - deepbridge.reports - INFO - Output directory ensured: /home/guhaase/projetos/DeepBridge/examples/notebooks/03_validation_tests/outputs/uncertainty_classification\n",
      "2025-11-12 20:59:36,096 - deepbridge.reports - INFO - Report saved to: /home/guhaase/projetos/DeepBridge/examples/notebooks/03_validation_tests/outputs/uncertainty_classification/uncertainty_classification_interactive.html\n",
      "2025-11-12 20:59:36,096 - deepbridge.reports - INFO - Report generated and saved to: /home/guhaase/projetos/DeepBridge/examples/notebooks/03_validation_tests/outputs/uncertainty_classification/uncertainty_classification_interactive.html (type: interactive)\n",
      "\n",
      "âœ… Interactive report generated!\n",
      "ğŸ“‚ Location: /home/guhaase/projetos/DeepBridge/examples/notebooks/03_validation_tests/outputs/uncertainty_classification/uncertainty_classification_interactive.html\n",
      "\n",
      "ğŸ’¡ Open the HTML file in your browser to explore:\n",
      "   - Coverage Analysis tab\n",
      "   - Feature Impact tab\n",
      "   - Calibration tab\n",
      "   - Interactive Plotly charts\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "output_dir = './outputs/uncertainty_classification'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate interactive HTML report\n",
    "html_output_path = os.path.join(output_dir, 'uncertainty_classification_interactive.html')\n",
    "\n",
    "print(\"ğŸ“ Generating interactive HTML report...\\n\")\n",
    "\n",
    "report_path = exp.save_html(\n",
    "    test_type='uncertainty',\n",
    "    file_path=html_output_path,\n",
    "    model_name='RandomForest Classifier',\n",
    "    report_type='interactive'\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Interactive report generated!\")\n",
    "print(f\"ğŸ“‚ Location: {report_path}\")\n",
    "print(f\"\\nğŸ’¡ Open the HTML file in your browser to explore:\")\n",
    "print(f\"   - Coverage Analysis tab\")\n",
    "print(f\"   - Feature Impact tab\")\n",
    "print(f\"   - Calibration tab\")\n",
    "print(f\"   - Interactive Plotly charts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Export Results to JSON\n",
    "\n",
    "### JSON export includes:\n",
    "- ğŸ” **Experiment Info**: Configuration and metadata\n",
    "- ğŸ“Š **Test Results**: Complete uncertainty analysis data\n",
    "- ğŸ¯ **Model Evaluation**: Initial model metrics and feature importance\n",
    "- ğŸ“ˆ **By Alpha Results**: Coverage and width for each confidence level\n",
    "- ğŸŒŸ **By Feature Results**: Per-feature uncertainty analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Exporting results to JSON...\n",
      "\n",
      "\n",
      "âœ… JSON files exported successfully!\n",
      "\n",
      "ğŸ“Š FILE SIZES:\n",
      "   Full JSON:     5665.95 KB\n",
      "   Compact JSON:   912.13 KB\n",
      "   Reduction:        83.9%\n",
      "\n",
      "ğŸ“‚ LOCATIONS:\n",
      "   Full:    /home/guhaase/projetos/DeepBridge/examples/notebooks/03_validation_tests/outputs/uncertainty_classification/uncertainty_classification_results.json\n",
      "   Compact: /home/guhaase/projetos/DeepBridge/examples/notebooks/03_validation_tests/outputs/uncertainty_classification/uncertainty_classification_results_COMPACT.json\n",
      "\n",
      "ğŸ“‹ COMPACT JSON STRUCTURE (optimized for AI):\n",
      "   â””â”€ experiment_info/\n",
      "      â”œâ”€ test_type, experiment_type, generation_time, config\n",
      "   â””â”€ test_results/\n",
      "      â””â”€ primary_model/\n",
      "         â”œâ”€ metrics (all model metrics)\n",
      "         â”œâ”€ uncertainty_quality_score\n",
      "         â”œâ”€ feature_importance_top10 (only top 10 features)\n",
      "         â””â”€ crqr/\n",
      "            â””â”€ by_alpha/ (only overall_result per alpha, no sample data)\n",
      "   â””â”€ initial_model_evaluation/ (compact)\n",
      "      â””â”€ models/\n",
      "         â””â”€ primary_model/\n",
      "            â”œâ”€ metrics\n",
      "            â”œâ”€ model_type\n",
      "            â””â”€ feature_importance_top5\n",
      "   â””â”€ summary/  (AI-friendly analysis)\n",
      "      â”œâ”€ key_findings\n",
      "      â”œâ”€ model_performance\n",
      "      â”œâ”€ calibration_quality\n",
      "      â”œâ”€ recommendations\n",
      "      â””â”€ per_alpha_analysis\n",
      "\n",
      "ğŸ’¡ USE CASES:\n",
      "   â€¢ Full JSON: Deep dive analysis, debugging, research\n",
      "   â€¢ Compact JSON: AI validation, automated testing, CI/CD pipelines\n"
     ]
    }
   ],
   "source": [
    "# Export to JSON (COMPACT MODE for AI validation)\n",
    "json_output_path = os.path.join(output_dir, 'uncertainty_classification_results.json')\n",
    "json_compact_path = os.path.join(output_dir, 'uncertainty_classification_results_COMPACT.json')\n",
    "\n",
    "print(\"ğŸ“ Exporting results to JSON...\\n\")\n",
    "\n",
    "# Full JSON (with all data)\n",
    "json_path_full = exp._experiment_result.save_json(\n",
    "    test_type='uncertainty',\n",
    "    file_path=json_output_path,\n",
    "    include_summary=True,\n",
    "    compact=False  # Full data\n",
    ")\n",
    "\n",
    "# Compact JSON (optimized for AI validation)\n",
    "json_path_compact = exp._experiment_result.save_json(\n",
    "    test_type='uncertainty',\n",
    "    file_path=json_compact_path,\n",
    "    include_summary=True,\n",
    "    compact=True  # Remove large arrays, keep only essentials\n",
    ")\n",
    "\n",
    "# Compare file sizes\n",
    "import os\n",
    "size_full = os.path.getsize(json_path_full) / 1024  # KB\n",
    "size_compact = os.path.getsize(json_path_compact) / 1024  # KB\n",
    "reduction = ((size_full - size_compact) / size_full) * 100\n",
    "\n",
    "print(f\"\\nâœ… JSON files exported successfully!\")\n",
    "print(f\"\\nğŸ“Š FILE SIZES:\")\n",
    "print(f\"   Full JSON:    {size_full:>8.2f} KB\")\n",
    "print(f\"   Compact JSON: {size_compact:>8.2f} KB\")\n",
    "print(f\"   Reduction:    {reduction:>8.1f}%\")\n",
    "\n",
    "print(f\"\\nğŸ“‚ LOCATIONS:\")\n",
    "print(f\"   Full:    {json_path_full}\")\n",
    "print(f\"   Compact: {json_path_compact}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ COMPACT JSON STRUCTURE (optimized for AI):\")\n",
    "print(f\"   â””â”€ experiment_info/\")\n",
    "print(f\"      â”œâ”€ test_type, experiment_type, generation_time, config\")\n",
    "print(f\"   â””â”€ test_results/\")\n",
    "print(f\"      â””â”€ primary_model/\")\n",
    "print(f\"         â”œâ”€ metrics (all model metrics)\")\n",
    "print(f\"         â”œâ”€ uncertainty_quality_score\")\n",
    "print(f\"         â”œâ”€ feature_importance_top10 (only top 10 features)\")\n",
    "print(f\"         â””â”€ crqr/\")\n",
    "print(f\"            â””â”€ by_alpha/ (only overall_result per alpha, no sample data)\")\n",
    "print(f\"   â””â”€ initial_model_evaluation/ (compact)\")\n",
    "print(f\"      â””â”€ models/\")\n",
    "print(f\"         â””â”€ primary_model/\")\n",
    "print(f\"            â”œâ”€ metrics\")\n",
    "print(f\"            â”œâ”€ model_type\")\n",
    "print(f\"            â””â”€ feature_importance_top5\")\n",
    "print(f\"   â””â”€ summary/  (AI-friendly analysis)\")\n",
    "print(f\"      â”œâ”€ key_findings\")\n",
    "print(f\"      â”œâ”€ model_performance\")\n",
    "print(f\"      â”œâ”€ calibration_quality\")\n",
    "print(f\"      â”œâ”€ recommendations\")\n",
    "print(f\"      â””â”€ per_alpha_analysis\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ USE CASES:\")\n",
    "print(f\"   â€¢ Full JSON: Deep dive analysis, debugging, research\")\n",
    "print(f\"   â€¢ Compact JSON: AI validation, automated testing, CI/CD pipelines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Load and Analyze JSON Results\n",
    "\n",
    "Demonstrate how to load and use the exported JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“Š ANALYZING COMPACT JSON (optimized for AI validation)\n",
      "================================================================================\n",
      "\n",
      "ğŸ”¬ EXPERIMENT INFO:\n",
      "   Test Type: uncertainty\n",
      "   Experiment Type: binary_classification\n",
      "   Generation Time: 2025-11-12 20:59:36\n",
      "\n",
      "ğŸ¯ KEY FINDINGS:\n",
      "   â€¢ Average coverage: 95.9% (calibration error: 0.0915)\n",
      "   â€¢ High uncertainty detected (avg width: 0.7809)\n",
      "\n",
      "ğŸ“ˆ MODEL PERFORMANCE:\n",
      "   average_coverage: 0.9591\n",
      "   average_calibration_error: 0.0915\n",
      "   average_interval_width: 0.7809\n",
      "   uncertainty_score: 0.6376\n",
      "   Base Metrics:\n",
      "      accuracy: 0.9912\n",
      "      roc_auc: 1.0\n",
      "      f1: 0.9912\n",
      "      precision: 0.9913\n",
      "      recall: 0.9912\n",
      "\n",
      "ğŸ” CALIBRATION QUALITY:\n",
      "   Status: GOOD\n",
      "   Description: Calibration error < 0.10\n",
      "   Interval Width: MODERATE\n",
      "   Width Description: Moderate uncertainty\n",
      "\n",
      "ğŸ’¡ RECOMMENDATIONS:\n",
      "   â€¢ Collect more training data to reduce prediction variance\n",
      "   â€¢ Consider ensemble methods\n",
      "\n",
      "ğŸ“Š PER-ALPHA ANALYSIS:\n",
      "   Alpha    Target     Actual     Cal.Error    Width\n",
      "   ------------------------------------------------------------\n",
      "   0.01     99.0     % 98.9     % 0.0010       0.8690\n",
      "   0.05     95.0     % 99.6     % 0.0456       1.6416\n",
      "   0.10     90.0     % 97.6     % 0.0758       0.8730\n",
      "   0.20     80.0     % 93.4     % 0.1341       0.3298\n",
      "   0.30     70.0     % 90.1     % 0.2011       0.1913\n",
      "\n",
      "ğŸ”¬ TEST RESULTS (Compact):\n",
      "   Uncertainty Score: 0.6375946636000064\n",
      "\n",
      "ğŸŒŸ TOP 10 MOST IMPORTANT FEATURES:\n",
      "   Feature                        Importance\n",
      "   --------------------------------------------------\n",
      "   mean texture                   1.0000\n",
      "   mean smoothness                0.2531\n",
      "   mean area                      0.2180\n",
      "   mean perimeter                 0.1473\n",
      "   mean radius                    0.0633\n",
      "\n",
      "ğŸ“Š CRQR BY ALPHA (Overall Results Only):\n",
      "   5 alpha levels analyzed\n",
      "   (No sample-level data in compact mode)\n",
      "\n",
      "================================================================================\n",
      "ğŸ’¾ COMPACT JSON IS OPTIMIZED FOR:\n",
      "================================================================================\n",
      "âœ… AI/LLM validation (smaller token count)\n",
      "âœ… Automated testing pipelines\n",
      "âœ… CI/CD integration\n",
      "âœ… Quick metrics extraction\n",
      "âœ… Summary-based decision making\n",
      "\n",
      "âŒ NOT SUITABLE FOR:\n",
      "   â€¢ Sample-level analysis\n",
      "   â€¢ Detailed debugging\n",
      "   â€¢ Visualization generation\n",
      "   â€¢ Research with raw data needs\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“Š ANALYZING COMPACT JSON (optimized for AI validation)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load COMPACT JSON results\n",
    "with open(json_compact_path, 'r', encoding='utf-8') as f:\n",
    "    results_json = json.load(f)\n",
    "\n",
    "# Experiment Info\n",
    "exp_info = results_json['experiment_info']\n",
    "print(f\"\\nğŸ”¬ EXPERIMENT INFO:\")\n",
    "print(f\"   Test Type: {exp_info['test_type']}\")\n",
    "print(f\"   Experiment Type: {exp_info['experiment_type']}\")\n",
    "print(f\"   Generation Time: {exp_info['generation_time']}\")\n",
    "\n",
    "# Summary (AI-friendly)\n",
    "if 'summary' in results_json:\n",
    "    summary = results_json['summary']\n",
    "    \n",
    "    print(f\"\\nğŸ¯ KEY FINDINGS:\")\n",
    "    for finding in summary.get('key_findings', []):\n",
    "        print(f\"   â€¢ {finding}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ MODEL PERFORMANCE:\")\n",
    "    perf = summary.get('model_performance', {})\n",
    "    for key, value in perf.items():\n",
    "        if key != 'base_metrics':\n",
    "            print(f\"   {key}: {value}\")\n",
    "    \n",
    "    if 'base_metrics' in perf:\n",
    "        print(f\"   Base Metrics:\")\n",
    "        for metric, val in perf['base_metrics'].items():\n",
    "            print(f\"      {metric}: {val}\")\n",
    "    \n",
    "    print(f\"\\nğŸ” CALIBRATION QUALITY:\")\n",
    "    calib = summary.get('calibration_quality', {})\n",
    "    print(f\"   Status: {calib.get('status', 'N/A')}\")\n",
    "    print(f\"   Description: {calib.get('description', 'N/A')}\")\n",
    "    print(f\"   Interval Width: {calib.get('interval_width', 'N/A')}\")\n",
    "    print(f\"   Width Description: {calib.get('width_description', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ RECOMMENDATIONS:\")\n",
    "    for rec in summary.get('recommendations', []):\n",
    "        print(f\"   â€¢ {rec}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š PER-ALPHA ANALYSIS:\")\n",
    "    print(f\"   {'Alpha':<8} {'Target':<10} {'Actual':<10} {'Cal.Error':<12} {'Width'}\")\n",
    "    print(f\"   {'-'*60}\")\n",
    "    for alpha_data in summary.get('per_alpha_analysis', []):\n",
    "        print(f\"   {alpha_data['alpha']:<8.2f} \"\n",
    "              f\"{alpha_data['target_coverage']*100:<9.1f}% \"\n",
    "              f\"{alpha_data['actual_coverage']*100:<9.1f}% \"\n",
    "              f\"{alpha_data['calibration_error']:<12.4f} \"\n",
    "              f\"{alpha_data['mean_width']:.4f}\")\n",
    "\n",
    "# Test Results (compact)\n",
    "test_results = results_json['test_results']\n",
    "primary = test_results.get('primary_model', {})\n",
    "\n",
    "print(f\"\\nğŸ”¬ TEST RESULTS (Compact):\")\n",
    "print(f\"   Uncertainty Score: {primary.get('uncertainty_quality_score', 'N/A')}\")\n",
    "\n",
    "# Top Features\n",
    "if 'feature_importance_top10' in primary:\n",
    "    print(f\"\\nğŸŒŸ TOP 10 MOST IMPORTANT FEATURES:\")\n",
    "    print(f\"   {'Feature':<30} {'Importance'}\")\n",
    "    print(f\"   {'-'*50}\")\n",
    "    for feat, imp in primary['feature_importance_top10'].items():\n",
    "        print(f\"   {feat:<30} {imp:.4f}\")\n",
    "\n",
    "# CRQR Summary\n",
    "if 'crqr' in primary and 'by_alpha' in primary['crqr']:\n",
    "    by_alpha = primary['crqr']['by_alpha']\n",
    "    print(f\"\\nğŸ“Š CRQR BY ALPHA (Overall Results Only):\")\n",
    "    print(f\"   {len(by_alpha)} alpha levels analyzed\")\n",
    "    print(f\"   (No sample-level data in compact mode)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ğŸ’¾ COMPACT JSON IS OPTIMIZED FOR:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"âœ… AI/LLM validation (smaller token count)\")\n",
    "print(f\"âœ… Automated testing pipelines\")\n",
    "print(f\"âœ… CI/CD integration\")\n",
    "print(f\"âœ… Quick metrics extraction\")\n",
    "print(f\"âœ… Summary-based decision making\")\n",
    "print(f\"\\nâŒ NOT SUITABLE FOR:\")\n",
    "print(f\"   â€¢ Sample-level analysis\")\n",
    "print(f\"   â€¢ Detailed debugging\")\n",
    "print(f\"   â€¢ Visualization generation\")\n",
    "print(f\"   â€¢ Research with raw data needs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ Uncertainty Analysis Summary\n",
    "\n",
    "Extract key insights from uncertainty quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š UNCERTAINTY QUANTIFICATION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ˆ OVERALL STATISTICS:\n",
      "   Average Coverage: 95.91%\n",
      "   Average Calibration Error: 0.0915\n",
      "   Average Interval Width: 0.7809\n",
      "\n",
      "ğŸ¯ QUALITY ASSESSMENT:\n",
      "   ğŸŸ¡ Calibration: GOOD (error < 0.10)\n",
      "   ğŸŸ¡ Interval Width: MODERATE\n",
      "\n",
      "ğŸ’¡ RECOMMENDATIONS:\n",
      "   â€¢ High uncertainty detected - collect more training data\n",
      "   â€¢ Use ensemble methods to reduce prediction variance\n",
      "\n",
      "âœ… Always use uncertainty in critical decisions (medical, financial, safety)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ“Š UNCERTAINTY QUANTIFICATION SUMMARY\\n\" + \"=\"*70)\n",
    "\n",
    "# Calculate overall statistics\n",
    "if 'crqr' in primary and 'by_alpha' in primary['crqr']:\n",
    "    by_alpha = primary['crqr']['by_alpha']\n",
    "    \n",
    "    # Collect all coverage and calibration errors\n",
    "    coverages = []\n",
    "    cal_errors = []\n",
    "    widths = []\n",
    "    \n",
    "    for alpha, alpha_data in by_alpha.items():\n",
    "        overall = alpha_data.get('overall_result', {})\n",
    "        alpha_val = float(alpha)\n",
    "        target_cov = 1 - alpha_val\n",
    "        actual_cov = overall.get('coverage', 0)\n",
    "        avg_width = overall.get('mean_width', 0)\n",
    "        \n",
    "        coverages.append(actual_cov)\n",
    "        cal_errors.append(abs(target_cov - actual_cov))\n",
    "        widths.append(avg_width)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_coverage = np.mean(coverages)\n",
    "    avg_cal_error = np.mean(cal_errors)\n",
    "    avg_width = np.mean(widths)\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ OVERALL STATISTICS:\")\n",
    "    print(f\"   Average Coverage: {avg_coverage*100:.2f}%\")\n",
    "    print(f\"   Average Calibration Error: {avg_cal_error:.4f}\")\n",
    "    print(f\"   Average Interval Width: {avg_width:.4f}\")\n",
    "    \n",
    "    # Quality assessment\n",
    "    print(f\"\\nğŸ¯ QUALITY ASSESSMENT:\")\n",
    "    \n",
    "    if avg_cal_error < 0.05:\n",
    "        print(f\"   âœ… Calibration: EXCELLENT (error < 0.05)\")\n",
    "    elif avg_cal_error < 0.10:\n",
    "        print(f\"   ğŸŸ¡ Calibration: GOOD (error < 0.10)\")\n",
    "    else:\n",
    "        print(f\"   ğŸ”´ Calibration: NEEDS IMPROVEMENT (error â‰¥ 0.10)\")\n",
    "    \n",
    "    if avg_width < 0.5:\n",
    "        print(f\"   âœ… Interval Width: NARROW (confident predictions)\")\n",
    "    elif avg_width < 1.0:\n",
    "        print(f\"   ğŸŸ¡ Interval Width: MODERATE\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  Interval Width: WIDE (high uncertainty)\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nğŸ’¡ RECOMMENDATIONS:\")\n",
    "    \n",
    "    if avg_cal_error >= 0.10:\n",
    "        print(f\"   â€¢ Consider calibration methods (Platt scaling, isotonic regression)\")\n",
    "    \n",
    "    if avg_width > 0.5:\n",
    "        print(f\"   â€¢ High uncertainty detected - collect more training data\")\n",
    "        print(f\"   â€¢ Use ensemble methods to reduce prediction variance\")\n",
    "    \n",
    "    if avg_coverage < 0.90:\n",
    "        print(f\"   â€¢ Coverage below 90% - model may be overconfident\")\n",
    "        print(f\"   â€¢ Consider increasing interval width or recalibration\")\n",
    "    \n",
    "    print(f\"\\nâœ… Always use uncertainty in critical decisions (medical, financial, safety)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ Practical Decision Examples\n",
    "\n",
    "How to use uncertainty in real-world scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¼ PRACTICAL USE CASES\n",
      "======================================================================\n",
      "\n",
      "ğŸ¥ MEDICAL DIAGNOSIS - Breast Cancer\n",
      "----------------------------------------------------------------------\n",
      "   Patient ID: #001\n",
      "   Prediction: 100.0% probability of MALIGNANT\n",
      "   95% Confidence Interval: [85.0%, 100.0%]\n",
      "   Uncertainty Width: Â±15.0%\n",
      "   Actual: MALIGNANT\n",
      "\n",
      "   ğŸ“‹ DECISION PROTOCOL:\n",
      "   âš ï¸  HIGH RISK: Immediate biopsy recommended\n",
      "   Reason: Even with uncertainty, risk remains high\n",
      "\n",
      "\n",
      "ğŸ’° CREDIT APPROVAL - Default Risk\n",
      "----------------------------------------------------------------------\n",
      "   Customer ID: #12345\n",
      "   Prediction: 35.0% probability of DEFAULT\n",
      "   95% Confidence Interval: [27.0%, 43.0%]\n",
      "   Uncertainty Width: Â±8.0%\n",
      "\n",
      "   ğŸ“‹ DECISION PROTOCOL:\n",
      "   âœ… APPROVE with standard rate\n",
      "   Reason: Low uncertainty, risk well-quantified\n",
      "   Recommended Rate: 8.50%\n",
      "\n",
      "\n",
      "ğŸ”’ FRAUD DETECTION - Transaction Security\n",
      "----------------------------------------------------------------------\n",
      "   Transaction ID: #TXN789\n",
      "   Prediction: 65.0% probability of FRAUD\n",
      "   95% Confidence Interval: [40.0%, 90.0%]\n",
      "   Uncertainty Width: Â±25.0%\n",
      "\n",
      "   ğŸ“‹ DECISION PROTOCOL:\n",
      "   ğŸ” BLOCK + Require 2FA verification\n",
      "   Reason: High uncertainty - apply maximum security\n",
      "   Action: SMS code + Email confirmation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guhaase/.cache/pypoetry/virtualenvs/deepbridge-3F2lzRH3-py3.12/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ’¼ PRACTICAL USE CASES\\n\" + \"=\"*70)\n",
    "\n",
    "# Simulate predictions with uncertainty\n",
    "test_sample = X_test.iloc[0]\n",
    "pred_proba = model.predict_proba([test_sample])[0]\n",
    "actual_class = y_test.iloc[0]\n",
    "\n",
    "# Simulate uncertainty interval (in real scenario, this comes from CRQR)\n",
    "uncertainty_margin = 0.15  # Example: Â±15%\n",
    "prob_malignant = pred_proba[0]\n",
    "lower_bound = max(0, prob_malignant - uncertainty_margin)\n",
    "upper_bound = min(1, prob_malignant + uncertainty_margin)\n",
    "\n",
    "print(f\"\\nğŸ¥ MEDICAL DIAGNOSIS - Breast Cancer\\n\" + \"-\"*70)\n",
    "print(f\"   Patient ID: #001\")\n",
    "print(f\"   Prediction: {prob_malignant*100:.1f}% probability of MALIGNANT\")\n",
    "print(f\"   95% Confidence Interval: [{lower_bound*100:.1f}%, {upper_bound*100:.1f}%]\")\n",
    "print(f\"   Uncertainty Width: Â±{uncertainty_margin*100:.1f}%\")\n",
    "print(f\"   Actual: {'MALIGNANT' if actual_class == 0 else 'BENIGN'}\")\n",
    "\n",
    "# Decision logic\n",
    "print(f\"\\n   ğŸ“‹ DECISION PROTOCOL:\")\n",
    "if upper_bound > 0.7:  # High risk even in worst case\n",
    "    print(f\"   âš ï¸  HIGH RISK: Immediate biopsy recommended\")\n",
    "    print(f\"   Reason: Even with uncertainty, risk remains high\")\n",
    "elif lower_bound < 0.3:  # Low risk even in worst case\n",
    "    print(f\"   âœ… LOW RISK: Regular monitoring\")\n",
    "    print(f\"   Reason: Risk remains low even considering uncertainty\")\n",
    "else:  # Uncertain case\n",
    "    print(f\"   ğŸŸ¡ UNCERTAIN: Additional tests recommended\")\n",
    "    print(f\"   Reason: High uncertainty - need more information\")\n",
    "    print(f\"   Suggested: Ultrasound, MRI, or second opinion\")\n",
    "\n",
    "# Financial example\n",
    "print(f\"\\n\\nğŸ’° CREDIT APPROVAL - Default Risk\\n\" + \"-\"*70)\n",
    "default_prob = 0.35\n",
    "uncertainty_margin = 0.08\n",
    "lower_default = max(0, default_prob - uncertainty_margin)\n",
    "upper_default = min(1, default_prob + uncertainty_margin)\n",
    "\n",
    "print(f\"   Customer ID: #12345\")\n",
    "print(f\"   Prediction: {default_prob*100:.1f}% probability of DEFAULT\")\n",
    "print(f\"   95% Confidence Interval: [{lower_default*100:.1f}%, {upper_default*100:.1f}%]\")\n",
    "print(f\"   Uncertainty Width: Â±{uncertainty_margin*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n   ğŸ“‹ DECISION PROTOCOL:\")\n",
    "if uncertainty_margin < 0.10:  # Low uncertainty\n",
    "    print(f\"   âœ… APPROVE with standard rate\")\n",
    "    print(f\"   Reason: Low uncertainty, risk well-quantified\")\n",
    "    print(f\"   Recommended Rate: {5.0 + default_prob*10:.2f}%\")\n",
    "else:  # High uncertainty\n",
    "    print(f\"   âš ï¸  APPROVE with adjusted rate OR require guarantor\")\n",
    "    print(f\"   Reason: High uncertainty in risk assessment\")\n",
    "    print(f\"   Recommended Rate: {5.0 + upper_default*10:.2f}% (worst-case based)\")\n",
    "\n",
    "# Security example\n",
    "print(f\"\\n\\nğŸ”’ FRAUD DETECTION - Transaction Security\\n\" + \"-\"*70)\n",
    "fraud_prob = 0.65\n",
    "uncertainty_margin = 0.25\n",
    "lower_fraud = max(0, fraud_prob - uncertainty_margin)\n",
    "upper_fraud = min(1, fraud_prob + uncertainty_margin)\n",
    "\n",
    "print(f\"   Transaction ID: #TXN789\")\n",
    "print(f\"   Prediction: {fraud_prob*100:.1f}% probability of FRAUD\")\n",
    "print(f\"   95% Confidence Interval: [{lower_fraud*100:.1f}%, {upper_fraud*100:.1f}%]\")\n",
    "print(f\"   Uncertainty Width: Â±{uncertainty_margin*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n   ğŸ“‹ DECISION PROTOCOL:\")\n",
    "if uncertainty_margin > 0.20:  # High uncertainty\n",
    "    print(f\"   ğŸ” BLOCK + Require 2FA verification\")\n",
    "    print(f\"   Reason: High uncertainty - apply maximum security\")\n",
    "    print(f\"   Action: SMS code + Email confirmation\")\n",
    "elif fraud_prob > 0.70:\n",
    "    print(f\"   â›” BLOCK transaction\")\n",
    "    print(f\"   Reason: High fraud probability, low uncertainty\")\n",
    "else:\n",
    "    print(f\"   ğŸŸ¡ FLAG for review\")\n",
    "    print(f\"   Reason: Moderate risk, further analysis needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”Ÿ Visualize Uncertainty Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Uncertainty Statistics:\n",
      "   Mean uncertainty: Â±12.1%\n",
      "   Min uncertainty: Â±5.2%\n",
      "   Max uncertainty: Â±19.8%\n",
      "   Std uncertainty: 4.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1345512/1136276840.py:41: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Visualize prediction probabilities with uncertainty\n",
    "n_samples = 50\n",
    "probas = model.predict_proba(X_test[:n_samples])[:, 1]  # Probability of benign\n",
    "actual_labels = y_test.iloc[:n_samples].values\n",
    "\n",
    "# Simulate uncertainty margins\n",
    "uncertainty_margins = np.random.uniform(0.05, 0.20, n_samples)\n",
    "lower_bounds = np.maximum(0, probas - uncertainty_margins)\n",
    "upper_bounds = np.minimum(1, probas + uncertainty_margins)\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Predictions with uncertainty intervals\n",
    "x = np.arange(n_samples)\n",
    "colors = ['red' if label == 0 else 'green' for label in actual_labels]\n",
    "\n",
    "ax1.fill_between(x, lower_bounds, upper_bounds, alpha=0.3, color='skyblue',\n",
    "                 label='95% Confidence Interval')\n",
    "ax1.scatter(x, probas, c=colors, s=80, alpha=0.7, edgecolors='black', linewidth=1.5,\n",
    "           label='Prediction (Red=Malignant, Green=Benign)')\n",
    "ax1.axhline(y=0.5, color='gray', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "ax1.set_xlabel('Sample', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Probability (Benign)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Predictions with Uncertainty Intervals', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10, loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(-0.05, 1.05)\n",
    "\n",
    "# Plot 2: Uncertainty distribution\n",
    "ax2.hist(uncertainty_margins, bins=20, color='coral', alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(x=np.mean(uncertainty_margins), color='red', linestyle='--', \n",
    "           linewidth=2, label=f'Mean: {np.mean(uncertainty_margins):.3f}')\n",
    "ax2.set_xlabel('Uncertainty Margin', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Distribution of Uncertainty', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ“Š Uncertainty Statistics:\")\n",
    "print(f\"   Mean uncertainty: Â±{np.mean(uncertainty_margins)*100:.1f}%\")\n",
    "print(f\"   Min uncertainty: Â±{np.min(uncertainty_margins)*100:.1f}%\")\n",
    "print(f\"   Max uncertainty: Â±{np.max(uncertainty_margins)*100:.1f}%\")\n",
    "print(f\"   Std uncertainty: {np.std(uncertainty_margins)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£1ï¸âƒ£ Generate Static Report (Optional)\n",
    "\n",
    "Generate a static HTML report with matplotlib charts (for PDF export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Generating static HTML report...\n",
      "\n",
      "2025-11-12 20:59:36,587 - deepbridge.reports - INFO - Using static renderer for uncertainty report\n",
      "2025-11-12 20:59:36,588 - deepbridge.reports - INFO - Generating static uncertainty report to: /home/guhaase/projetos/DeepBridge/examples/notebooks/03_validation_tests/outputs/uncertainty_classification/uncertainty_classification_static.html\n",
      "2025-11-12 20:59:36,589 - deepbridge.reports - INFO - Found template at: /home/guhaase/projetos/DeepBridge/deepbridge/templates/report_types/uncertainty/static/index.html\n",
      "2025-11-12 20:59:36,589 - deepbridge.reports - INFO - Using static template: /home/guhaase/projetos/DeepBridge/deepbridge/templates/report_types/uncertainty/static/index.html\n",
      "2025-11-12 20:59:36,591 - deepbridge.reports - INFO - CSS compiled successfully using CSSManager for static uncertainty report\n",
      "2025-11-12 20:59:36,630 - deepbridge.reports - INFO - Starting data transformation with standard transformer\n",
      "2025-11-12 20:59:36,631 - deepbridge.reports - INFO - Transforming uncertainty data structure...\n",
      "2025-11-12 20:59:36,656 - deepbridge.reports - INFO - Found 'primary_model' key, extracting data...\n",
      "2025-11-12 20:59:36,657 - deepbridge.reports - INFO - Processing alternative models data...\n",
      "2025-11-12 20:59:37,530 - deepbridge.reports - INFO - Standard transformer produced data with keys: ['primary_model', 'alternative_models', 'config', 'test_predictions', 'test_labels', 'uncertainty_quality_score', 'avg_coverage_error', 'avg_normalized_width', 'alphas', 'feature_importance', 'alpha_levels', 'timestamp', 'model_type', 'dataset', 'predictions', 'metrics', 'initial_results', 'initial_model_evaluation', 'experiment_type', 'crqr', 'plot_data', 'reliability_analysis', 'reliability_charts', 'model_name', 'metric', 'uncertainty_score', 'avg_coverage', 'avg_width', 'method']\n",
      "2025-11-12 20:59:37,531 - deepbridge.reports - INFO - Transforming uncertainty data for static report - Using improved transformer\n",
      "2025-11-12 20:59:37,531 - deepbridge.reports - INFO - Data keys: ['primary_model', 'alternative_models', 'config', 'test_predictions', 'test_labels', 'uncertainty_quality_score', 'avg_coverage_error', 'avg_normalized_width', 'alphas', 'feature_importance', 'alpha_levels', 'timestamp', 'model_type', 'dataset', 'predictions', 'metrics', 'initial_results', 'initial_model_evaluation', 'experiment_type']\n",
      "2025-11-12 20:59:37,532 - deepbridge.reports - INFO - Primary model keys: ['crqr', 'alphas', 'feature_importance', 'avg_coverage_error', 'avg_normalized_width', 'uncertainty_quality_score', 'plot_data', 'reliability_analysis', 'reliability_charts', 'test_predictions', 'test_labels', 'model_type', 'metrics']\n",
      "2025-11-12 20:59:37,828 - deepbridge.reports - INFO - Data is serializable\n",
      "2025-11-12 20:59:37,828 - deepbridge.reports - INFO - [TRANSFORMER] primary_model keys: ['crqr', 'alphas', 'feature_importance', 'avg_coverage_error', 'avg_normalized_width', 'uncertainty_quality_score', 'plot_data', 'reliability_analysis', 'reliability_charts', 'test_predictions', 'test_labels', 'model_type', 'metrics']\n",
      "2025-11-12 20:59:37,829 - deepbridge.reports - INFO - alpha_comparison keys: ['alphas', 'coverages', 'expected_coverages', 'mean_widths']\n",
      "2025-11-12 20:59:37,830 - deepbridge.reports - INFO - Transformed calibration_results: {'alpha_values': [0.01, 0.05, 0.1, 0.2, 0.3], 'coverage_values': [np.float64(0.989010989010989), np.float64(0.9956043956043956), np.float64(0.9758241758241758), np.float64(0.9340659340659341), np.float64(0.9010989010989011)], 'expected_coverages': [0.99, 0.95, 0.9, 0.8, 0.7], 'width_values': [np.float64(0.8689939268472979), np.float64(1.641643321028634), np.float64(0.8730343654871155), np.float64(0.3297627617866611), np.float64(0.19126994946556847)]}\n",
      "2025-11-12 20:59:37,830 - deepbridge.reports - DEBUG - [TRANSFORM_DEBUG] Found coverage_vs_width in plot_data: dict_keys(['coverages', 'mean_widths', 'alphas'])\n",
      "2025-11-12 20:59:37,831 - deepbridge.reports - INFO - [TRANSFORM_DEBUG] Found reliability_analysis in primary_model\n",
      "2025-11-12 20:59:37,831 - deepbridge.reports - INFO - [TRANSFORM_DEBUG] reliability_analysis keys: ['mean texture', 'mean smoothness', 'mean area']\n",
      "2025-11-12 20:59:37,832 - deepbridge.reports - INFO - [TRANSFORM_DEBUG] Copied reliability_analysis to output\n",
      "2025-11-12 20:59:37,832 - deepbridge.reports - INFO - [TRANSFORM_DEBUG] Found test_predictions in primary_model with shape: (455, 2)\n",
      "2025-11-12 20:59:37,833 - deepbridge.reports - INFO - [TRANSFORM_DEBUG] Found test_labels in primary_model with shape: (455,)\n",
      "2025-11-12 20:59:37,833 - deepbridge.reports - INFO - Transformed uncertainty data: uncertainty_score=0.6375946636000064, coverage=0, mean_width=0.7809408649230555\n",
      "2025-11-12 20:59:37,837 - deepbridge.reports - INFO - Static transformer produced data with keys: ['model_name', 'test_type', 'timestamp', 'metrics', 'feature_importance', 'model_type', 'alpha_levels', 'uncertainty_score', 'mean_width', 'calibration_results', 'feature_importance_data', 'interval_widths', 'coverage_vs_width', 'reliability_analysis', 'test_predictions', 'test_labels', 'predictions', 'dataset', 'alternative_models', 'models_data', 'feature_subset', 'feature_subset_display', 'coverage']\n",
      "2025-11-12 20:59:37,838 - deepbridge.reports - INFO - Found initial_results in results, transforming...\n",
      "2025-11-12 20:59:37,838 - deepbridge.reports.transformers.initial_results - INFO - Transforming initial results data\n",
      "2025-11-12 20:59:37,839 - deepbridge.reports.transformers.initial_results - INFO - Transformed data for 1 models\n",
      "2025-11-12 20:59:37,840 - deepbridge.reports - INFO - Attempting to import deepbridge.templates.report_types.uncertainty.static.charts.enhanced_charts\n",
      "2025-11-12 20:59:37,840 - deepbridge.reports - INFO - Successfully imported module: deepbridge.templates.report_types.uncertainty.static.charts.enhanced_charts\n",
      "2025-11-12 20:59:37,841 - deepbridge.reports - INFO - Module contents: ['Any', 'Dict', 'EnhancedUncertaintyCharts', 'List', 'Optional', 'Union', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'base64', 'io', 'logger', 'logging', 'np', 'traceback']\n",
      "2025-11-12 20:59:37,841 - deepbridge.reports - INFO - Successfully loaded enhanced uncertainty charts with methods: ['generate_reliability_distribution', 'generate_marginal_bandwidth_chart', 'generate_interval_widths_boxplot', 'generate_model_metrics_comparison']\n",
      "2025-11-12 20:59:37,843 - deepbridge.reports - INFO - Using new modular chart generator system for uncertainty visualization\n",
      "2025-11-12 20:59:37,844 - deepbridge.reports - INFO - DADOS PARA COVERAGE VS EXPECTED CHART:\n",
      "2025-11-12 20:59:37,845 - deepbridge.reports - INFO -   - calibration_results disponÃ­vel: dict_keys(['alpha_values', 'coverage_values', 'expected_coverages', 'width_values'])\n",
      "2025-11-12 20:59:37,846 - deepbridge.reports - INFO -   - alpha_values: 5 valores\n",
      "2025-11-12 20:59:37,846 - deepbridge.reports - INFO -   - coverage_values: 5 valores\n",
      "2025-11-12 20:59:37,847 - deepbridge.reports - INFO -   - expected_coverages: 5 valores\n",
      "2025-11-12 20:59:37,847 - deepbridge.reports - INFO -   - width_values: 5 valores\n",
      "2025-11-12 20:59:37,848 - deepbridge.reports - INFO -   - alpha_levels: [0.01, 0.05, 0.1, 0.2, 0.3]\n",
      "2025-11-12 20:59:37,851 - deepbridge.reports - INFO - Alpha values: [0.01, 0.05, 0.1, 0.2, 0.3] (type: <class 'list'>, len: 5)\n",
      "2025-11-12 20:59:37,852 - deepbridge.reports - INFO - Coverage values: [np.float64(0.989010989010989), np.float64(0.9956043956043956), np.float64(0.9758241758241758), np.float64(0.9340659340659341), np.float64(0.9010989010989011)] (type: <class 'list'>, len: 5)\n",
      "2025-11-12 20:59:37,853 - deepbridge.reports - INFO - Expected coverages: [0.99, 0.95, 0.9, 0.8, 0.7] (type: <class 'list'>, len: 5)\n",
      "2025-11-12 20:59:37,853 - deepbridge.reports - INFO - Alpha values for generate_coverage_vs_expected: [0.01, 0.05, 0.1, 0.2, 0.3]\n",
      "2025-11-12 20:59:37,854 - deepbridge.reports - INFO - Coverage values for generate_coverage_vs_expected: [np.float64(0.989010989010989), np.float64(0.9956043956043956), np.float64(0.9758241758241758), np.float64(0.9340659340659341), np.float64(0.9010989010989011)]\n",
      "2025-11-12 20:59:37,855 - deepbridge.reports - INFO - Expected coverages for generate_coverage_vs_expected: [0.99, 0.95, 0.9, 0.8, 0.7]\n",
      "2025-11-12 20:59:37,855 - deepbridge.reports - INFO - Formatted data for coverage chart: {'Primary Model': {'alphas': [0.01, 0.05, 0.1, 0.2, 0.3], 'coverages': [np.float64(0.989010989010989), np.float64(0.9956043956043956), np.float64(0.9758241758241758), np.float64(0.9340659340659341), np.float64(0.9010989010989011)], 'expected_coverages': [0.99, 0.95, 0.9, 0.8, 0.7]}}\n",
      "2025-11-12 20:59:37,856 - deepbridge.reports - INFO - Validating coverage vs expected data: <class 'dict'>\n",
      "2025-11-12 20:59:37,856 - deepbridge.reports - INFO - models_data keys: ['Primary Model']\n",
      "2025-11-12 20:59:37,857 - deepbridge.reports - INFO - Checking model data for 'Primary Model': <class 'dict'>\n",
      "2025-11-12 20:59:37,857 - deepbridge.reports - INFO - model_data keys for 'Primary Model': ['alphas', 'coverages', 'expected_coverages']\n",
      "2025-11-12 20:59:37,858 - deepbridge.reports - INFO - 'alphas' for model 'Primary Model' has 5 values\n",
      "2025-11-12 20:59:37,859 - deepbridge.reports - INFO - 'coverages' for model 'Primary Model' has 5 values\n",
      "2025-11-12 20:59:37,859 - deepbridge.reports - INFO - 'expected_coverages' for model 'Primary Model' has 5 values\n",
      "2025-11-12 20:59:37,860 - deepbridge.reports - INFO - Found valid data for model 'Primary Model'\n",
      "2025-11-12 20:59:38,010 - deepbridge.reports - INFO - Generated coverage vs expected coverage chart\n",
      "2025-11-12 20:59:38,011 - deepbridge.reports - INFO - DADOS PARA WIDTH VS COVERAGE CHART:\n",
      "2025-11-12 20:59:38,012 - deepbridge.reports - INFO -   - calibration_results disponÃ­vel: dict_keys(['alpha_values', 'coverage_values', 'expected_coverages', 'width_values'])\n",
      "2025-11-12 20:59:38,012 - deepbridge.reports - INFO -   - width_values: 5 valores\n",
      "2025-11-12 20:59:38,013 - deepbridge.reports - INFO -   - coverage_values: 5 valores\n",
      "2025-11-12 20:59:38,014 - deepbridge.reports - INFO - Width values: [np.float64(0.8689939268472979), np.float64(1.641643321028634), np.float64(0.8730343654871155), np.float64(0.3297627617866611), np.float64(0.19126994946556847)] (type: <class 'list'>, len: 5)\n",
      "2025-11-12 20:59:38,015 - deepbridge.reports - INFO - Coverage values: [np.float64(0.989010989010989), np.float64(0.9956043956043956), np.float64(0.9758241758241758), np.float64(0.9340659340659341), np.float64(0.9010989010989011)] (type: <class 'list'>, len: 5)\n",
      "2025-11-12 20:59:38,015 - deepbridge.reports - INFO - Found calibration_results: 5 width values, 5 coverage values\n",
      "2025-11-12 20:59:38,016 - deepbridge.reports - INFO - Generating width_vs_coverage chart with data for models: ['Primary Model']\n",
      "2025-11-12 20:59:38,017 - deepbridge.reports - INFO - Validating width vs coverage data: <class 'dict'>\n",
      "2025-11-12 20:59:38,017 - deepbridge.reports - INFO - models_data keys: ['Primary Model']\n",
      "2025-11-12 20:59:38,018 - deepbridge.reports - INFO - Checking model data for 'Primary Model': <class 'dict'>\n",
      "2025-11-12 20:59:38,018 - deepbridge.reports - INFO - model_data keys for 'Primary Model': ['widths', 'coverages']\n",
      "2025-11-12 20:59:38,019 - deepbridge.reports - INFO - 'coverages' for model 'Primary Model' has 5 values\n",
      "2025-11-12 20:59:38,019 - deepbridge.reports - INFO - 'widths' for model 'Primary Model' has 5 values\n",
      "2025-11-12 20:59:38,020 - deepbridge.reports - INFO - Found valid data for model 'Primary Model'\n",
      "2025-11-12 20:59:38,159 - deepbridge.reports - INFO - Generated width vs coverage chart\n",
      "2025-11-12 20:59:38,160 - deepbridge.reports - INFO - DADOS PARA UNCERTAINTY METRICS CHART:\n",
      "2025-11-12 20:59:38,161 - deepbridge.reports - INFO -   - uncertainty_score: 0.6375946636000064\n",
      "2025-11-12 20:59:38,161 - deepbridge.reports - INFO -   - coverage: 0\n",
      "2025-11-12 20:59:38,163 - deepbridge.reports - INFO -   - mean_width: 0.7809408649230555\n",
      "2025-11-12 20:59:38,163 - deepbridge.reports - INFO -   - metrics disponÃ­vel: ['accuracy', 'roc_auc', 'f1', 'precision', 'recall']\n",
      "2025-11-12 20:59:38,173 - deepbridge.reports - ERROR - Error generating uncertainty metrics chart: 'str' object has no attribute 'keys'\n",
      "2025-11-12 20:59:38,174 - deepbridge.reports - INFO - DADOS PARA FEATURE IMPORTANCE CHART:\n",
      "2025-11-12 20:59:38,174 - deepbridge.reports - INFO -   - feature_importance type: <class 'dict'>\n",
      "2025-11-12 20:59:38,175 - deepbridge.reports - INFO -   - feature_importance tem 5 caracterÃ­sticas\n",
      "2025-11-12 20:59:38,176 - deepbridge.reports - INFO -   - Exemplo: mean texture: 1.0\n",
      "2025-11-12 20:59:38,176 - deepbridge.reports - INFO -   - Exemplo: mean smoothness: 0.2531435914528217\n",
      "2025-11-12 20:59:38,177 - deepbridge.reports - INFO -   - Exemplo: mean area: 0.2179902926298239\n",
      "2025-11-12 20:59:38,181 - deepbridge.reports - INFO -   - Exemplo: mean perimeter: 0.14730294224913473\n",
      "2025-11-12 20:59:38,182 - deepbridge.reports - INFO -   - Exemplo: mean radius: 0.06331312872511648\n",
      "2025-11-12 20:59:38,322 - deepbridge.reports - INFO - Generated feature importance chart\n",
      "2025-11-12 20:59:38,323 - deepbridge.reports - INFO - DADOS PARA RELIABILITY BANDWIDTH CHART:\n",
      "2025-11-12 20:59:38,324 - deepbridge.reports - INFO -   - Found test_predictions in report_data\n",
      "2025-11-12 20:59:38,325 - deepbridge.reports - INFO -   - Found test_labels in report_data\n",
      "2025-11-12 20:59:38,325 - deepbridge.reports - INFO -   - Primary model data prepared: y_true shape (455,), y_prob shape (455,)\n",
      "2025-11-12 20:59:38,326 - deepbridge.reports - ERROR - Error generating reliability bandwidth chart: 'UncertaintyChartGenerator' object has no attribute 'generate_reliability_bandwidth'\n",
      "2025-11-12 20:59:38,326 - deepbridge.reports - INFO - DADOS PARA RELIABILITY PERFORMANCE CHART:\n",
      "2025-11-12 20:59:38,327 - deepbridge.reports - ERROR - Error generating reliability performance chart: 'UncertaintyChartGenerator' object has no attribute 'generate_reliability_performance'\n",
      "2025-11-12 20:59:38,328 - deepbridge.reports - INFO - DADOS PARA MODEL COMPARISON CHART:\n",
      "2025-11-12 20:59:38,328 - deepbridge.reports - INFO -   - alternative_models: 0\n",
      "2025-11-12 20:59:38,329 - deepbridge.reports - INFO -   - Modelos alternativos: []\n",
      "2025-11-12 20:59:38,330 - deepbridge.reports - INFO - generate_model_comparison called with: models_data=<class 'dict'>\n",
      "2025-11-12 20:59:38,330 - deepbridge.reports - INFO - models_data keys: ['model_name', 'test_type', 'timestamp', 'metrics', 'feature_importance', 'model_type', 'alpha_levels', 'uncertainty_score', 'mean_width', 'calibration_results', 'feature_importance_data', 'interval_widths', 'coverage_vs_width', 'reliability_analysis', 'test_predictions', 'test_labels', 'predictions', 'dataset', 'alternative_models', 'models_data', 'feature_subset', 'feature_subset_display', 'coverage', 'initial_results']\n",
      "2025-11-12 20:59:38,331 - deepbridge.reports - INFO - Added primary model 'RandomForest Classifier' with metrics: {'uncertainty_score': np.float64(0.6375946636000064), 'coverage': 0, 'mean_width': np.float64(0.7809408649230555)}\n",
      "2025-11-12 20:59:38,332 - deepbridge.reports - INFO - Formatted data for model comparison: {'RandomForest Classifier': {'uncertainty_score': np.float64(0.6375946636000064), 'coverage': 0, 'mean_width': np.float64(0.7809408649230555)}}\n",
      "2025-11-12 20:59:38,333 - deepbridge.reports - INFO - Validating model comparison data: <class 'dict'>\n",
      "2025-11-12 20:59:38,333 - deepbridge.reports - INFO - models_data keys: ['RandomForest Classifier']\n",
      "2025-11-12 20:59:38,334 - deepbridge.reports - INFO - Model comparison data validation result: False\n",
      "2025-11-12 20:59:38,334 - deepbridge.reports - WARNING - Invalid data provided for model comparison chart\n",
      "2025-11-12 20:59:38,335 - deepbridge.reports - INFO - DADOS PARA PERFORMANCE GAP BY ALPHA CHART:\n",
      "2025-11-12 20:59:38,336 - deepbridge.reports - INFO -   - alpha_levels: [0.01, 0.05, 0.1, 0.2, 0.3]\n",
      "2025-11-12 20:59:38,336 - deepbridge.reports - INFO -   - calibration_results disponÃ­vel: dict_keys(['alpha_values', 'coverage_values', 'expected_coverages', 'width_values'])\n",
      "2025-11-12 20:59:38,337 - deepbridge.reports - INFO -   - alpha_values: 5 valores\n",
      "2025-11-12 20:59:38,338 - deepbridge.reports - INFO -   - coverage_values: 5 valores\n",
      "2025-11-12 20:59:38,338 - deepbridge.reports - INFO -   - expected_coverages: 5 valores\n",
      "2025-11-12 20:59:38,340 - deepbridge.reports - INFO -   - Argumentos necessÃ¡rios para generate_performance_gap_by_alpha: ['self', 'models_data', 'title', 'add_annotations']\n",
      "2025-11-12 20:59:38,341 - deepbridge.reports - INFO - generate_performance_gap_by_alpha called with: models_data=<class 'dict'>, title=Performance Gap by Alpha Level, add_annotations=True\n",
      "2025-11-12 20:59:38,342 - deepbridge.reports - INFO - models_data keys: ['model_name', 'test_type', 'timestamp', 'metrics', 'feature_importance', 'model_type', 'alpha_levels', 'uncertainty_score', 'mean_width', 'calibration_results', 'feature_importance_data', 'interval_widths', 'coverage_vs_width', 'reliability_analysis', 'test_predictions', 'test_labels', 'predictions', 'dataset', 'alternative_models', 'models_data', 'feature_subset', 'feature_subset_display', 'coverage', 'initial_results']\n",
      "2025-11-12 20:59:38,343 - deepbridge.reports - INFO - calibration_results keys: ['alpha_values', 'coverage_values', 'expected_coverages', 'width_values']\n",
      "2025-11-12 20:59:38,344 - deepbridge.reports - INFO - alpha_levels: [0.01, 0.05, 0.1, 0.2, 0.3]\n",
      "2025-11-12 20:59:38,345 - deepbridge.reports - INFO - Validating performance gap by alpha data with: type=<class 'dict'>\n",
      "2025-11-12 20:59:38,345 - deepbridge.reports - INFO - models_data keys: ['model_name', 'test_type', 'timestamp', 'metrics', 'feature_importance', 'model_type', 'alpha_levels', 'uncertainty_score', 'mean_width', 'calibration_results', 'feature_importance_data', 'interval_widths', 'coverage_vs_width', 'reliability_analysis', 'test_predictions', 'test_labels', 'predictions', 'dataset', 'alternative_models', 'models_data', 'feature_subset', 'feature_subset_display', 'coverage', 'initial_results']\n",
      "2025-11-12 20:59:38,346 - deepbridge.reports - INFO - calibration_results type: <class 'dict'>\n",
      "2025-11-12 20:59:38,347 - deepbridge.reports - INFO - calibration_results keys: ['alpha_values', 'coverage_values', 'expected_coverages', 'width_values']\n",
      "2025-11-12 20:59:38,347 - deepbridge.reports - INFO - Has required data: alpha_values=True, coverage_values=True, expected_coverages=True\n",
      "2025-11-12 20:59:38,700 - deepbridge.reports - INFO - Generated performance gap by alpha chart\n",
      "2025-11-12 20:59:38,701 - deepbridge.reports - INFO - Generating enhanced uncertainty charts\n",
      "2025-11-12 20:59:38,702 - deepbridge.reports - INFO - Attempting to generate reliability distribution chart\n",
      "2025-11-12 20:59:38,703 - deepbridge.reports - INFO - reliability_analysis keys: ['mean texture', 'mean smoothness', 'mean area']\n",
      "2025-11-12 20:59:38,703 - deepbridge.reports - INFO -   - mean texture is a dictionary with keys: ['bins', 'confidence_scores', 'sample_counts', 'low_confidence_regions', 'high_confidence_regions', 'avg_confidence', 'min_confidence', 'max_confidence', 'feature_name', 'n_bins']\n",
      "2025-11-12 20:59:38,704 - deepbridge.reports - INFO -   - mean smoothness is a dictionary with keys: ['bins', 'confidence_scores', 'sample_counts', 'low_confidence_regions', 'high_confidence_regions', 'avg_confidence', 'min_confidence', 'max_confidence', 'feature_name', 'n_bins']\n",
      "2025-11-12 20:59:38,705 - deepbridge.reports - INFO -   - mean area is a dictionary with keys: ['bins', 'confidence_scores', 'sample_counts', 'low_confidence_regions', 'high_confidence_regions', 'avg_confidence', 'min_confidence', 'max_confidence', 'feature_name', 'n_bins']\n",
      "2025-11-12 20:59:38,707 - deepbridge.reports - WARNING - feature_distributions not available in reliability_analysis\n",
      "2025-11-12 20:59:38,707 - deepbridge.reports - INFO - Selected top feature: mean texture\n",
      "2025-11-12 20:59:38,708 - deepbridge.reports - INFO - Calling generate_reliability_distribution\n",
      "2025-11-12 20:59:38,709 - deepbridge.reports - INFO - [DISTRIBUTION_DEBUG] generate_reliability_distribution called with data keys: ['model_name', 'test_type', 'timestamp', 'metrics', 'feature_importance', 'model_type', 'alpha_levels', 'uncertainty_score', 'mean_width', 'calibration_results', 'feature_importance_data', 'interval_widths', 'coverage_vs_width', 'reliability_analysis', 'test_predictions', 'test_labels', 'predictions', 'dataset', 'alternative_models', 'models_data', 'feature_subset', 'feature_subset_display', 'coverage', 'initial_results']\n",
      "2025-11-12 20:59:38,709 - deepbridge.reports - INFO - [DISTRIBUTION_DEBUG] reliability_analysis keys: ['mean texture', 'mean smoothness', 'mean area']\n",
      "2025-11-12 20:59:38,710 - deepbridge.reports - ERROR - [DISTRIBUTION_DEBUG] No feature_distributions found in reliability_analysis\n",
      "2025-11-12 20:59:38,711 - deepbridge.reports - WARNING - generate_reliability_distribution returned None\n",
      "2025-11-12 20:59:38,712 - deepbridge.reports - WARNING - marginal_bandwidth not available in report_data\n",
      "2025-11-12 20:59:38,712 - deepbridge.reports - INFO - Attempting to generate interval widths boxplot\n",
      "2025-11-12 20:59:38,713 - deepbridge.reports - INFO - interval_widths is a list with 5 elements\n",
      "2025-11-12 20:59:38,714 - deepbridge.reports - INFO - First element is a list with 455 values\n",
      "2025-11-12 20:59:38,714 - deepbridge.reports - INFO - Calling generate_interval_widths_boxplot\n",
      "2025-11-12 20:59:38,715 - deepbridge.reports - INFO - [INTERVAL_DEBUG] generate_interval_widths_boxplot called with data keys: ['model_name', 'test_type', 'timestamp', 'metrics', 'feature_importance', 'model_type', 'alpha_levels', 'uncertainty_score', 'mean_width', 'calibration_results', 'feature_importance_data', 'interval_widths', 'coverage_vs_width', 'reliability_analysis', 'test_predictions', 'test_labels', 'predictions', 'dataset', 'alternative_models', 'models_data', 'feature_subset', 'feature_subset_display', 'coverage', 'initial_results']\n",
      "2025-11-12 20:59:38,715 - deepbridge.reports - INFO - [INTERVAL_DEBUG] interval_widths type: <class 'list'>\n",
      "2025-11-12 20:59:38,716 - deepbridge.reports - INFO - [INTERVAL_DEBUG] interval_widths is a list with 5 elements\n",
      "2025-11-12 20:59:38,716 - deepbridge.reports - INFO - [INTERVAL_DEBUG] First element type: <class 'list'>\n",
      "2025-11-12 20:59:38,717 - deepbridge.reports - INFO - [INTERVAL_DEBUG] First element is a list with 455 values\n",
      "2025-11-12 20:59:38,718 - deepbridge.reports - INFO - [INTERVAL_DEBUG] Sample values: [0.7532931480697986, 0.7828750612719368, 0.9078119635678925, 0.8515523976054558, 0.8270426749178645]\n",
      "2025-11-12 20:59:38,824 - deepbridge.reports - INFO - Successfully generated interval widths boxplot\n",
      "2025-11-12 20:59:38,825 - deepbridge.reports - INFO - Generating model metrics comparison\n",
      "2025-11-12 20:59:38,826 - deepbridge.reports - INFO - metrics_radar_chart parameters: ['models_metrics', 'title']\n",
      "2025-11-12 20:59:38,827 - deepbridge.reports - INFO - Calling radar chart with models_metrics parameter\n",
      "2025-11-12 20:59:38,832 - deepbridge.reports - ERROR - No metrics found for radar chart\n",
      "2025-11-12 20:59:38,902 - deepbridge.reports - INFO - Generated model metrics comparison\n",
      "2025-11-12 20:59:38,903 - deepbridge.reports - INFO - ==================================================\n",
      "2025-11-12 20:59:38,904 - deepbridge.reports - INFO - Generating custom DeepBridge charts\n",
      "2025-11-12 20:59:38,905 - deepbridge.reports - INFO - ==================================================\n",
      "2025-11-12 20:59:38,905 - deepbridge.reports - INFO - Generating Interval Boxplot chart...\n",
      "2025-11-12 20:59:38,906 - deepbridge.reports - INFO - Found interval_widths in report_data\n",
      "2025-11-12 20:59:38,907 - deepbridge.reports - ERROR - Error generating Interval Boxplot chart: 'UncertaintyChartGenerator' object has no attribute 'generate_interval_boxplot'\n",
      "2025-11-12 20:59:38,909 - deepbridge.reports - ERROR - Error in custom charts generation block: cannot access local variable 'traceback' where it is not associated with a value\n",
      "2025-11-12 20:59:38,910 - deepbridge.reports - ERROR - Error generating charts: cannot access local variable 'traceback' where it is not associated with a value\n",
      "2025-11-12 20:59:38,910 - deepbridge.reports - WARNING - No charts were generated for the report\n",
      "2025-11-12 20:59:38,914 - deepbridge.reports - INFO - Output directory ensured: /home/guhaase/projetos/DeepBridge/examples/notebooks/03_validation_tests/outputs/uncertainty_classification\n",
      "2025-11-12 20:59:38,915 - deepbridge.reports - INFO - Static report saved to: /home/guhaase/projetos/DeepBridge/examples/notebooks/03_validation_tests/outputs/uncertainty_classification/uncertainty_classification_static.html\n",
      "2025-11-12 20:59:38,916 - deepbridge.reports - INFO - Uncertainty report generated at: /home/guhaase/projetos/DeepBridge/examples/notebooks/03_validation_tests/outputs/uncertainty_classification/uncertainty_classification_static.html\n",
      "2025-11-12 20:59:38,917 - deepbridge.reports - INFO - Report generated and saved to: /home/guhaase/projetos/DeepBridge/examples/notebooks/03_validation_tests/outputs/uncertainty_classification/uncertainty_classification_static.html (type: static)\n",
      "\n",
      "âœ… Static report generated!\n",
      "ğŸ“‚ Location: /home/guhaase/projetos/DeepBridge/examples/notebooks/03_validation_tests/outputs/uncertainty_classification/uncertainty_classification_static.html\n",
      "\n",
      "ğŸ’¡ Static reports can be easily printed or converted to PDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guhaase/projetos/DeepBridge/deepbridge/templates/report_types/uncertainty/static/charts/enhanced_charts.py:446: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(labels)\n"
     ]
    }
   ],
   "source": [
    "# Generate static HTML report\n",
    "static_html_path = os.path.join(output_dir, 'uncertainty_classification_static.html')\n",
    "\n",
    "print(\"ğŸ“ Generating static HTML report...\\n\")\n",
    "\n",
    "static_report_path = exp.save_html(\n",
    "    test_type='uncertainty',\n",
    "    file_path=static_html_path,\n",
    "    model_name='RandomForest Classifier',\n",
    "    report_type='static'  # Uses matplotlib instead of Plotly\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Static report generated!\")\n",
    "print(f\"ğŸ“‚ Location: {static_report_path}\")\n",
    "print(f\"\\nğŸ’¡ Static reports can be easily printed or converted to PDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ Summary - Files Generated\n",
    "\n",
    "### ğŸ“‚ Output Directory Structure:\n",
    "```\n",
    "outputs/uncertainty_classification/\n",
    "â”œâ”€â”€ uncertainty_classification_interactive.html  # Interactive report with Plotly\n",
    "â”œâ”€â”€ uncertainty_classification_static.html       # Static report with Matplotlib\n",
    "â””â”€â”€ uncertainty_classification_results.json      # Complete results in JSON\n",
    "```\n",
    "\n",
    "### âœ… What You Learned:\n",
    "\n",
    "1. **Uncertainty in Classification**\n",
    "   - Quantify confidence in binary predictions\n",
    "   - Generate probability intervals\n",
    "   - Assess calibration quality\n",
    "\n",
    "2. **Report Generation**\n",
    "   - Interactive HTML reports with Plotly\n",
    "   - Static HTML reports with Matplotlib\n",
    "   - Complete control over report type\n",
    "\n",
    "3. **JSON Export**\n",
    "   - Full experiment metadata\n",
    "   - By-alpha coverage analysis\n",
    "   - By-feature uncertainty analysis\n",
    "   - Feature importance data\n",
    "   - Easy integration with other tools\n",
    "\n",
    "4. **Practical Applications**\n",
    "   - Medical diagnosis decision protocols\n",
    "   - Credit approval with risk quantification\n",
    "   - Fraud detection with security levels\n",
    "   - Uncertainty-based decision rules\n",
    "\n",
    "### ğŸ’¡ Best Practices:\n",
    "\n",
    "- âœ… Always generate reports for stakeholder communication\n",
    "- âœ… Export JSON for automated analysis and monitoring\n",
    "- âœ… Use interactive reports for exploration\n",
    "- âœ… Use static reports for documentation and archiving\n",
    "- âœ… Define clear decision rules based on uncertainty\n",
    "- âœ… Monitor calibration in production\n",
    "\n",
    "### ğŸš€ Next Steps:\n",
    "\n",
    "- ğŸ“˜ `04_resilience_drift.ipynb` - Detect data distribution changes\n",
    "- ğŸ“˜ `02_complete_robustness.ipynb` - Model robustness testing\n",
    "- ğŸ“˜ `../04_fairness/` - Fairness and bias analysis\n",
    "\n",
    "<div style=\"background-color: #e8f5e9; padding: 15px; border-radius: 5px; border-left: 5px solid #4caf50;\">\n",
    "<b>ğŸ¯ Key Takeaway:</b> In critical applications (medical, financial, safety), uncertainty quantification is not optional - it's essential for responsible AI deployment.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
