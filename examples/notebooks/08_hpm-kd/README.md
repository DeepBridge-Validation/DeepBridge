# HPM-KD: Hierarchical Progressive Multi-Teacher Knowledge Distillation

Este diret√≥rio cont√©m notebooks de demonstra√ß√£o do framework HPM-KD implementado na biblioteca DeepBridge.

## üìö Conte√∫do

### `hpmkd_demo.ipynb`

Demonstra√ß√£o completa do uso do HPM-KD, incluindo:

1. **Configura√ß√£o e Importa√ß√µes**: Setup do ambiente e bibliotecas necess√°rias
2. **Prepara√ß√£o de Dados**: Carregamento e divis√£o do dataset Digits
3. **Treinamento do Professor**: Cria√ß√£o de um modelo grande e complexo (Random Forest)
4. **Baseline**: Treinamento direto de um modelo pequeno para compara√ß√£o
5. **HPM-KD em A√ß√£o**: Destila√ß√£o com configura√ß√£o autom√°tica
6. **Avalia√ß√£o**: Compara√ß√£o de m√©tricas entre Professor, Baseline e HPM-KD
7. **Visualiza√ß√µes**: Gr√°ficos comparativos de acur√°cia e compress√£o
8. **An√°lise de Componentes**: Verifica√ß√£o dos componentes ativos do HPM-KD

## üéØ Objetivo

Demonstrar o uso simplificado do HPM-KD conforme apresentado no **Listing 1** do paper:

```python
from deepbridge.distillation.techniques.hpm import HPMDistiller, HPMConfig

# Configura√ß√£o autom√°tica via meta-learning
hpmkd = HPMDistiller(
    teacher_model=teacher,
    student_model_type=ModelType.DECISION_TREE,
    config=HPMConfig(use_progressive=True, use_multi_teacher=True)
)

# Destila√ß√£o progressiva multi-professor
hpmkd.fit(X_train, y_train, X_val, y_val)

# Avaliar estudante comprimido
student_acc = accuracy_score(y_test, hpmkd.predict(X_test))
```

## üîß Componentes do HPM-KD

O framework integra 6 componentes sin√©rgicos:

1. **Adaptive Configuration Manager**: Meta-aprendizado para sele√ß√£o autom√°tica de hiperpar√¢metros
2. **Progressive Distillation Chain**: Cadeia hier√°rquica de modelos intermedi√°rios
3. **Multi-Teacher Ensemble**: Ensemble com aten√ß√£o aprendida para pondera√ß√£o din√¢mica
4. **Meta Temperature Scheduler**: Ajuste adaptativo da temperatura durante treinamento
5. **Parallel Processing Pipeline**: Distribui√ß√£o eficiente de tarefas de destila√ß√£o
6. **Shared Optimization Memory**: Caching cross-experimento para reutiliza√ß√£o

## üìä Resultados Esperados

- **Compress√£o**: 10√ó-15√ó redu√ß√£o no tamanho do modelo
- **Reten√ß√£o**: 85%+ da acur√°cia do professor
- **Efici√™ncia**: Configura√ß√£o autom√°tica sem ajuste manual
- **Superioridade**: Ganhos sobre baseline de treinamento direto

## üöÄ Como Executar

### Pr√©-requisitos

```bash
pip install deepbridge scikit-learn matplotlib seaborn pandas numpy
```

### Executando o Notebook

```bash
jupyter notebook hpmkd_demo.ipynb
```

Ou use o Jupyter Lab:

```bash
jupyter lab hpmkd_demo.ipynb
```

## üìñ Refer√™ncias

- **Paper**: HPM-KD: Hierarchical Progressive Multi-Teacher Knowledge Distillation Framework
- **Documenta√ß√£o DeepBridge**: https://deepbridge.readthedocs.io
- **Reposit√≥rio**: https://github.com/DeepBridge-Validation/DeepBridge

## üí° Dicas

- O notebook usa o dataset **Digits** do scikit-learn para facilitar execu√ß√£o r√°pida
- Para experimentos maiores (CIFAR-10, CIFAR-100), ajuste os par√¢metros de configura√ß√£o
- O tempo de execu√ß√£o depende da configura√ß√£o `n_trials` no HPMConfig
- Para processamento paralelo, ajuste `use_parallel=True` e `parallel_workers`

## üîç Troubleshooting

### Erro de Importa√ß√£o

Se encontrar erro ao importar HPMDistiller:

```python
# Verifique se o DeepBridge est√° instalado
pip install -e /path/to/DeepBridge

# Ou adicione ao PYTHONPATH
import sys
sys.path.append('/path/to/DeepBridge')
```

### Problemas de Mem√≥ria

Para datasets grandes, ajuste:

```python
config = HPMConfig(
    cache_memory_gb=1.0,  # Reduzir cache
    use_parallel=False,   # Desabilitar paraleliza√ß√£o
    n_trials=3            # Menos trials
)
```

## üìù Notas

- O c√≥digo foi validado contra a implementa√ß√£o do paper
- A API segue o padr√£o scikit-learn para facilitar integra√ß√£o
- Todos os componentes podem ser habilitados/desabilitados individualmente

---

**Autores**: Gustavo Coelho Haase, Paulo Henrique Dourado da Silva
**Institui√ß√µes**: Universidade Cat√≥lica de Bras√≠lia, Universidade de S√£o Paulo
**Licen√ßa**: MIT
