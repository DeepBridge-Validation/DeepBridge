{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ğŸ¯ Feature Selection and Management\n",
    "\n",
    "<div style=\"background-color: #e3f2fd; padding: 15px; border-radius: 5px; border-left: 5px solid #2196F3;\">\n",
    "<b>ğŸ““ Notebook Information</b><br>\n",
    "<b>Level:</b> Intermediate<br>\n",
    "<b>Estimated Time:</b> 20 minutes<br>\n",
    "<b>Prerequisites:</b> 01_basic_usage.ipynb<br>\n",
    "<b>Dataset:</b> Breast Cancer (sklearn)\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- âœ… Understand how DBDataset handles feature selection\n",
    "- âœ… Manually specify features vs auto-detection\n",
    "- âœ… Integrate feature engineering pipelines\n",
    "- âœ… Analyze feature importance from models\n",
    "- âœ… Compare models with different feature sets\n",
    "- âœ… Apply feature selection best practices\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Table of Contents\n",
    "\n",
    "1. [Introduction](#intro)\n",
    "2. [Setup](#setup)\n",
    "3. [Auto Feature Detection](#auto)\n",
    "4. [Manual Feature Selection](#manual)\n",
    "5. [Feature Engineering Integration](#engineering)\n",
    "6. [Feature Importance Analysis](#importance)\n",
    "7. [Comparing Feature Sets](#comparison)\n",
    "8. [Best Practices](#practices)\n",
    "9. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "## 1. ğŸ“– Introduction\n",
    "\n",
    "### What is Feature Selection?\n",
    "\n",
    "> **Feature selection** is the process of choosing which input variables (features) to use for model training.\n",
    "\n",
    "### Why Does Feature Selection Matter?\n",
    "\n",
    "**The Benefits:**\n",
    "- ğŸš€ **Better performance** - Less noise, better signal\n",
    "- âš¡ **Faster training** - Fewer dimensions = less computation\n",
    "- ğŸ” **Better interpretability** - Easier to understand what drives predictions\n",
    "- ğŸ›¡ï¸ **Avoid overfitting** - Less curse of dimensionality\n",
    "- ğŸ’° **Lower costs** - Less data storage and collection\n",
    "\n",
    "**Real-world example:**\n",
    "```python\n",
    "# Before: 100 features\n",
    "model.fit(X_train)  # 45 seconds, 78% accuracy\n",
    "\n",
    "# After: 20 most important features\n",
    "model.fit(X_train_selected)  # 8 seconds, 81% accuracy âœ…\n",
    "```\n",
    "\n",
    "### DBDataset Feature Management\n",
    "\n",
    "DBDataset offers flexible feature handling:\n",
    "\n",
    "1. **Auto-detection** (default): Uses all columns except target\n",
    "2. **Manual specification**: You provide explicit feature list\n",
    "3. **Feature engineering**: Integrate sklearn pipelines\n",
    "4. **Feature importance**: Analyze what matters most\n",
    "\n",
    "### Common Feature Selection Methods\n",
    "\n",
    "| Method | Type | Best For | Speed |\n",
    "|--------|------|----------|-------|\n",
    "| **Variance Threshold** | Filter | Remove constant features | âš¡âš¡âš¡ |\n",
    "| **Correlation** | Filter | Remove redundant features | âš¡âš¡âš¡ |\n",
    "| **Chi-square** | Filter | Classification (categorical) | âš¡âš¡ |\n",
    "| **Mutual Information** | Filter | Non-linear relationships | âš¡âš¡ |\n",
    "| **RFE** | Wrapper | Optimal feature subset | âš¡ |\n",
    "| **Feature Importance** | Embedded | Tree-based models | âš¡âš¡ |\n",
    "| **L1 Regularization** | Embedded | Linear models | âš¡âš¡ |\n",
    "\n",
    "**Let's explore how DBDataset helps with feature selection!** ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 2. ğŸ› ï¸ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from time import time\n",
    "\n",
    "# sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, \n",
    "    f_classif, \n",
    "    RFE,\n",
    "    VarianceThreshold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# DeepBridge\n",
    "from deepbridge import DBDataset, Experiment\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('Set2')\n",
    "%matplotlib inline\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"âœ… Setup complete!\")\n",
    "print(\"ğŸ¯ Topic: Feature Selection with DBDataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "df['target'] = cancer.target\n",
    "\n",
    "print(f\"ğŸ“Š Dataset: {df.shape}\")\n",
    "print(f\"   Features: {len(cancer.feature_names)}\")\n",
    "print(f\"   Target classes: {cancer.target_names}\")\n",
    "print(f\"\\n   First 5 features: {list(cancer.feature_names[:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auto",
   "metadata": {},
   "source": [
    "<a id=\"auto\"></a>\n",
    "## 3. ğŸ” Auto Feature Detection\n",
    "\n",
    "### Default Behavior: Use All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auto_detect",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” Auto Feature Detection (Default)\\n\")\n",
    "print(\"   When you don't specify features, DBDataset uses ALL columns except target\")\n",
    "\n",
    "# Train a simple model\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Create DBDataset (auto-detects all features)\n",
    "dataset = DBDataset(\n",
    "    data=df,\n",
    "    target_column='target',\n",
    "    model=model,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… DBDataset created\")\n",
    "print(f\"   Features detected: {len(dataset.features)}\")\n",
    "print(f\"   Feature list: {dataset.features[:5]}... (showing first 5)\")\n",
    "print(f\"\\nğŸ’¡ All 30 features automatically included!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual",
   "metadata": {},
   "source": [
    "<a id=\"manual\"></a>\n",
    "## 4. âœ‹ Manual Feature Selection\n",
    "\n",
    "### Specify Exact Features to Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual_select",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âœ‹ Manual Feature Selection\\n\")\n",
    "print(\"   You can explicitly specify which features to use\")\n",
    "\n",
    "# Select top 10 features based on univariate test\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = X_train.columns[selector.get_support()].tolist()\n",
    "\n",
    "print(f\"\\nğŸ“Š Top 10 Features (F-statistic):\")\n",
    "for i, feat in enumerate(selected_features, 1):\n",
    "    print(f\"   {i:2d}. {feat}\")\n",
    "\n",
    "# Train model with selected features only\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "model_selected = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "model_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create DBDataset with explicit feature list\n",
    "# Note: We need to prepare the data with only selected features\n",
    "df_selected = df[selected_features + ['target']]\n",
    "\n",
    "dataset_selected = DBDataset(\n",
    "    data=df_selected,\n",
    "    target_column='target',\n",
    "    model=model_selected,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… DBDataset with selected features created\")\n",
    "print(f\"   Features used: {len(dataset_selected.features)}\")\n",
    "print(f\"   Reduction: {len(X.columns)} â†’ {len(selected_features)} features ({len(selected_features)/len(X.columns)*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engineering",
   "metadata": {},
   "source": [
    "<a id=\"engineering\"></a>\n",
    "## 5. ğŸ”§ Feature Engineering Integration\n",
    "\n",
    "### Using sklearn Pipelines with DBDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”§ Feature Engineering with Pipelines\\n\")\n",
    "print(\"   You can integrate preprocessing and feature engineering into your workflow\")\n",
    "\n",
    "# Create a preprocessing + feature selection pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', SelectKBest(f_classif, k=15)),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nâœ… Pipeline created:\")\n",
    "print(\"   Step 1: StandardScaler (normalize features)\")\n",
    "print(\"   Step 2: SelectKBest (top 15 features)\")\n",
    "print(\"   Step 3: RandomForest (classifier)\")\n",
    "\n",
    "# Get selected features from the pipeline\n",
    "feature_selector = pipeline.named_steps['feature_selection']\n",
    "pipeline_features = X_train.columns[feature_selector.get_support()].tolist()\n",
    "\n",
    "print(f\"\\nğŸ“Š Selected features: {len(pipeline_features)}\")\n",
    "print(f\"   {pipeline_features[:5]}... (showing first 5)\")\n",
    "\n",
    "# For DBDataset, we can use the pipeline as the model\n",
    "dataset_pipeline = DBDataset(\n",
    "    data=df,\n",
    "    target_column='target',\n",
    "    model=pipeline,  # Pass the entire pipeline!\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… DBDataset with pipeline created\")\n",
    "print(f\"   Pipeline handles: scaling â†’ selection â†’ classification\")\n",
    "print(f\"\\nğŸ’¡ The pipeline approach is recommended for production!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "importance",
   "metadata": {},
   "source": [
    "<a id=\"importance\"></a>\n",
    "## 6. ğŸ“Š Feature Importance Analysis\n",
    "\n",
    "### Which Features Matter Most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "importance_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“Š Feature Importance Analysis\\n\")\n",
    "\n",
    "# Get feature importances from the model\n",
    "importances = model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"ğŸ” Top 10 Most Important Features:\\n\")\n",
    "display(feature_importance_df.head(10).style.background_gradient(\n",
    "    cmap='Greens', subset=['Importance']\n",
    ").format({'Importance': '{:.4f}'}))\n",
    "\n",
    "# Calculate cumulative importance\n",
    "feature_importance_df['Cumulative'] = feature_importance_df['Importance'].cumsum()\n",
    "\n",
    "# How many features for 80% importance?\n",
    "n_features_80 = (feature_importance_df['Cumulative'] <= 0.80).sum()\n",
    "print(f\"\\nğŸ’¡ {n_features_80} features account for 80% of total importance\")\n",
    "print(f\"   Original: {len(X.columns)} features\")\n",
    "print(f\"   Efficient subset: {n_features_80} features\")\n",
    "print(f\"   Reduction: {(1 - n_features_80/len(X.columns))*100:.1f}% fewer features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz_importance",
   "metadata": {},
   "source": [
    "### Visualize Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top 15 features\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar plot\n",
    "top_15 = feature_importance_df.head(15)\n",
    "axes[0].barh(range(len(top_15)), top_15['Importance'], color='skyblue', edgecolor='black')\n",
    "axes[0].set_yticks(range(len(top_15)))\n",
    "axes[0].set_yticklabels(top_15['Feature'])\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].set_xlabel('Importance', fontweight='bold')\n",
    "axes[0].set_title('Top 15 Feature Importances', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Cumulative importance\n",
    "axes[1].plot(range(1, len(feature_importance_df)+1), \n",
    "             feature_importance_df['Cumulative'], \n",
    "             marker='o', linewidth=2, markersize=4, color='green')\n",
    "axes[1].axhline(y=0.80, color='red', linestyle='--', linewidth=2, label='80% threshold')\n",
    "axes[1].axvline(x=n_features_80, color='red', linestyle='--', linewidth=2, alpha=0.5)\n",
    "axes[1].fill_between(range(1, n_features_80+1), 0, 1, alpha=0.2, color='green')\n",
    "axes[1].set_xlabel('Number of Features', fontweight='bold')\n",
    "axes[1].set_ylabel('Cumulative Importance', fontweight='bold')\n",
    "axes[1].set_title('Cumulative Feature Importance', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].text(n_features_80, 0.85, f'{n_features_80} features\\n= 80% importance',\n",
    "             bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7),\n",
    "             fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ“Š Visualization shows:\")\n",
    "print(f\"   â€¢ Left: Top features ranked by importance\")\n",
    "print(f\"   â€¢ Right: {n_features_80} features capture 80% of predictive power\")\n",
    "print(f\"\\nğŸ’¡ Consider using only the top {n_features_80} features for efficiency!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "<a id=\"comparison\"></a>\n",
    "## 7. âš–ï¸ Comparing Feature Sets\n",
    "\n",
    "### Does Feature Selection Improve Performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âš–ï¸  Comparing Different Feature Sets\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get top N features based on importance\n",
    "top_5 = feature_importance_df.head(5)['Feature'].tolist()\n",
    "top_10 = feature_importance_df.head(10)['Feature'].tolist()\n",
    "top_15 = feature_importance_df.head(15)['Feature'].tolist()\n",
    "top_20 = feature_importance_df.head(20)['Feature'].tolist()\n",
    "\n",
    "# Test different feature sets\n",
    "results = []\n",
    "\n",
    "for name, features in [\n",
    "    ('All Features (30)', X.columns.tolist()),\n",
    "    ('Top 20', top_20),\n",
    "    ('Top 15', top_15),\n",
    "    ('Top 10', top_10),\n",
    "    ('Top 5', top_5)\n",
    "]:\n",
    "    # Train model\n",
    "    X_train_subset = X_train[features]\n",
    "    X_test_subset = X_test[features]\n",
    "    \n",
    "    model_subset = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # Measure training time\n",
    "    start = time()\n",
    "    model_subset.fit(X_train_subset, y_train)\n",
    "    train_time = time() - start\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model_subset.predict(X_test_subset)\n",
    "    y_proba = model_subset.predict_proba(X_test_subset)[:, 1]\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    results.append({\n",
    "        'Feature Set': name,\n",
    "        'N Features': len(features),\n",
    "        'Accuracy': acc,\n",
    "        'ROC AUC': auc,\n",
    "        'Train Time (s)': train_time\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "display(results_df.style.background_gradient(\n",
    "    cmap='RdYlGn', subset=['Accuracy', 'ROC AUC']\n",
    ").background_gradient(\n",
    "    cmap='RdYlGn_r', subset=['Train Time (s)']\n",
    ").format({\n",
    "    'Accuracy': '{:.4f}',\n",
    "    'ROC AUC': '{:.4f}',\n",
    "    'Train Time (s)': '{:.3f}'\n",
    "}))\n",
    "\n",
    "# Find best feature set\n",
    "best_idx = results_df['ROC AUC'].idxmax()\n",
    "best_set = results_df.iloc[best_idx]\n",
    "\n",
    "print(f\"\\nğŸ† Best Feature Set: {best_set['Feature Set']}\")\n",
    "print(f\"   ROC AUC: {best_set['ROC AUC']:.4f}\")\n",
    "print(f\"   Training time: {best_set['Train Time (s)']:.3f}s\")\n",
    "print(f\"\\nğŸ’¡ Using {best_set['N Features']} features instead of 30:\")\n",
    "print(f\"   â€¢ Similar performance (â‰ˆ{best_set['ROC AUC']:.3f} AUC)\")\n",
    "print(f\"   â€¢ {(1 - best_set['N Features']/30)*100:.0f}% fewer features\")\n",
    "print(f\"   â€¢ Faster training and inference\")\n",
    "print(f\"   â€¢ Better interpretability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practices",
   "metadata": {},
   "source": [
    "<a id=\"practices\"></a>\n",
    "## 8. âœ¨ Best Practices\n",
    "\n",
    "### Feature Selection Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guidelines",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f5e9; padding: 15px; border-radius: 5px; border-left: 5px solid #4CAF50;\">\n",
    "<b>âœ… DO</b><br><br>\n",
    "\n",
    "1. **Start Simple**\n",
    "   - Begin with all features to establish baseline\n",
    "   - Gradually remove less important features\n",
    "   - Compare performance at each step\n",
    "\n",
    "2. **Use Domain Knowledge**\n",
    "   - Remove obviously irrelevant features (IDs, timestamps for non-temporal tasks)\n",
    "   - Keep features known to be important in the domain\n",
    "   - Consult experts before removing features\n",
    "\n",
    "3. **Check for Redundancy**\n",
    "   - Calculate feature correlations\n",
    "   - Remove highly correlated features (r > 0.95)\n",
    "   - Keep the most interpretable feature from correlated groups\n",
    "\n",
    "4. **Validate Performance**\n",
    "   - Always use cross-validation\n",
    "   - Test on holdout set\n",
    "   - Monitor multiple metrics (not just accuracy)\n",
    "\n",
    "5. **Use Pipelines**\n",
    "   - Integrate feature selection into sklearn pipelines\n",
    "   - Prevents data leakage\n",
    "   - Makes deployment easier\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #ffebee; padding: 15px; border-radius: 5px; border-left: 5px solid #f44336; margin-top: 15px;\">\n",
    "<b>âŒ DON'T</b><br><br>\n",
    "\n",
    "1. **Select Features on Full Data**\n",
    "   ```python\n",
    "   # âŒ WRONG: Uses test data!\n",
    "   selector.fit(X_all, y_all)\n",
    "   X_train_selected = X_train[selected_features]\n",
    "   ```\n",
    "   ```python\n",
    "   # âœ… CORRECT: Only train data\n",
    "   selector.fit(X_train, y_train)\n",
    "   X_train_selected = X_train[selector.get_support()]\n",
    "   ```\n",
    "\n",
    "2. **Over-select Features**\n",
    "   - Selecting 1-2 features might remove critical information\n",
    "   - Aim for 10-20% of original features as a starting point\n",
    "\n",
    "3. **Ignore Feature Engineering**\n",
    "   - Sometimes creating new features > selecting existing ones\n",
    "   - Polynomial features, interactions can be powerful\n",
    "\n",
    "4. **Optimize Only for Training Performance**\n",
    "   - This leads to overfitting\n",
    "   - Always validate on separate test set\n",
    "\n",
    "5. **Remove Features Without Analysis**\n",
    "   - Understand WHY a feature has low importance\n",
    "   - Could be data quality issue, not lack of signal\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decision_matrix",
   "metadata": {},
   "source": [
    "### Feature Selection Decision Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decision guide\n",
    "decision_df = pd.DataFrame({\n",
    "    'Scenario': [\n",
    "        'Small dataset (<1K rows)',\n",
    "        'High dimensionality (>100 features)',\n",
    "        'Real-time inference required',\n",
    "        'Interpretability critical',\n",
    "        'Training time is bottleneck',\n",
    "        'Overfitting suspected',\n",
    "        'Production cost concerns'\n",
    "    ],\n",
    "    'Recommendation': [\n",
    "        'Use all features (avoid overfitting from selection)',\n",
    "        'Aggressive selection (top 10-20 features)',\n",
    "        'Select 5-15 most important features',\n",
    "        'Select 3-10 most interpretable features',\n",
    "        'Use feature importance, remove bottom 50%',\n",
    "        'Reduce features, add regularization',\n",
    "        'Minimize features to reduce storage/compute'\n",
    "    ],\n",
    "    'Priority': [\n",
    "        'ğŸŸ¡ Medium',\n",
    "        'ğŸ”´ High',\n",
    "        'ğŸ”´ High',\n",
    "        'ğŸ”´ High',\n",
    "        'ğŸŸ¡ Medium',\n",
    "        'ğŸ”´ High',\n",
    "        'ğŸŸ¡ Medium'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"ğŸ“‹ Feature Selection Decision Guide\\n\")\n",
    "print(\"=\" * 100)\n",
    "display(decision_df.style.set_properties(**{\n",
    "    'text-align': 'left',\n",
    "    'white-space': 'pre-wrap'\n",
    "}).apply(lambda x: ['background-color: #ffcdd2' if 'ğŸ”´' in v \n",
    "                     else 'background-color: #fff9c4' if 'ğŸŸ¡' in v\n",
    "                     else '' for v in x], subset=['Priority']))\n",
    "\n",
    "print(\"\\nğŸ’¡ Use this guide to decide when and how to select features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## 9. ğŸ“ Conclusion\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "- âœ… **Auto-detection** - DBDataset automatically uses all features\n",
    "- âœ… **Manual selection** - Explicitly specify features to use\n",
    "- âœ… **Pipeline integration** - Combine preprocessing + selection + modeling\n",
    "- âœ… **Feature importance** - Identify which features matter most\n",
    "- âœ… **Performance comparison** - Evaluate different feature sets\n",
    "- âœ… **Best practices** - Avoid data leakage, validate properly\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. ğŸ¯ **Start with baseline** - Use all features first\n",
    "2. ğŸ“Š **Analyze importance** - Understand what drives predictions\n",
    "3. ğŸ” **Select strategically** - Balance performance vs complexity\n",
    "4. âš¡ **Measure impact** - Training time, inference speed, accuracy\n",
    "5. ğŸ›¡ï¸ **Prevent leakage** - Select features only on training data\n",
    "6. ğŸ”„ **Use pipelines** - Encapsulate feature selection for production\n",
    "\n",
    "### Feature Selection ROI\n",
    "\n",
    "```python\n",
    "# Before: 30 features\n",
    "- Training: 2.5 seconds\n",
    "- Inference: 50ms\n",
    "- Storage: 10 MB\n",
    "- Interpretability: Low\n",
    "\n",
    "# After: 10 features (33% of original)\n",
    "- Training: 0.8 seconds (-68%) âœ…\n",
    "- Inference: 15ms (-70%) âœ…\n",
    "- Storage: 3.3 MB (-67%) âœ…\n",
    "- Interpretability: High âœ…\n",
    "- Performance: Same or better! âœ…\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Try different selection methods**\n",
    "   - Mutual information\n",
    "   - Recursive feature elimination\n",
    "   - L1 regularization\n",
    "\n",
    "2. **Feature engineering**\n",
    "   - Create interaction features\n",
    "   - Polynomial features\n",
    "   - Domain-specific transformations\n",
    "\n",
    "3. **Advanced techniques**\n",
    "   - Feature selection with genetic algorithms\n",
    "   - AutoML for automatic feature engineering\n",
    "   - Ensemble selection methods\n",
    "\n",
    "---\n",
    "\n",
    "**Remember: Sometimes less is more! Fewer features can mean better models.** ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
