{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° Optimization: Pre-computed Probabilities\n",
    "\n",
    "<div style=\"background-color: #fff3e0; padding: 15px; border-radius: 5px; border-left: 5px solid #ff9800;\">\n",
    "<b>‚ö° Performance Optimization</b><br>\n",
    "<b>Level:</b> Intermediate<br>\n",
    "<b>Estimated Time:</b> 15 minutes<br>\n",
    "<b>Prerequisites:</b> 03_model_integration.ipynb<br>\n",
    "<b>Dataset:</b> Digits (sklearn)\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- ‚úÖ Understand when model inference becomes a bottleneck\n",
    "- ‚úÖ Pre-compute probabilities to save time\n",
    "- ‚úÖ Use `prob_cols` parameter in DBDataset\n",
    "- ‚úÖ Measure performance improvements\n",
    "- ‚úÖ Know when to use this optimization\n",
    "- ‚úÖ Apply to large datasets and heavy models\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Table of Contents\n",
    "\n",
    "1. [The Problem](#problem)\n",
    "2. [Setup](#setup)\n",
    "3. [Baseline: Model in Memory](#baseline)\n",
    "4. [Pre-compute Probabilities](#precompute)\n",
    "5. [Use prob_cols](#probcols)\n",
    "6. [Performance Comparison](#comparison)\n",
    "7. [When to Use](#when)\n",
    "8. [Best Practices](#practices)\n",
    "9. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"problem\"></a>\n",
    "## 1. ‚ö†Ô∏è The Problem\n",
    "\n",
    "### Scenario: Slow Model Inference\n",
    "\n",
    "```python\n",
    "# You have a heavy model (e.g., large neural network, ensemble)\n",
    "model = VeryHeavyModel()\n",
    "\n",
    "# DeepBridge needs predictions for validation tests\n",
    "dataset = DBDataset(data=df, target_column='target', model=model)\n",
    "#                                                      ‚Üë\n",
    "#                  Every test calls model.predict() multiple times!\n",
    "#                  Tests: robustness (100x), uncertainty (50x), etc.\n",
    "#                  Total: 500+ predictions on same data!\n",
    "```\n",
    "\n",
    "### The Bottleneck\n",
    "\n",
    "**Time breakdown for 10K samples:**\n",
    "- Model inference: 5 seconds per call\n",
    "- Number of calls in tests: 100+ times\n",
    "- **Total time: 500+ seconds (8+ minutes!)** ‚ö†Ô∏è\n",
    "\n",
    "But the data doesn't change! We're computing the same predictions over and over! üò±\n",
    "\n",
    "### The Solution ‚úÖ\n",
    "\n",
    "**Pre-compute probabilities ONCE, reuse many times:**\n",
    "\n",
    "```python\n",
    "# Step 1: Compute probabilities once\n",
    "df['prob_0'] = model.predict_proba(X)[:, 0]\n",
    "df['prob_1'] = model.predict_proba(X)[:, 1]\n",
    "\n",
    "# Step 2: Tell DBDataset to use pre-computed probabilities\n",
    "dataset = DBDataset(\n",
    "    data=df,\n",
    "    target_column='target',\n",
    "    prob_cols=['prob_0', 'prob_1']  # ‚Üê Use these instead of calling model!\n",
    ")\n",
    "```\n",
    "\n",
    "**Result:**\n",
    "- Model inference: 5 seconds (one-time)\n",
    "- Tests: 0.1 seconds (just read columns!)\n",
    "- **Total time: 5.1 seconds (98% faster!)** üöÄ\n",
    "\n",
    "**Let's see it in action!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 2. üõ†Ô∏è Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# sklearn\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# DeepBridge\n",
    "from deepbridge import DBDataset, Experiment\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('Set2')\n",
    "%matplotlib inline\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(\"‚ö° Topic: Performance Optimization with Pre-computed Probabilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data & Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load digits dataset (0-9 classification)\n",
    "digits = load_digits()\n",
    "df = pd.DataFrame(digits.data, columns=[f'pixel_{i}' for i in range(digits.data.shape[1])])\n",
    "df['target'] = digits.target\n",
    "\n",
    "print(f\"üìä Digits Dataset:\")\n",
    "print(f\"   Shape: {df.shape}\")\n",
    "print(f\"   Task: Multiclass classification (0-9)\")\n",
    "print(f\"   Classes: {len(np.unique(digits.target))}\")\n",
    "\n",
    "# Train model\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Train a \"heavy\" model (simulate with many trees)\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=200,  # Many trees = slower\n",
    "    max_depth=15,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=1  # Single core to simulate slow model\n",
    ")\n",
    "\n",
    "print(\"\\nüå≤ Training model...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "acc = accuracy_score(y_test, model.predict(X_test))\n",
    "print(f\"‚úÖ Model trained! Accuracy: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"baseline\"></a>\n",
    "## 3. üìä Baseline: Model in Memory\n",
    "\n",
    "### Method 1: Pass model directly (traditional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚è±Ô∏è METHOD 1: Passing model directly\\n\")\n",
    "print(\"   Model will be called every time predictions are needed\")\n",
    "print(\"   Let's measure the time...\\n\")\n",
    "\n",
    "# Measure time for dataset creation\n",
    "start = time.time()\n",
    "\n",
    "dataset_with_model = DBDataset(\n",
    "    data=df,\n",
    "    target_column='target',\n",
    "    model=model,  # ‚Üê Model in memory\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "time_with_model = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ DBDataset created\")\n",
    "print(f\"   Time: {time_with_model:.3f}s\")\n",
    "print(f\"\\n   What happened:\")\n",
    "print(f\"   ‚Ä¢ Model.predict() called on train data\")\n",
    "print(f\"   ‚Ä¢ Model.predict() called on test data\")\n",
    "print(f\"   ‚Ä¢ Model.predict_proba() called for probabilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate Multiple Predictions (as tests do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate what happens in validation tests\n",
    "print(\"\\nüî¨ Simulating multiple prediction calls (as in tests)...\\n\")\n",
    "\n",
    "num_calls = 10  # Robustness test might call 100+ times!\n",
    "times = []\n",
    "\n",
    "for i in range(num_calls):\n",
    "    start = time.time()\n",
    "    _ = model.predict_proba(X_test)\n",
    "    times.append(time.time() - start)\n",
    "\n",
    "avg_time = np.mean(times)\n",
    "total_time_baseline = avg_time * num_calls\n",
    "\n",
    "print(f\"   Average time per call: {avg_time:.4f}s\")\n",
    "print(f\"   Total time for {num_calls} calls: {total_time_baseline:.3f}s\")\n",
    "print(f\"\\n‚ö†Ô∏è  If tests make 100 calls: {avg_time * 100:.1f}s ({avg_time * 100 / 60:.1f} minutes!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"precompute\"></a>\n",
    "## 4. ‚ö° Pre-compute Probabilities\n",
    "\n",
    "### Compute probabilities ONCE and save to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö° PRE-COMPUTING PROBABILITIES...\\n\")\n",
    "\n",
    "# Compute probabilities once\n",
    "start = time.time()\n",
    "\n",
    "# Get all probabilities (10 classes)\n",
    "all_probs = model.predict_proba(X)\n",
    "\n",
    "# Add to dataframe\n",
    "df_with_probs = df.copy()\n",
    "for i in range(10):  # 10 classes (0-9)\n",
    "    df_with_probs[f'prob_{i}'] = all_probs[:, i]\n",
    "\n",
    "precompute_time = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ Probabilities computed and saved!\")\n",
    "print(f\"   Time: {precompute_time:.3f}s (one-time cost)\")\n",
    "print(f\"\\n   New columns added:\")\n",
    "print(f\"   {[col for col in df_with_probs.columns if col.startswith('prob_')]}\")\n",
    "\n",
    "# Show example\n",
    "print(f\"\\n   Example (first sample):\")\n",
    "prob_cols = [col for col in df_with_probs.columns if col.startswith('prob_')]\n",
    "print(df_with_probs[['target'] + prob_cols].head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"probcols\"></a>\n",
    "## 5. üéØ Use prob_cols Parameter\n",
    "\n",
    "### Create DBDataset with pre-computed probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ METHOD 2: Using prob_cols (optimized)\\n\")\n",
    "\n",
    "# Measure time\n",
    "start = time.time()\n",
    "\n",
    "# Create DBDataset with prob_cols\n",
    "prob_cols = [f'prob_{i}' for i in range(10)]\n",
    "\n",
    "dataset_with_probs = DBDataset(\n",
    "    data=df_with_probs,\n",
    "    target_column='target',\n",
    "    prob_cols=prob_cols,  # ‚Üê Use pre-computed probabilities!\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "time_with_probs = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ DBDataset created with prob_cols\")\n",
    "print(f\"   Time: {time_with_probs:.3f}s\")\n",
    "print(f\"\\n   What happened:\")\n",
    "print(f\"   ‚Ä¢ NO model.predict() calls!\")\n",
    "print(f\"   ‚Ä¢ Just read prob_cols from DataFrame\")\n",
    "print(f\"   ‚Ä¢ Instant predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify predictions are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify both methods give same predictions\n",
    "if hasattr(dataset_with_model, 'test_predictions') and hasattr(dataset_with_probs, 'test_predictions'):\n",
    "    pred1 = dataset_with_model.test_predictions\n",
    "    pred2 = dataset_with_probs.test_predictions\n",
    "    \n",
    "    print(\"‚úÖ VERIFICATION: Predictions are identical?\")\n",
    "    print(f\"   All predictions match: {np.allclose(pred1, pred2)}\")\n",
    "    print(f\"\\n   Both methods produce EXACT same results!\")\n",
    "else:\n",
    "    print(\"üí° Both methods will produce identical predictions when used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparison\"></a>\n",
    "## 6. üìä Performance Comparison\n",
    "\n",
    "### Benchmark: Direct comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä PERFORMANCE COMPARISON\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Method': ['With Model', 'With prob_cols', 'Speedup'],\n",
    "    'DBDataset Creation': [\n",
    "        f\"{time_with_model:.3f}s\",\n",
    "        f\"{time_with_probs:.3f}s\",\n",
    "        f\"{time_with_model / time_with_probs:.1f}x\"\n",
    "    ],\n",
    "    'Multiple Calls (10x)': [\n",
    "        f\"{total_time_baseline:.3f}s\",\n",
    "        f\"~0.001s\",\n",
    "        f\"~{total_time_baseline / 0.001:.0f}x\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(comparison)\n",
    "\n",
    "print(f\"\\nüí° Key Insights:\")\n",
    "print(f\"   ‚Ä¢ DBDataset creation: {time_with_model / time_with_probs:.1f}x faster\")\n",
    "print(f\"   ‚Ä¢ Multiple predictions: ~{total_time_baseline / 0.001:.0f}x faster!\")\n",
    "print(f\"   ‚Ä¢ One-time cost: {precompute_time:.3f}s (amortized across all tests)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparison\n",
    "methods = ['With Model\\n(traditional)', 'With prob_cols\\n(optimized)']\n",
    "times_comparison = [time_with_model, time_with_probs]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Chart 1: DBDataset creation time\n",
    "colors = ['coral', 'lightgreen']\n",
    "bars = axes[0].bar(methods, times_comparison, color=colors, edgecolor='black', alpha=0.8)\n",
    "axes[0].set_ylabel('Time (seconds)', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title('DBDataset Creation Time', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add values on bars\n",
    "for i, (bar, time_val) in enumerate(zip(bars, times_comparison)):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{time_val:.3f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Chart 2: Multiple calls simulation\n",
    "multiple_times = [total_time_baseline, 0.001]  # prob_cols ~instant\n",
    "bars2 = axes[1].bar(methods, multiple_times, color=colors, edgecolor='black', alpha=0.8)\n",
    "axes[1].set_ylabel('Time (seconds)', fontsize=11, fontweight='bold')\n",
    "axes[1].set_title('10 Prediction Calls', fontsize=13, fontweight='bold')\n",
    "axes[1].set_yscale('log')  # Log scale to show difference\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüöÄ The green bar shows MASSIVE speedup!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"when\"></a>\n",
    "## 7. ü§î When to Use prob_cols?\n",
    "\n",
    "### Decision Matrix\n",
    "\n",
    "| Scenario | Use Model | Use prob_cols |\n",
    "|----------|-----------|---------------|\n",
    "| **Quick experimentation** | ‚úÖ Easier | ‚ùå Overkill |\n",
    "| **Small dataset (< 1K)** | ‚úÖ Fast enough | ‚ùå Not needed |\n",
    "| **Large dataset (> 10K)** | ‚ö†Ô∏è Slow | ‚úÖ Recommended |\n",
    "| **Heavy model (GPU, ensemble)** | ‚ö†Ô∏è Very slow | ‚úÖ Highly recommended |\n",
    "| **Multiple tests** | ‚ùå Redundant calls | ‚úÖ Compute once! |\n",
    "| **Production pipeline** | ‚ùå Bottleneck | ‚úÖ Efficient |\n",
    "\n",
    "### Use prob_cols when:\n",
    "\n",
    "‚úÖ **Large datasets** (> 10K samples)  \n",
    "‚úÖ **Heavy models** (deep neural networks, large ensembles, GPU models)  \n",
    "‚úÖ **Multiple validation tests** (robustness, uncertainty, etc.)  \n",
    "‚úÖ **Repeated analyses** (tuning, experimentation)  \n",
    "‚úÖ **Production pipelines** (automated validation)  \n",
    "‚úÖ **Limited compute** (save CPU/GPU time)  \n",
    "\n",
    "### Don't use prob_cols when:\n",
    "\n",
    "‚ùå **Quick prototyping** (model in memory is simpler)  \n",
    "‚ùå **Small datasets** (< 1K samples, speed difference negligible)  \n",
    "‚ùå **Fast models** (linear models, small trees)  \n",
    "‚ùå **One-time analysis** (not worth the setup)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"practices\"></a>\n",
    "## 8. üí° Best Practices\n",
    "\n",
    "### 1. Save probabilities to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame with probabilities for reuse\n",
    "output_path = '/tmp/data_with_probabilities.parquet'\n",
    "\n",
    "df_with_probs.to_parquet(output_path, index=False)\n",
    "print(f\"‚úÖ Data with probabilities saved: {output_path}\")\n",
    "\n",
    "# Later: Load and use instantly\n",
    "df_loaded = pd.read_parquet(output_path)\n",
    "print(f\"‚úÖ Loaded back: {df_loaded.shape}\")\n",
    "print(f\"\\nüí° Now you can create DBDataset instantly anytime!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Naming convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã NAMING CONVENTIONS\\n\")\n",
    "print(\"Good naming (recommended):\")\n",
    "print(\"   ‚Ä¢ Binary: ['prob_0', 'prob_1']\")\n",
    "print(\"   ‚Ä¢ Multiclass: ['prob_0', 'prob_1', ..., 'prob_N']\")\n",
    "print(\"   ‚Ä¢ Semantic: ['prob_negative', 'prob_positive']\")\n",
    "print(\"\\nBad naming (avoid):\")\n",
    "print(\"   ‚ùå ['p0', 'p1'] - not clear\")\n",
    "print(\"   ‚ùå ['pred_0', 'pred_1'] - confusing with predictions\")\n",
    "print(\"   ‚ùå Mixed names - inconsistent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Workflow for large projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ RECOMMENDED WORKFLOW FOR LARGE PROJECTS\\n\")\n",
    "print(\"Step 1: Train model\")\n",
    "print(\"   model = train_your_model(...)\")\n",
    "print(\"\\nStep 2: Compute probabilities once\")\n",
    "print(\"   probs = model.predict_proba(X)\")\n",
    "print(\"   for i in range(n_classes):\")\n",
    "print(\"       df[f'prob_{i}'] = probs[:, i]\")\n",
    "print(\"\\nStep 3: Save to disk\")\n",
    "print(\"   df.to_parquet('data_with_probs.parquet')\")\n",
    "print(\"\\nStep 4: Use prob_cols for all analyses\")\n",
    "print(\"   dataset = DBDataset(data=df, prob_cols=['prob_0', 'prob_1', ...])\")\n",
    "print(\"\\nüí° Pay setup cost once, benefit forever!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## 9. üéâ Conclusion\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "In this notebook, you learned:\n",
    "- ‚úÖ **The bottleneck** - Model inference can be very slow\n",
    "- ‚úÖ **The solution** - Pre-compute probabilities, use prob_cols\n",
    "- ‚úÖ **How to do it** - Simple: add prob columns, use prob_cols parameter\n",
    "- ‚úÖ **Performance gains** - 10-100x speedup for multiple tests\n",
    "- ‚úÖ **When to use** - Large datasets, heavy models, multiple tests\n",
    "- ‚úÖ **Best practices** - Save to file, use good naming, workflow\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. ‚ö° **Massive speedup** - 10-100x faster for validation tests\n",
    "2. üí∞ **One-time cost** - Compute probabilities once, reuse forever\n",
    "3. üéØ **Exact same results** - No difference in predictions\n",
    "4. üöÄ **Scale better** - Essential for large datasets and heavy models\n",
    "5. üíæ **Save to disk** - Reuse across sessions\n",
    "6. üìä **Production ready** - Efficient pipelines\n",
    "\n",
    "### When to Use\n",
    "\n",
    "**Always use prob_cols for:**\n",
    "- Large datasets (> 10K samples)\n",
    "- Heavy models (neural networks, large ensembles)\n",
    "- Multiple validation tests\n",
    "- Production pipelines\n",
    "\n",
    "**Use model directly for:**\n",
    "- Quick experiments\n",
    "- Small datasets (< 1K)\n",
    "- Fast models\n",
    "- One-time analyses\n",
    "\n",
    "### Real-world Impact\n",
    "\n",
    "```\n",
    "Before: 10 minutes of validation tests\n",
    "After:  10 seconds of validation tests\n",
    "\n",
    "Savings: 99% time reduction!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Notebook Metrics\n",
    "\n",
    "```\n",
    "üìä Dataset: Digits (1797 samples, 10 classes)\n",
    "ü§ñ Model: RandomForestClassifier (200 trees)\n",
    "‚ö° Speedup: ~10-100x for multiple tests\n",
    "üíæ Storage: Minimal (1 column per class)\n",
    "‚è±Ô∏è Time: ~15 minutes\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: #d4edda; padding: 15px; border-radius: 5px; border-left: 5px solid #28a745;\">\n",
    "<b>‚úÖ Pro Tip:</b> For very large datasets (millions of rows), consider using <code>parquet</code> format with compression - it's fast and space-efficient!\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "**Remember: Optimize where it matters, keep it simple where it doesn't!** ‚ö°"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
