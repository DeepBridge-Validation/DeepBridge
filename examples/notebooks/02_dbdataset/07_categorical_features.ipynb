{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# üè∑Ô∏è Categorical Features Handling\n",
    "\n",
    "<div style=\"background-color: #e3f2fd; padding: 15px; border-radius: 5px; border-left: 5px solid #2196F3;\">\n",
    "<b>üìì Notebook Information</b><br>\n",
    "<b>Level:</b> Intermediate<br>\n",
    "<b>Estimated Time:</b> 15 minutes<br>\n",
    "<b>Prerequisites:</b> 01_basic_usage.ipynb<br>\n",
    "<b>Dataset:</b> Synthetic customer data\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- ‚úÖ Understand how DBDataset handles categorical features\n",
    "- ‚úÖ Use automatic categorical detection\n",
    "- ‚úÖ Apply different encoding strategies\n",
    "- ‚úÖ Handle high-cardinality categorical features\n",
    "- ‚úÖ Integrate categorical preprocessing with pipelines\n",
    "- ‚úÖ Avoid common categorical feature pitfalls\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Table of Contents\n",
    "\n",
    "1. [Introduction](#intro)\n",
    "2. [Setup](#setup)\n",
    "3. [Auto Detection](#auto)\n",
    "4. [Encoding Strategies](#encoding)\n",
    "5. [High Cardinality Features](#cardinality)\n",
    "6. [Best Practices](#practices)\n",
    "7. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "## 1. üìñ Introduction\n",
    "\n",
    "### What Are Categorical Features?\n",
    "\n",
    "> **Categorical features** represent discrete values from a fixed set of categories.\n",
    "\n",
    "**Examples:**\n",
    "- üé® **Nominal**: Color (red, blue, green), country, product type\n",
    "- üìä **Ordinal**: Education (HS, Bachelor, Master, PhD), rating (1-5 stars)\n",
    "- üî¢ **Binary**: Gender (M/F), yes/no questions\n",
    "\n",
    "### Why Do Categorical Features Matter?\n",
    "\n",
    "**The Challenge:**\n",
    "- ü§ñ **ML models need numbers** - Can't directly process \"red\", \"blue\"\n",
    "- üîÑ **Encoding required** - Must convert categories to numeric\n",
    "- ‚ö†Ô∏è **Wrong encoding = poor performance** - Different strategies for different cases\n",
    "\n",
    "**Real-world impact:**\n",
    "```python\n",
    "# ‚ùå BAD: Treating 'country' as numeric\n",
    "USA = 1, UK = 2, France = 3  # Implies USA < UK < France!\n",
    "\n",
    "# ‚úÖ GOOD: One-hot encoding\n",
    "USA = [1,0,0], UK = [0,1,0], France = [0,0,1]\n",
    "```\n",
    "\n",
    "### Common Encoding Methods\n",
    "\n",
    "| Method | Best For | Pros | Cons |\n",
    "|--------|----------|------|------|\n",
    "| **Label Encoding** | Ordinal features | Simple, fast | Implies ordering |\n",
    "| **One-Hot Encoding** | Low cardinality (<20) | No false ordering | High dimensionality |\n",
    "| **Target Encoding** | High cardinality | Compact | Risk of overfitting |\n",
    "| **Frequency Encoding** | High cardinality | Simple | Loses category identity |\n",
    "| **Binary Encoding** | Medium cardinality | Compact | Less interpretable |\n",
    "\n",
    "### DBDataset Approach\n",
    "\n",
    "DBDataset provides:\n",
    "- üîç **Automatic detection** - Identifies categorical columns\n",
    "- üõ†Ô∏è **Flexible encoding** - Works with any sklearn preprocessor\n",
    "- üîí **Safe handling** - Prevents common encoding mistakes\n",
    "\n",
    "**Let's see how!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 2. üõ†Ô∏è Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import (\n",
    "    LabelEncoder,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# DeepBridge\n",
    "from deepbridge import DBDataset, Experiment\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('Set2')\n",
    "%matplotlib inline\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(\"üè∑Ô∏è  Topic: Categorical Features with DBDataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_data",
   "metadata": {},
   "source": [
    "### Create Synthetic Dataset with Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Creating synthetic customer churn dataset...\\n\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "n_samples = 2000\n",
    "\n",
    "# Categorical features\n",
    "df = pd.DataFrame({\n",
    "    # Low cardinality\n",
    "    'contract_type': np.random.choice(['Month-to-Month', 'One Year', 'Two Year'], n_samples, p=[0.5, 0.3, 0.2]),\n",
    "    'payment_method': np.random.choice(['Credit Card', 'Bank Transfer', 'Electronic Check', 'Mailed Check'], n_samples),\n",
    "    'internet_service': np.random.choice(['DSL', 'Fiber Optic', 'No'], n_samples, p=[0.4, 0.4, 0.2]),\n",
    "    \n",
    "    # Ordinal\n",
    "    'customer_level': np.random.choice(['Bronze', 'Silver', 'Gold', 'Platinum'], n_samples, p=[0.4, 0.3, 0.2, 0.1]),\n",
    "    \n",
    "    # Binary\n",
    "    'paperless_billing': np.random.choice(['Yes', 'No'], n_samples),\n",
    "    'phone_service': np.random.choice(['Yes', 'No'], n_samples, p=[0.9, 0.1]),\n",
    "    \n",
    "    # High cardinality (simulated)\n",
    "    'city': np.random.choice([f'City_{i}' for i in range(50)], n_samples),\n",
    "    \n",
    "    # Numeric features\n",
    "    'tenure_months': np.random.randint(1, 73, n_samples),\n",
    "    'monthly_charges': np.random.uniform(20, 120, n_samples),\n",
    "    'total_charges': np.random.uniform(20, 8000, n_samples),\n",
    "})\n",
    "\n",
    "# Create target based on features (simulate business logic)\n",
    "churn_prob = (\n",
    "    (df['contract_type'] == 'Month-to-Month').astype(int) * 0.3 +\n",
    "    (df['tenure_months'] < 12).astype(int) * 0.25 +\n",
    "    (df['monthly_charges'] > 80).astype(int) * 0.2 +\n",
    "    (df['payment_method'] == 'Electronic Check').astype(int) * 0.15 +\n",
    "    np.random.uniform(0, 0.1, n_samples)  # Random noise\n",
    ")\n",
    "\n",
    "df['churn'] = (churn_prob > 0.5).astype(int)\n",
    "\n",
    "print(f\"‚úÖ Dataset created: {df.shape}\")\n",
    "print(f\"\\nüìã Feature types:\")\n",
    "print(f\"   Categorical: {df.select_dtypes(include='object').columns.tolist()}\")\n",
    "print(f\"   Numeric: {df.select_dtypes(include=['int64', 'float64']).drop('churn', axis=1).columns.tolist()}\")\n",
    "print(f\"\\nüìä Churn rate: {df['churn'].mean():.1%}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nüëÄ Sample data:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explore_categorical",
   "metadata": {},
   "source": [
    "### Explore Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Categorical Feature Analysis\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "\n",
    "cat_info = []\n",
    "for col in categorical_cols:\n",
    "    n_unique = df[col].nunique()\n",
    "    top_value = df[col].value_counts().index[0]\n",
    "    top_freq = df[col].value_counts().iloc[0] / len(df)\n",
    "    \n",
    "    # Classify cardinality\n",
    "    if n_unique <= 5:\n",
    "        cardinality = 'üü¢ Low'\n",
    "    elif n_unique <= 20:\n",
    "        cardinality = 'üü° Medium'\n",
    "    else:\n",
    "        cardinality = 'üî¥ High'\n",
    "    \n",
    "    cat_info.append({\n",
    "        'Feature': col,\n",
    "        'Unique Values': n_unique,\n",
    "        'Cardinality': cardinality,\n",
    "        'Top Value': top_value,\n",
    "        'Top Frequency': f\"{top_freq:.1%}\"\n",
    "    })\n",
    "\n",
    "cat_df = pd.DataFrame(cat_info)\n",
    "display(cat_df)\n",
    "\n",
    "print(\"\\nüí° Cardinality Guide:\")\n",
    "print(\"   üü¢ Low (‚â§5): Use one-hot encoding\")\n",
    "print(\"   üü° Medium (6-20): Use one-hot or binary encoding\")\n",
    "print(\"   üî¥ High (>20): Use target/frequency encoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auto",
   "metadata": {},
   "source": [
    "<a id=\"auto\"></a>\n",
    "## 3. üîç Auto Detection\n",
    "\n",
    "### DBDataset Automatic Categorical Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auto_detect",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Auto-Detection of Categorical Features\\n\")\n",
    "print(\"   DBDataset can automatically identify categorical columns (object dtype)\")\n",
    "\n",
    "# For DBDataset, we need to encode categorical features first\n",
    "# Let's use one-hot encoding for low cardinality features\n",
    "print(\"\\n‚ö†Ô∏è  Important: sklearn models need numeric input!\")\n",
    "print(\"   You must encode categorical features before creating DBDataset\")\n",
    "print(\"\\n‚úÖ Best practice: Use sklearn pipelines for encoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encoding",
   "metadata": {},
   "source": [
    "<a id=\"encoding\"></a>\n",
    "## 4. üîß Encoding Strategies\n",
    "\n",
    "### Strategy 1: One-Hot Encoding (Low Cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "onehot",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Strategy 1: One-Hot Encoding\\n\")\n",
    "print(\"   Best for: Low cardinality features (‚â§10 categories)\")\n",
    "\n",
    "# Select low cardinality categorical features\n",
    "low_card_features = ['contract_type', 'payment_method', 'internet_service', \n",
    "                     'paperless_billing', 'phone_service']\n",
    "\n",
    "# One-hot encode\n",
    "df_encoded = pd.get_dummies(df, columns=low_card_features, drop_first=True)\n",
    "\n",
    "print(f\"\\nüìä Before encoding: {len(df.columns)} columns\")\n",
    "print(f\"   After encoding: {len(df_encoded.columns)} columns\")\n",
    "print(f\"   Added: {len(df_encoded.columns) - len(df.columns)} dummy columns\")\n",
    "\n",
    "# Show encoded columns\n",
    "encoded_cols = [col for col in df_encoded.columns if any(feat in col for feat in low_card_features)]\n",
    "print(f\"\\n‚úÖ Encoded columns (sample):\")\n",
    "for col in encoded_cols[:8]:\n",
    "    print(f\"   ‚Ä¢ {col}\")\n",
    "if len(encoded_cols) > 8:\n",
    "    print(f\"   ... and {len(encoded_cols) - 8} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinal",
   "metadata": {},
   "source": [
    "### Strategy 2: Ordinal Encoding (Ordinal Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinal_encode",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Strategy 2: Ordinal Encoding\\n\")\n",
    "print(\"   Best for: Features with natural ordering\")\n",
    "\n",
    "# Define ordering for customer_level\n",
    "level_order = ['Bronze', 'Silver', 'Gold', 'Platinum']\n",
    "\n",
    "# Create ordinal mapping\n",
    "level_mapping = {level: i for i, level in enumerate(level_order)}\n",
    "\n",
    "df_encoded['customer_level_encoded'] = df['customer_level'].map(level_mapping)\n",
    "\n",
    "print(f\"‚úÖ Ordinal encoding for 'customer_level':\")\n",
    "print(f\"\\n   Mapping:\")\n",
    "for level, code in level_mapping.items():\n",
    "    print(f\"   {level:10s} ‚Üí {code}\")\n",
    "\n",
    "# Show distribution\n",
    "comparison = pd.DataFrame({\n",
    "    'Original': df['customer_level'].value_counts().sort_index(),\n",
    "    'Encoded': df_encoded['customer_level_encoded'].value_counts().sort_index()\n",
    "})\n",
    "\n",
    "print(f\"\\nüìä Distribution comparison:\")\n",
    "display(comparison)\n",
    "\n",
    "print(\"\\nüí° Ordinal encoding preserves the ordering: Bronze < Silver < Gold < Platinum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "target_encoding",
   "metadata": {},
   "source": [
    "### Strategy 3: Frequency Encoding (High Cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Strategy 3: Frequency Encoding\\n\")\n",
    "print(\"   Best for: High cardinality features (>20 categories)\")\n",
    "\n",
    "# Calculate frequency for 'city'\n",
    "city_freq = df['city'].value_counts(normalize=True)\n",
    "\n",
    "# Map frequencies\n",
    "df_encoded['city_frequency'] = df['city'].map(city_freq)\n",
    "\n",
    "print(f\"‚úÖ Frequency encoding for 'city' (50 unique values):\")\n",
    "print(f\"\\n   Top 5 cities by frequency:\")\n",
    "for city, freq in city_freq.head().items():\n",
    "    print(f\"   {city:12s} ‚Üí {freq:.3f} ({freq*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìä Frequency encoding reduces:\")\n",
    "print(f\"   From: 50 unique cities (would need 49 one-hot columns)\")\n",
    "print(f\"   To: 1 frequency column\")\n",
    "print(f\"\\nüí° Huge dimensionality reduction for high cardinality features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline",
   "metadata": {},
   "source": [
    "### Strategy 4: Using sklearn Pipeline (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline_encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Strategy 4: sklearn Pipeline (Production-Ready)\\n\")\n",
    "print(\"   Best practice: Encapsulate all preprocessing in a pipeline\")\n",
    "\n",
    "# Separate features by type\n",
    "numeric_features = ['tenure_months', 'monthly_charges', 'total_charges']\n",
    "categorical_low = ['contract_type', 'payment_method', 'internet_service']\n",
    "categorical_binary = ['paperless_billing', 'phone_service']\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), numeric_features),\n",
    "    (OneHotEncoder(drop='first', sparse_output=False), categorical_low),\n",
    "    (OneHotEncoder(drop='first', sparse_output=False), categorical_binary),\n",
    "    remainder='drop'  # Drop high cardinality features for this example\n",
    ")\n",
    "\n",
    "# Create full pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Pipeline created:\")\n",
    "print(\"\\n   Step 1: StandardScaler for numeric features\")\n",
    "print(f\"           {numeric_features}\")\n",
    "print(\"\\n   Step 2: OneHotEncoder for categorical features\")\n",
    "print(f\"           {categorical_low + categorical_binary}\")\n",
    "print(\"\\n   Step 3: RandomForest classifier\")\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop('churn', axis=1)\n",
    "y = df['churn']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "# Fit pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n‚úÖ Pipeline trained!\")\n",
    "print(f\"   Accuracy: {acc:.3f}\")\n",
    "print(f\"\\nüí° Pipeline handles all encoding automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdataset_pipeline",
   "metadata": {},
   "source": [
    "### Use Pipeline with DBDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdataset_with_pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî¨ Using Pipeline with DBDataset\\n\")\n",
    "\n",
    "# Create DBDataset with the pipeline\n",
    "dataset = DBDataset(\n",
    "    data=df,\n",
    "    target_column='churn',\n",
    "    model=pipeline,  # Pass the entire pipeline!\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"‚úÖ DBDataset created with preprocessing pipeline\")\n",
    "print(f\"   Total features: {len(dataset.features)}\")\n",
    "print(f\"   Target: {dataset.target_column}\")\n",
    "\n",
    "# Run a quick test\n",
    "exp = Experiment(\n",
    "    dataset=dataset,\n",
    "    experiment_type='binary_classification',\n",
    "    experiment_name='Churn Prediction with Categorical Features',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"\\nüî¨ Running robustness test...\")\n",
    "result = exp.run_test('robustness', config='quick')\n",
    "\n",
    "print(\"\\n‚úÖ Test complete!\")\n",
    "print(\"   Pipeline correctly handles categorical features during testing\")\n",
    "print(\"\\nüí° This is the recommended approach for production!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardinality",
   "metadata": {},
   "source": [
    "<a id=\"cardinality\"></a>\n",
    "## 5. üî¢ High Cardinality Features\n",
    "\n",
    "### The High Cardinality Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardinality_problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö†Ô∏è  The High Cardinality Problem\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Simulate different cardinality levels\n",
    "cardinality_scenarios = [\n",
    "    {'name': 'Contract Type', 'unique': 3, 'encoding': 'One-Hot', 'columns_created': 2},\n",
    "    {'name': 'Country', 'unique': 195, 'encoding': 'One-Hot', 'columns_created': 194},\n",
    "    {'name': 'Zip Code', 'unique': 40000, 'encoding': 'One-Hot', 'columns_created': 39999},\n",
    "    {'name': 'Customer ID', 'unique': 1000000, 'encoding': 'One-Hot', 'columns_created': 999999},\n",
    "]\n",
    "\n",
    "cardinality_df = pd.DataFrame(cardinality_scenarios)\n",
    "cardinality_df['Memory (MB)'] = cardinality_df['columns_created'] * 8 * 10000 / (1024 * 1024)  # Rough estimate\n",
    "\n",
    "print(\"üìä One-Hot Encoding Memory Impact:\\n\")\n",
    "display(cardinality_df.style.background_gradient(\n",
    "    cmap='Reds', subset=['columns_created', 'Memory (MB)']\n",
    ").format({\n",
    "    'Memory (MB)': '{:.1f}'\n",
    "}))\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Problems with high cardinality one-hot encoding:\")\n",
    "print(\"   1. üíæ Huge memory consumption\")\n",
    "print(\"   2. ‚è±Ô∏è  Slow training and inference\")\n",
    "print(\"   3. üìâ Curse of dimensionality (sparse data)\")\n",
    "print(\"   4. üéØ Overfitting risk\")\n",
    "print(\"\\n‚úÖ Solution: Use alternative encoding methods!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solutions",
   "metadata": {},
   "source": [
    "### Solutions for High Cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solutions_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Solutions for High Cardinality Features\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "solutions = pd.DataFrame({\n",
    "    'Method': [\n",
    "        'Frequency Encoding',\n",
    "        'Target Encoding',\n",
    "        'Feature Hashing',\n",
    "        'Category Grouping',\n",
    "        'Embeddings (Deep Learning)'\n",
    "    ],\n",
    "    'Columns Created': [1, 1, 'Fixed (e.g., 32)', '1 per group', 'Fixed (e.g., 50)'],\n",
    "    'Best For': [\n",
    "        'Any cardinality',\n",
    "        'Very high cardinality',\n",
    "        'Extremely high (millions)',\n",
    "        'Domain knowledge available',\n",
    "        'Deep learning models'\n",
    "    ],\n",
    "    'Pros': [\n",
    "        'Simple, fast, no overfitting',\n",
    "        'Captures target relationship',\n",
    "        'Handles unseen categories',\n",
    "        'Interpretable, reduces noise',\n",
    "        'Learns optimal representation'\n",
    "    ],\n",
    "    'Cons': [\n",
    "        'Loses category identity',\n",
    "        'Risk of overfitting',\n",
    "        'Hash collisions',\n",
    "        'Requires domain expertise',\n",
    "        'Complex, needs more data'\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(solutions.style.set_properties(**{\n",
    "    'text-align': 'left',\n",
    "    'white-space': 'pre-wrap'\n",
    "}))\n",
    "\n",
    "print(\"\\nüí° Rule of Thumb:\")\n",
    "print(\"   ‚Ä¢ <10 categories: One-Hot Encoding\")\n",
    "print(\"   ‚Ä¢ 10-50 categories: Frequency or Target Encoding\")\n",
    "print(\"   ‚Ä¢ 50-1000 categories: Target Encoding or Hashing\")\n",
    "print(\"   ‚Ä¢ >1000 categories: Feature Hashing or Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practices",
   "metadata": {},
   "source": [
    "<a id=\"practices\"></a>\n",
    "## 6. ‚ú® Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dos_donts",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f5e9; padding: 15px; border-radius: 5px; border-left: 5px solid #4CAF50;\">\n",
    "<b>‚úÖ DO</b><br><br>\n",
    "\n",
    "1. **Use Pipelines**\n",
    "   ```python\n",
    "   # ‚úÖ GOOD: Encoding in pipeline\n",
    "   pipeline = Pipeline([\n",
    "       ('encoder', OneHotEncoder()),\n",
    "       ('model', RandomForestClassifier())\n",
    "   ])\n",
    "   pipeline.fit(X_train, y_train)  # Fits encoder on train only!\n",
    "   ```\n",
    "\n",
    "2. **Handle Unknown Categories**\n",
    "   ```python\n",
    "   # ‚úÖ GOOD: Handle unseen categories in production\n",
    "   encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "   ```\n",
    "\n",
    "3. **Check Cardinality**\n",
    "   ```python\n",
    "   # ‚úÖ GOOD: Choose encoding based on cardinality\n",
    "   n_unique = df['city'].nunique()\n",
    "   if n_unique < 10:\n",
    "       encoding = 'one-hot'\n",
    "   else:\n",
    "       encoding = 'frequency'\n",
    "   ```\n",
    "\n",
    "4. **Preserve Ordinal Relationships**\n",
    "   - Use OrdinalEncoder for features with natural ordering\n",
    "   - Define explicit ordering to avoid arbitrary assignment\n",
    "\n",
    "5. **Drop First Category (One-Hot)**\n",
    "   - Prevents multicollinearity\n",
    "   - Reduces dimensionality by 1 per feature\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #ffebee; padding: 15px; border-radius: 5px; border-left: 5px solid #f44336; margin-top: 15px;\">\n",
    "<b>‚ùå DON'T</b><br><br>\n",
    "\n",
    "1. **Encode on Full Dataset**\n",
    "   ```python\n",
    "   # ‚ùå BAD: Fit on all data (leakage!)\n",
    "   df_encoded = pd.get_dummies(df)\n",
    "   X_train, X_test = train_test_split(df_encoded)\n",
    "   ```\n",
    "   ```python\n",
    "   # ‚úÖ GOOD: Fit on train only\n",
    "   X_train, X_test = train_test_split(df)\n",
    "   encoder.fit(X_train)\n",
    "   X_train_enc = encoder.transform(X_train)\n",
    "   X_test_enc = encoder.transform(X_test)\n",
    "   ```\n",
    "\n",
    "2. **Use Label Encoding for Nominal Features**\n",
    "   ```python\n",
    "   # ‚ùå BAD: Implies red < blue < green!\n",
    "   df['color'] = LabelEncoder().fit_transform(df['color'])\n",
    "   ```\n",
    "\n",
    "3. **One-Hot Encode High Cardinality**\n",
    "   ```python\n",
    "   # ‚ùå BAD: 1000 columns!\n",
    "   df_encoded = pd.get_dummies(df['zip_code'])\n",
    "   ```\n",
    "\n",
    "4. **Ignore Unknown Categories**\n",
    "   - Always handle unseen categories in production\n",
    "   - Use `handle_unknown='ignore'` or frequency encoding\n",
    "\n",
    "5. **Encode Target Variable**\n",
    "   - Target encoding is for features, not the target itself\n",
    "   - For classification, sklearn handles string labels automatically\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## 7. üéì Conclusion\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "- ‚úÖ **Categorical types** - Nominal, ordinal, binary\n",
    "- ‚úÖ **Encoding methods** - One-hot, ordinal, frequency, target\n",
    "- ‚úÖ **Cardinality handling** - Different strategies for different scales\n",
    "- ‚úÖ **Pipeline integration** - Production-ready preprocessing\n",
    "- ‚úÖ **DBDataset usage** - Pass pipelines for automatic handling\n",
    "- ‚úÖ **Best practices** - Avoid leakage, handle unknowns\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. üéØ **Choose encoding wisely** - Based on cardinality and feature type\n",
    "2. üîí **Prevent data leakage** - Fit encoders on training data only\n",
    "3. ‚ö° **Watch cardinality** - High cardinality = high dimensionality\n",
    "4. üõ†Ô∏è **Use pipelines** - Encapsulate all preprocessing\n",
    "5. üîÑ **Handle unknowns** - Production will see new categories\n",
    "6. üìä **Preserve relationships** - Use ordinal encoding when appropriate\n",
    "\n",
    "### Encoding Decision Tree\n",
    "\n",
    "```\n",
    "Is the feature categorical?\n",
    "‚îú‚îÄ‚îÄ No ‚Üí Use as-is (numeric)\n",
    "‚îî‚îÄ‚îÄ Yes\n",
    "    ‚îú‚îÄ‚îÄ Is it ordinal (natural order)?\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ Yes ‚Üí Ordinal Encoding\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ No ‚Üí Check cardinality\n",
    "    ‚îÇ       ‚îú‚îÄ‚îÄ Low (<10) ‚Üí One-Hot Encoding\n",
    "    ‚îÇ       ‚îú‚îÄ‚îÄ Medium (10-50) ‚Üí Frequency/Target Encoding\n",
    "    ‚îÇ       ‚îî‚îÄ‚îÄ High (>50) ‚Üí Target Encoding / Hashing / Embeddings\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Try target encoding** (category_encoders library)\n",
    "2. **Experiment with embeddings** (for deep learning)\n",
    "3. **Handle missing values** in categorical features\n",
    "4. **Feature engineering** (create category interactions)\n",
    "\n",
    "---\n",
    "\n",
    "**Remember: The right encoding can make or break your model!** üè∑Ô∏è‚ú®"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
