{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# üîå Advanced Extensibility\n",
    "\n",
    "<div style=\"background-color: #e3f2fd; padding: 15px; border-radius: 5px; border-left: 5px solid #2196F3;\">\n",
    "<b>üìì Notebook Information</b><br>\n",
    "<b>Level:</b> Advanced<br>\n",
    "<b>Estimated Time:</b> 30 minutes<br>\n",
    "<b>Prerequisites:</b> All basic and intermediate notebooks<br>\n",
    "<b>Dataset:</b> Custom synthetic data\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- ‚úÖ Create custom validation tests\n",
    "- ‚úÖ Extend DeepBridge with custom metrics\n",
    "- ‚úÖ Build custom data transformers\n",
    "- ‚úÖ Implement custom report renderers\n",
    "- ‚úÖ Integrate external validation frameworks\n",
    "- ‚úÖ Design reusable validation components\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Table of Contents\n",
    "\n",
    "1. [Introduction](#intro)\n",
    "2. [Setup](#setup)\n",
    "3. [Custom Validation Tests](#custom-tests)\n",
    "4. [Custom Metrics](#custom-metrics)\n",
    "5. [Custom Transformers](#transformers)\n",
    "6. [Custom Report Renderers](#renderers)\n",
    "7. [Plugin Architecture](#plugins)\n",
    "8. [Best Practices](#practices)\n",
    "9. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "## 1. üìñ Introduction\n",
    "\n",
    "### Why Extend DeepBridge?\n",
    "\n",
    "**The Reality:**\n",
    "- üéØ **Domain-specific needs** - Every industry has unique requirements\n",
    "- üìä **Custom metrics** - Standard metrics don't capture everything\n",
    "- üî¨ **Novel tests** - New research ‚Üí new validation techniques\n",
    "- üè¢ **Company standards** - Organization-specific validation policies\n",
    "- üîå **Integration** - Connect with existing tools and workflows\n",
    "\n",
    "**Real-world examples:**\n",
    "\n",
    "1. **Medical AI**\n",
    "   ```python\n",
    "   # Custom test: Clinical safety margin\n",
    "   class ClinicalSafetyTest(CustomTest):\n",
    "       \"\"\"Ensure model errs on the side of caution\"\"\"\n",
    "       def run(self, dataset):\n",
    "           # False negatives > false positives for cancer detection\n",
    "           return validate_safety_margin(dataset)\n",
    "   ```\n",
    "\n",
    "2. **Finance**\n",
    "   ```python\n",
    "   # Custom metric: Economic value\n",
    "   def profit_based_accuracy(y_true, y_pred, loan_amounts):\n",
    "       \"\"\"Weight accuracy by loan size\"\"\"\n",
    "       return calculate_expected_profit(y_true, y_pred, loan_amounts)\n",
    "   ```\n",
    "\n",
    "3. **E-commerce**\n",
    "   ```python\n",
    "   # Custom transformer: Seasonality injection\n",
    "   class SeasonalTransformer:\n",
    "       \"\"\"Test resilience to seasonal patterns\"\"\"\n",
    "       def transform(self, X):\n",
    "           return inject_seasonal_patterns(X)\n",
    "   ```\n",
    "\n",
    "### DeepBridge Extension Points\n",
    "\n",
    "```\n",
    "DeepBridge Core\n",
    "‚îú‚îÄ‚îÄ 1. Custom Tests\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Inherit from BaseTest\n",
    "‚îú‚îÄ‚îÄ 2. Custom Metrics  \n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Register with MetricRegistry\n",
    "‚îú‚îÄ‚îÄ 3. Custom Transformers\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Implement transform() method\n",
    "‚îú‚îÄ‚îÄ 4. Custom Renderers\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Inherit from BaseRenderer\n",
    "‚îî‚îÄ‚îÄ 5. Plugins\n",
    "    ‚îî‚îÄ‚îÄ Package as installable module\n",
    "```\n",
    "\n",
    "### Extension Philosophy\n",
    "\n",
    "| Principle | Description | Example |\n",
    "|-----------|-------------|----------|\n",
    "| **Open/Closed** | Open for extension, closed for modification | Add tests without changing core |\n",
    "| **Composability** | Combine components like LEGO | Chain transformers |\n",
    "| **Discoverability** | Easy to find and use extensions | Auto-register plugins |\n",
    "| **Consistency** | Follow same API patterns | Same interface as built-in tests |\n",
    "\n",
    "**Let's build custom components!** üîå"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 2. üõ†Ô∏è Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Any, Optional, List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# sklearn\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# DeepBridge\n",
    "from deepbridge import DBDataset, Experiment\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('Set2')\n",
    "%matplotlib inline\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(\"üîå Topic: Advanced Extensibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-tests",
   "metadata": {},
   "source": [
    "<a id=\"custom-tests\"></a>\n",
    "## 3. üß™ Custom Validation Tests\n",
    "\n",
    "### Define Base Test Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "base_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß™ Creating Custom Validation Test Framework\\n\")\n",
    "\n",
    "# Base class for custom tests\n",
    "class CustomValidationTest(ABC):\n",
    "    \"\"\"Base class for custom validation tests.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, description: str):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.results = {}\n",
    "    \n",
    "    @abstractmethod\n",
    "    def run(self, dataset: DBDataset) -> Dict[str, Any]:\n",
    "        \"\"\"Run the validation test.\n",
    "        \n",
    "        Args:\n",
    "            dataset: DBDataset to validate\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with test results\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_status(self, results: Dict[str, Any]) -> str:\n",
    "        \"\"\"Determine test status (PASS/FAIL/WARNING).\"\"\"\n",
    "        if 'status' in results:\n",
    "            return results['status']\n",
    "        return 'UNKNOWN'\n",
    "    \n",
    "    def report(self) -> str:\n",
    "        \"\"\"Generate human-readable report.\"\"\"\n",
    "        status = self.get_status(self.results)\n",
    "        return f\"{self.name}: {status}\"\n",
    "\n",
    "print(\"‚úÖ Base test interface defined\")\n",
    "print(\"\\nüìã Required methods:\")\n",
    "print(\"   ‚Ä¢ run(dataset) ‚Üí results\")\n",
    "print(\"   ‚Ä¢ get_status(results) ‚Üí PASS/FAIL/WARNING\")\n",
    "print(\"   ‚Ä¢ report() ‚Üí human-readable summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example_test",
   "metadata": {},
   "source": [
    "### Example: Business Logic Validation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "business_logic_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Creating Business Logic Validation Test\\n\")\n",
    "\n",
    "class BusinessLogicTest(CustomValidationTest):\n",
    "    \"\"\"Validate model predictions against business rules.\"\"\"\n",
    "    \n",
    "    def __init__(self, rules: Dict[str, Any]):\n",
    "        super().__init__(\n",
    "            name=\"Business Logic Validation\",\n",
    "            description=\"Ensure model respects business constraints\"\n",
    "        )\n",
    "        self.rules = rules\n",
    "    \n",
    "    def run(self, dataset: DBDataset) -> Dict[str, Any]:\n",
    "        \"\"\"Run business logic validation.\"\"\"\n",
    "        results = {\n",
    "            'test_name': self.name,\n",
    "            'violations': [],\n",
    "            'rules_checked': len(self.rules),\n",
    "            'rules_passed': 0\n",
    "        }\n",
    "        \n",
    "        # Get predictions\n",
    "        y_pred = dataset.model.predict(dataset.X_test)\n",
    "        y_proba = dataset.model.predict_proba(dataset.X_test)\n",
    "        \n",
    "        # Rule 1: Minimum confidence threshold\n",
    "        if 'min_confidence' in self.rules:\n",
    "            threshold = self.rules['min_confidence']\n",
    "            max_probs = y_proba.max(axis=1)\n",
    "            low_confidence = (max_probs < threshold).sum()\n",
    "            \n",
    "            if low_confidence > 0:\n",
    "                results['violations'].append({\n",
    "                    'rule': 'Minimum Confidence',\n",
    "                    'threshold': threshold,\n",
    "                    'violations': int(low_confidence),\n",
    "                    'percentage': float(low_confidence / len(y_pred) * 100)\n",
    "                })\n",
    "            else:\n",
    "                results['rules_passed'] += 1\n",
    "        \n",
    "        # Rule 2: Maximum false negative rate (for safety-critical apps)\n",
    "        if 'max_fnr' in self.rules:\n",
    "            from sklearn.metrics import confusion_matrix\n",
    "            cm = confusion_matrix(dataset.y_test, y_pred)\n",
    "            fn = cm[1, 0] if cm.shape[0] > 1 else 0\n",
    "            tp = cm[1, 1] if cm.shape[0] > 1 else 0\n",
    "            fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "            \n",
    "            max_fnr = self.rules['max_fnr']\n",
    "            if fnr > max_fnr:\n",
    "                results['violations'].append({\n",
    "                    'rule': 'Maximum False Negative Rate',\n",
    "                    'threshold': max_fnr,\n",
    "                    'actual': float(fnr),\n",
    "                    'severity': 'CRITICAL'\n",
    "                })\n",
    "            else:\n",
    "                results['rules_passed'] += 1\n",
    "        \n",
    "        # Rule 3: Class distribution limits\n",
    "        if 'class_distribution' in self.rules:\n",
    "            pred_dist = np.bincount(y_pred) / len(y_pred)\n",
    "            expected = self.rules['class_distribution']\n",
    "            \n",
    "            for cls, (min_pct, max_pct) in expected.items():\n",
    "                actual_pct = pred_dist[cls] if cls < len(pred_dist) else 0\n",
    "                if not (min_pct <= actual_pct <= max_pct):\n",
    "                    results['violations'].append({\n",
    "                        'rule': f'Class {cls} Distribution',\n",
    "                        'expected_range': f'{min_pct:.1%} - {max_pct:.1%}',\n",
    "                        'actual': f'{actual_pct:.1%}'\n",
    "                    })\n",
    "                else:\n",
    "                    results['rules_passed'] += 1\n",
    "        \n",
    "        # Determine status\n",
    "        if len(results['violations']) == 0:\n",
    "            results['status'] = 'PASS'\n",
    "        elif any(v.get('severity') == 'CRITICAL' for v in results['violations']):\n",
    "            results['status'] = 'FAIL'\n",
    "        else:\n",
    "            results['status'] = 'WARNING'\n",
    "        \n",
    "        self.results = results\n",
    "        return results\n",
    "\n",
    "print(\"‚úÖ Business Logic Test created\")\n",
    "print(\"\\nüìã Validated rules:\")\n",
    "print(\"   ‚Ä¢ Minimum prediction confidence\")\n",
    "print(\"   ‚Ä¢ Maximum false negative rate (safety-critical)\")\n",
    "print(\"   ‚Ä¢ Class distribution constraints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_custom_test",
   "metadata": {},
   "source": [
    "### Run Custom Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî¨ Running Custom Business Logic Test\\n\")\n",
    "\n",
    "# Create dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_classes=2,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "df['target'] = y\n",
    "\n",
    "# Train model\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop('target', axis=1), df['target'],\n",
    "    test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Create DBDataset\n",
    "dataset = DBDataset(\n",
    "    data=df,\n",
    "    target_column='target',\n",
    "    model=model,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Define business rules\n",
    "business_rules = {\n",
    "    'min_confidence': 0.7,  # Predictions must be 70%+ confident\n",
    "    'max_fnr': 0.15,  # False negative rate < 15%\n",
    "    'class_distribution': {\n",
    "        0: (0.3, 0.7),  # Class 0: 30-70%\n",
    "        1: (0.3, 0.7)   # Class 1: 30-70%\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run test\n",
    "test = BusinessLogicTest(rules=business_rules)\n",
    "results = test.run(dataset)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Test: {results['test_name']}\")\n",
    "print(f\"Status: {results['status']}\")\n",
    "print(f\"Rules Checked: {results['rules_checked']}\")\n",
    "print(f\"Rules Passed: {results['rules_passed']}\")\n",
    "\n",
    "if results['violations']:\n",
    "    print(f\"\\n‚ö†Ô∏è  Violations ({len(results['violations'])}):\")\n",
    "    for v in results['violations']:\n",
    "        print(f\"\\n   Rule: {v['rule']}\")\n",
    "        for key, value in v.items():\n",
    "            if key != 'rule':\n",
    "                print(f\"      {key}: {value}\")\nelse:\n",
    "    print(f\"\\n‚úÖ All business rules satisfied!\")\n",
    "\n",
    "print(\"\\nüí° This test ensures model behavior aligns with business requirements!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-metrics",
   "metadata": {},
   "source": [
    "<a id=\"custom-metrics\"></a>\n",
    "## 4. üìä Custom Metrics\n",
    "\n",
    "### Define Domain-Specific Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom_metrics_def",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Creating Custom Metrics\\n\")\n",
    "\n",
    "# Custom Metric 1: Cost-sensitive accuracy\n",
    "def cost_weighted_accuracy(y_true, y_pred, costs: Dict[str, float]) -> float:\n",
    "    \"\"\"Calculate accuracy weighted by misclassification costs.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        costs: Dictionary of costs {'fn': cost, 'fp': cost}\n",
    "    \n",
    "    Returns:\n",
    "        Cost-weighted accuracy score\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    \n",
    "    # Total cost of errors\n",
    "    total_cost = fn * costs.get('fn', 1.0) + fp * costs.get('fp', 1.0)\n",
    "    \n",
    "    # Maximum possible cost (all predictions wrong)\n",
    "    max_cost = len(y_true) * max(costs.get('fn', 1.0), costs.get('fp', 1.0))\n",
    "    \n",
    "    # Cost-weighted accuracy (1 = perfect, 0 = worst)\n",
    "    return 1 - (total_cost / max_cost) if max_cost > 0 else 1.0\n",
    "\n",
    "\n",
    "# Custom Metric 2: Balanced performance score\n",
    "def balanced_performance_score(y_true, y_pred, weights: Dict[str, float]) -> float:\n",
    "    \"\"\"Composite score combining multiple metrics.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels  \n",
    "        weights: Metric weights {'accuracy': w, 'precision': w, 'recall': w}\n",
    "    \n",
    "    Returns:\n",
    "        Weighted composite score\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    score = (\n",
    "        weights.get('accuracy', 1.0) * acc +\n",
    "        weights.get('precision', 1.0) * prec +\n",
    "        weights.get('recall', 1.0) * rec\n",
    "    )\n",
    "    \n",
    "    total_weight = sum(weights.values())\n",
    "    return score / total_weight if total_weight > 0 else 0.0\n",
    "\n",
    "\n",
    "# Custom Metric 3: Confidence-calibrated accuracy\n",
    "def confidence_calibrated_accuracy(y_true, y_pred, y_proba) -> float:\n",
    "    \"\"\"Accuracy adjusted for prediction confidence.\n",
    "    \n",
    "    Rewards high-confidence correct predictions,\n",
    "    penalizes high-confidence incorrect predictions.\n",
    "    \"\"\"\n",
    "    max_proba = y_proba.max(axis=1)\n",
    "    correct = (y_true == y_pred).astype(float)\n",
    "    \n",
    "    # Weight by confidence: correct predictions gain, wrong lose\n",
    "    weighted_correct = correct * max_proba - (1 - correct) * max_proba\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    return (weighted_correct.mean() + 1) / 2\n",
    "\n",
    "\n",
    "print(\"‚úÖ Custom metrics defined:\")\n",
    "print(\"\\n1. Cost-Weighted Accuracy\")\n",
    "print(\"   ‚Ä¢ Accounts for different error costs\")\n",
    "print(\"   ‚Ä¢ Use case: Fraud detection, medical diagnosis\")\n",
    "print(\"\\n2. Balanced Performance Score\")\n",
    "print(\"   ‚Ä¢ Composite of accuracy, precision, recall\")\n",
    "print(\"   ‚Ä¢ Configurable weights for each metric\")\n",
    "print(\"\\n3. Confidence-Calibrated Accuracy\")\n",
    "print(\"   ‚Ä¢ Penalizes overconfident wrong predictions\")\n",
    "print(\"   ‚Ä¢ Rewards calibrated confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "use_custom_metrics",
   "metadata": {},
   "source": [
    "### Test Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß™ Testing Custom Metrics\\n\")\n",
    "\n",
    "# Get predictions\n",
    "y_true = dataset.y_test\n",
    "y_pred = model.predict(dataset.X_test)\n",
    "y_proba = model.predict_proba(dataset.X_test)\n",
    "\n",
    "# Standard metrics\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, average='weighted')\n",
    "rec = recall_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Custom metrics\n",
    "cost_acc = cost_weighted_accuracy(y_true, y_pred, costs={'fn': 10, 'fp': 1})\n",
    "balanced = balanced_performance_score(\n",
    "    y_true, y_pred, \n",
    "    weights={'accuracy': 0.3, 'precision': 0.3, 'recall': 0.4}\n",
    ")\n",
    "conf_acc = confidence_calibrated_accuracy(y_true, y_pred, y_proba)\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Standard Accuracy',\n",
    "        'Standard Precision',\n",
    "        'Standard Recall',\n",
    "        '---',\n",
    "        'Cost-Weighted Accuracy',\n",
    "        'Balanced Performance Score',\n",
    "        'Confidence-Calibrated Accuracy'\n",
    "    ],\n",
    "    'Score': [\n",
    "        f'{acc:.4f}',\n",
    "        f'{prec:.4f}',\n",
    "        f'{rec:.4f}',\n",
    "        '---',\n",
    "        f'{cost_acc:.4f}',\n",
    "        f'{balanced:.4f}',\n",
    "        f'{conf_acc:.4f}'\n",
    "    ],\n",
    "    'Type': [\n",
    "        'Standard', 'Standard', 'Standard', '',\n",
    "        'Custom', 'Custom', 'Custom'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"üìä Metrics Comparison:\\n\")\n",
    "display(results_df.style.apply(\n",
    "    lambda x: ['background-color: #e3f2fd' if v == 'Custom' \n",
    "               else 'background-color: #f5f5f5' if v == 'Standard'\n",
    "               else '' for v in x],\n",
    "    subset=['Type']\n",
    ").hide(axis='index'))\n",
    "\n",
    "print(\"\\nüí° Custom metrics provide domain-specific insights!\")\n",
    "print(f\"   Cost-weighted accuracy is lower ({cost_acc:.3f} vs {acc:.3f})\")\n",
    "print(f\"   because false negatives are 10x more expensive than false positives.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transformers",
   "metadata": {},
   "source": [
    "<a id=\"transformers\"></a>\n",
    "## 5. üîÑ Custom Transformers\n",
    "\n",
    "### Build Data Transformation Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom_transformer",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Creating Custom Data Transformers\\n\")\n",
    "\n",
    "class BusinessHoursTransformer:\n",
    "    \"\"\"Transform data to simulate business hours patterns.\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_cols: List[str], scale_factor: float = 0.3):\n",
    "        self.feature_cols = feature_cols\n",
    "        self.scale_factor = scale_factor\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Simulate business hours by scaling certain features.\"\"\"\n",
    "        X_transformed = X.copy()\n",
    "        \n",
    "        for col in self.feature_cols:\n",
    "            if col in X_transformed.columns:\n",
    "                # Add temporal pattern (sinusoidal)\n",
    "                pattern = np.sin(np.linspace(0, 2*np.pi, len(X_transformed)))\n",
    "                X_transformed[col] = X_transformed[col] * (1 + self.scale_factor * pattern)\n",
    "        \n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "class OutlierInjectionTransformer:\n",
    "    \"\"\"Inject outliers to test model robustness.\"\"\"\n",
    "    \n",
    "    def __init__(self, outlier_fraction: float = 0.05, std_multiplier: float = 3.0):\n",
    "        self.outlier_fraction = outlier_fraction\n",
    "        self.std_multiplier = std_multiplier\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Inject outliers into random samples.\"\"\"\n",
    "        X_transformed = X.copy()\n",
    "        n_outliers = int(len(X) * self.outlier_fraction)\n",
    "        \n",
    "        # Select random samples to corrupt\n",
    "        outlier_idx = np.random.choice(len(X), n_outliers, replace=False)\n",
    "        \n",
    "        # Inject outliers (shift by multiple of std)\n",
    "        for col in X_transformed.columns:\n",
    "            if X_transformed[col].dtype in ['float64', 'float32', 'int64', 'int32']:\n",
    "                std = X_transformed[col].std()\n",
    "                X_transformed.loc[outlier_idx, col] += np.random.choice(\n",
    "                    [-1, 1], size=n_outliers\n",
    "                ) * self.std_multiplier * std\n",
    "        \n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "class MissingDataTransformer:\n",
    "    \"\"\"Simulate missing data patterns.\"\"\"\n",
    "    \n",
    "    def __init__(self, missing_fraction: float = 0.1, pattern: str = 'random'):\n",
    "        self.missing_fraction = missing_fraction\n",
    "        self.pattern = pattern  # 'random', 'mcar', 'mar', 'mnar'\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Inject missing values.\"\"\"\n",
    "        X_transformed = X.copy()\n",
    "        n_values = int(X.size * self.missing_fraction)\n",
    "        \n",
    "        if self.pattern == 'random':\n",
    "            # Randomly select values to make NaN\n",
    "            rows = np.random.choice(len(X), n_values)\n",
    "            cols = np.random.choice(X.columns, n_values)\n",
    "            \n",
    "            for row, col in zip(rows, cols):\n",
    "                X_transformed.loc[row, col] = np.nan\n",
    "        \n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "print(\"‚úÖ Custom transformers created:\")\n",
    "print(\"\\n1. BusinessHoursTransformer\")\n",
    "print(\"   ‚Ä¢ Simulates temporal patterns\")\n",
    "print(\"   ‚Ä¢ Tests model on time-varying data\")\n",
    "print(\"\\n2. OutlierInjectionTransformer\")\n",
    "print(\"   ‚Ä¢ Injects statistical outliers\")\n",
    "print(\"   ‚Ä¢ Tests robustness to anomalies\")\n",
    "print(\"\\n3. MissingDataTransformer\")\n",
    "print(\"   ‚Ä¢ Simulates missing values\")\n",
    "print(\"   ‚Ä¢ Tests handling of incomplete data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "use_transformers",
   "metadata": {},
   "source": [
    "### Apply Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_transformers",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß™ Testing Custom Transformers\\n\")\n",
    "\n",
    "# Original performance\n",
    "y_pred_original = model.predict(dataset.X_test)\n",
    "acc_original = accuracy_score(dataset.y_test, y_pred_original)\n",
    "\n",
    "# Apply transformers\n",
    "transformers = [\n",
    "    ('Business Hours', BusinessHoursTransformer(feature_cols=['feature_0', 'feature_1'])),\n",
    "    ('Outlier Injection', OutlierInjectionTransformer(outlier_fraction=0.05)),\n",
    "    ('Missing Data', MissingDataTransformer(missing_fraction=0.1))\n",
    "]\n",
    "\n",
    "results = [('Original', acc_original)]\n",
    "\n",
    "for name, transformer in transformers:\n",
    "    X_transformed = transformer.transform(dataset.X_test)\n",
    "    \n",
    "    # Handle missing values if any\n",
    "    if X_transformed.isna().any().any():\n",
    "        X_transformed = X_transformed.fillna(X_transformed.mean())\n",
    "    \n",
    "    # Test model on transformed data\n",
    "    y_pred = model.predict(X_transformed)\n",
    "    acc = accuracy_score(dataset.y_test, y_pred)\n",
    "    results.append((name, acc))\n",
    "\n",
    "# Display results\n",
    "print(\"üìä Model Performance Under Different Transformations:\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, acc in results:\n",
    "    degradation = ((acc_original - acc) / acc_original * 100) if name != 'Original' else 0\n",
    "    status = '‚úÖ' if degradation < 5 else '‚ö†Ô∏è' if degradation < 10 else '‚ùå'\n",
    "    \n",
    "    print(f\"{status} {name:20s}: {acc:.4f} ({degradation:+.1f}% degradation)\")\n",
    "\n",
    "print(\"\\nüí° Transformers help test model resilience to real-world conditions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practices",
   "metadata": {},
   "source": [
    "<a id=\"practices\"></a>\n",
    "## 8. ‚ú® Best Practices\n",
    "\n",
    "### Extension Development Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best_practices_content",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f5e9; padding: 15px; border-radius: 5px; border-left: 5px solid #4CAF50;\">\n",
    "<b>‚úÖ DO</b><br><br>\n",
    "\n",
    "1. **Follow Standard Interfaces**\n",
    "   ```python\n",
    "   # ‚úÖ GOOD: Consistent with sklearn API\n",
    "   class MyTransformer:\n",
    "       def fit(self, X, y=None):\n",
    "           return self\n",
    "       \n",
    "       def transform(self, X):\n",
    "           return X_transformed\n",
    "   ```\n",
    "\n",
    "2. **Document Thoroughly**\n",
    "   - Clear docstrings\n",
    "   - Usage examples\n",
    "   - Parameter descriptions\n",
    "   - Expected inputs/outputs\n",
    "\n",
    "3. **Include Tests**\n",
    "   ```python\n",
    "   # Always test your extensions\n",
    "   def test_custom_metric():\n",
    "       y_true = [0, 1, 1, 0]\n",
    "       y_pred = [0, 1, 0, 0]\n",
    "       score = my_custom_metric(y_true, y_pred)\n",
    "       assert 0 <= score <= 1\n",
    "   ```\n",
    "\n",
    "4. **Handle Edge Cases**\n",
    "   - Empty inputs\n",
    "   - Single class\n",
    "   - Missing values\n",
    "   - Invalid parameters\n",
    "\n",
    "5. **Make It Configurable**\n",
    "   ```python\n",
    "   # ‚úÖ GOOD: Configurable behavior\n",
    "   class MyTest:\n",
    "       def __init__(self, threshold=0.8, mode='strict'):\n",
    "           self.threshold = threshold\n",
    "           self.mode = mode\n",
    "   ```\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #ffebee; padding: 15px; border-radius: 5px; border-left: 5px solid #f44336; margin-top: 15px;\">\n",
    "<b>‚ùå DON'T</b><br><br>\n",
    "\n",
    "1. **Modify Core Classes**\n",
    "   ```python\n",
    "   # ‚ùå BAD: Modifying DeepBridge internals\n",
    "   from deepbridge import Experiment\n",
    "   Experiment.some_method = my_custom_method\n",
    "   ```\n",
    "   ```python\n",
    "   # ‚úÖ GOOD: Extend, don't modify\n",
    "   class MyExperiment(Experiment):\n",
    "       def custom_method(self):\n",
    "           pass\n",
    "   ```\n",
    "\n",
    "2. **Ignore Performance**\n",
    "   - Profile your extensions\n",
    "   - Optimize hot paths\n",
    "   - Consider memory usage\n",
    "\n",
    "3. **Hardcode Values**\n",
    "   ```python\n",
    "   # ‚ùå BAD: Hardcoded\n",
    "   threshold = 0.75\n",
    "   ```\n",
    "   ```python\n",
    "   # ‚úÖ GOOD: Configurable\n",
    "   def __init__(self, threshold=0.75):\n",
    "       self.threshold = threshold\n",
    "   ```\n",
    "\n",
    "4. **Assume Data Format**\n",
    "   - Validate inputs\n",
    "   - Handle different dtypes\n",
    "   - Support various shapes\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## 9. üéì Conclusion\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "- ‚úÖ **Custom tests** - Build domain-specific validation tests\n",
    "- ‚úÖ **Custom metrics** - Create business-relevant metrics\n",
    "- ‚úÖ **Custom transformers** - Test resilience to perturbations\n",
    "- ‚úÖ **Extension patterns** - Follow best practices\n",
    "- ‚úÖ **Integration** - Connect with existing workflows\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. üîå **Extend, don't modify** - Use inheritance and composition\n",
    "2. üìã **Follow interfaces** - Consistency with sklearn/DeepBridge APIs\n",
    "3. üìù **Document everything** - Your future self will thank you\n",
    "4. üß™ **Test thoroughly** - Extensions need tests too\n",
    "5. ‚öôÔ∏è **Make configurable** - Flexibility is key\n",
    "6. üéØ **Solve real problems** - Build what you actually need\n",
    "\n",
    "### Extension Use Cases by Industry\n",
    "\n",
    "| Industry | Custom Extensions | Example |\n",
    "|----------|------------------|----------|\n",
    "| **Healthcare** | Safety-critical tests | Max false negative rate test |\n",
    "| **Finance** | Economic metrics | Profit-weighted accuracy |\n",
    "| **E-commerce** | Business logic tests | Price range validation |\n",
    "| **Manufacturing** | Quality assurance | Defect rate constraints |\n",
    "| **Insurance** | Regulatory compliance | Actuarial fairness tests |\n",
    "\n",
    "### Building Your Extension Library\n",
    "\n",
    "```python\n",
    "# 1. Start simple\n",
    "class MyFirstTest(CustomValidationTest):\n",
    "    def run(self, dataset):\n",
    "        # Your validation logic\n",
    "        return results\n",
    "\n",
    "# 2. Test it\n",
    "test = MyFirstTest()\n",
    "results = test.run(dataset)\n",
    "\n",
    "# 3. Package for reuse\n",
    "# my_validations/\n",
    "#   __init__.py\n",
    "#   tests.py\n",
    "#   metrics.py\n",
    "#   transformers.py\n",
    "\n",
    "# 4. Share with team\n",
    "from my_validations import MyFirstTest\n",
    "```\n",
    "\n",
    "### Real-World Extension Example\n",
    "\n",
    "**Medical AI Safety Suite:**\n",
    "```python\n",
    "from deepbridge import Experiment\n",
    "from medical_ai_validation import (\n",
    "    ClinicalSafetyTest,        # Max FN rate\n",
    "    CalibrationTest,            # Probability calibration\n",
    "    SubgroupFairnessTest,      # Performance across demographics\n",
    "    RegulatoryComplianceTest   # FDA requirements\n",
    ")\n",
    "\n",
    "# Run complete medical validation suite\n",
    "exp = Experiment(dataset, experiment_type='binary_classification')\n",
    "\n",
    "# Standard tests\n",
    "exp.run_test('robustness')\n",
    "exp.run_test('uncertainty')\n",
    "\n",
    "# Custom medical tests\n",
    "clinical_safety = ClinicalSafetyTest(max_fnr=0.05)\n",
    "clinical_safety.run(exp.dataset)\n",
    "\n",
    "calibration = CalibrationTest()\n",
    "calibration.run(exp.dataset)\n",
    "\n",
    "# Generate FDA-compliant report\n",
    "exp.generate_report(\n",
    "    'fda_submission_report.pdf',\n",
    "    include_custom_tests=[clinical_safety, calibration]\n",
    ")\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Identify your needs** - What validations does your domain require?\n",
    "2. **Build incrementally** - Start with one custom test\n",
    "3. **Test extensively** - Ensure reliability\n",
    "4. **Share and iterate** - Get feedback from team\n",
    "5. **Contribute back** - Open source useful extensions\n",
    "\n",
    "### Resources\n",
    "\n",
    "- üìö **DeepBridge Extension Guide**: docs/extending-deepbridge.md\n",
    "- üîå **Example Extensions**: examples/custom_extensions/\n",
    "- üí¨ **Community Forum**: discuss.deepbridge.ai\n",
    "- üêõ **Report Issues**: github.com/deepbridge/deepbridge/issues\n",
    "\n",
    "---\n",
    "\n",
    "**Remember: The best validation framework is the one you can extend to meet your needs!** üîå‚ú®\n",
    "\n",
    "**You've completed all 27 DeepBridge notebooks! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
