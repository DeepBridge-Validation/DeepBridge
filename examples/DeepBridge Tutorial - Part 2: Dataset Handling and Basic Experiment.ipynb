{
  "cells": [
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "# DeepBridge Tutorial - Part 2: AutoDistiller and Robustness Testing\n",
     "\n",
     "This notebook continues our exploration of the DeepBridge library, focusing on advanced usage including:\n",
     "\n",
     "1. Using the AutoDistiller for automated model distillation\n",
     "2. Robustness testing and validation\n",
     "3. Feature importance analysis\n",
     "\n",
     "Let's begin by importing the necessary components."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "source": [
     "# Import core DeepBridge components\n",
     "from deepbridge.core.db_data import DBDataset\n",
     "from deepbridge.core.experiment import Experiment\n",
     "from deepbridge.distillation.auto_distiller import AutoDistiller\n",
     "from deepbridge.utils.model_registry import ModelType\n",
     "from deepbridge.validation.robustness_test import RobustnessTest\n",
     "from deepbridge.validation.robustness_metrics import RobustnessScore\n",
     "from deepbridge.visualization.robustness_viz import RobustnessViz\n",
     "\n",
     "# Additional imports\n",
     "import pandas as pd\n",
     "import numpy as np\n",
     "from sklearn.datasets import load_breast_cancer\n",
     "from sklearn.ensemble import RandomForestClassifier\n",
     "from sklearn.model_selection import train_test_split\n",
     "import matplotlib.pyplot as plt\n",
     "import seaborn as sns\n",
     "import tempfile\n",
     "import os\n",
     "import joblib\n",
     "\n",
     "# Set random seed for reproducibility\n",
     "np.random.seed(42)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 1. Preparing a Dataset with Model Predictions\n",
     "\n",
     "For advanced distillation, we'll start with a more realistic dataset and create a complex teacher model."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "source": [
     "# Load breast cancer dataset\n",
     "data = load_breast_cancer()\n",
     "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
     "y = pd.Series(data.target, name='target')\n",
     "\n",
     "# Split into train and test sets\n",
     "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
     "\n",
     "# Create a complex teacher model (Random Forest with many trees)\n",
     "teacher_model = RandomForestClassifier(\n",
     "    n_estimators=200,\n",
     "    max_depth=20,\n",
     "    min_samples_split=2,\n",
     "    min_samples_leaf=1,\n",
     "    random_state=42\n",
     ")\n",
     "\n",
     "# Train the model\n",
     "teacher_model.fit(X_train, y_train)\n",
     "\n",
     "# Generate predictions\n",
     "train_probs = teacher_model.predict_proba(X_train)\n",
     "test_probs = teacher_model.predict_proba(X_test)\n",
     "\n",
     "# Convert to DataFrames\n",
     "train_probs_df = pd.DataFrame(train_probs, columns=['prob_class_0', 'prob_class_1'], index=X_train.index)\n",
     "test_probs_df = pd.DataFrame(test_probs, columns=['prob_class_0', 'prob_class_1'], index=X_test.index)\n",
     "\n",
     "# Save the teacher model\n",
     "temp_dir = tempfile.mkdtemp()\n",
     "model_path = os.path.join(temp_dir, 'teacher_model.pkl')\n",
     "joblib.dump(teacher_model, model_path)\n",
     "\n",
     "# Combine features and target\n",
     "train_data = X_train.copy()\n",
     "train_data['target'] = y_train\n",
     "test_data = X_test.copy()\n",
     "test_data['target'] = y_test\n",
     "\n",
     "# Print model info\n",
     "print(f\"Teacher model: {type(teacher_model).__name__}\")\n",
     "print(f\"Number of trees: {teacher_model.n_estimators}\")\n",
     "print(f\"Train accuracy: {teacher_model.score(X_train, y_train):.4f}\")\n",
     "print(f\"Test accuracy: {teacher_model.score(X_test, y_test):.4f}\")\n",
     "print(f\"Model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB\")"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "source": [
     "# Create a DBDataset with the teacher model\n",
     "dataset = DBDataset(\n",
     "    train_data=train_data,\n",
     "    test_data=test_data,\n",
     "    target_column='target',\n",
     "    model_path=model_path,\n",
     "    train_predictions=train_probs_df,\n",
     "    test_predictions=test_probs_df,\n",
     "    prob_cols=['prob_class_0', 'prob_class_1'],\n",
     "    dataset_name='breast_cancer_with_teacher_model'\n",
     ")\n",
     "\n",
     "print(dataset)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 2. Using AutoDistiller for Automated Model Distillation\n",
     "\n",
     "The `AutoDistiller` class automates the process of finding the best distilled model by testing various combinations of:\n",
     "- Student model types\n",
     "- Temperatures for knowledge distillation\n",
     "- Alpha values (weighting between soft and hard targets)\n",
     "\n",
     "It performs hyperparameter optimization for each configuration and calculates detailed metrics."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "source": [
     "# Create an AutoDistiller instance\n",
     "auto_distiller = AutoDistiller(\n",
     "    dataset=dataset,\n",
     "    output_dir=os.path.join(temp_dir, 'distillation_results'),\n",
     "    test_size=0.2,  # This is used for validation during distillation\n",
     "    random_state=42,\n",
     "    n_trials=10,    # Number of hyperparameter optimization trials\n",
     "    verbose=True    # Show detailed output\n",
     ")\n",
     "\n",
     "# Customize the configuration\n",
     "auto_distiller.customize_config(\n",
     "    model_types=[ModelType.LOGISTIC_REGRESSION, ModelType.GBM, ModelType.DECISION_TREE],\n",
     "    temperatures=[1.0, 2.0],         # Temperature values to test\n",
     "    alphas=[0.3, 0.7],               # Alpha values to test\n",
     "    distillation_method=\"knowledge_distillation\"  # Method to use\n",
     ")\n",
     "\n",
     "# Calculate metrics for the original model\n",
     "original_metrics = auto_distiller.original_metrics()\n",
     "print(\"\\nOriginal Model Metrics (Test Set):\")\n",
     "for metric, value in original_metrics['test'].items():\n",
     "    if isinstance(value, (int, float)):\n",
     "        print(f\"  {metric}: {value:.4f}\")"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "source": [
     "# Run the automated distillation process\n",
     "# This will test all combinations of model types, temperatures, and alphas\n",
     "results = auto_distiller.run(use_probabilities=True, verbose_output=True)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### 2.1 Analyzing AutoDistiller Results\n",
     "\n",
     "Now, let's explore the results of our automated distillation process."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "source": [
     "# Display the results DataFrame\n",
     "print(\"Distillation Results:\")\n",
     "results.head()"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "source": [
     "# Find the best model based on test accuracy\n",
     "best_config = auto_distiller.find_best_model(metric='test_accuracy')\n",
     "print(\"Best Model Configuration (by Test Accuracy):\")\n",
     "for key, value in best_config.items():\n",
     "    if key in ['model_type', 'temperature', 'alpha']:\n",
     "        print(f\"  {key}: {value}\")\n",
     "        \n",
     "print(\"\\nTest Metrics:\")\n",
     "for key, value in best_config.items():\n",
     "    if key.startswith('test_') and isinstance(value, (int, float)):\n",
     "        print(f\"  {key}: {value:.4f}\")"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "source": [
     "# Find the best model for distribution matching (KL divergence)\n",
     "best_distribution = auto_distiller.find_best_model(metric='test_kl_divergence', minimize=True)\n",
     "print(\"Best Model Configuration (by KL Divergence):\")\n",
     "for key, value in best_distribution.items():\n",
     "    if key in ['model_type', 'temperature', 'alpha']:\n",
     "        print(f\"  {key}: {value}\")\n",
     "        \n",
     "print(\"\\nDistribution Metrics:\")\n",
     "for key, value in best_distribution.items():\n",
     "    if key in ['test_kl_divergence', 'test_ks_statistic', 'test_r2_score'] and isinstance(value, (int, float)):\n",
     "        print(f\"  {key}: {value:.4f}\")"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "source": [
     "# Compare the original model with the best distilled model\n",
     "comparison = auto_distiller.compare_models()\n",
     "print(\"Original vs. Best Distilled Model Comparison (Test Set):\")\n",
     "print(comparison[comparison['dataset'] == 'test'])"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "source": [
     "# Generate a summary report\n",
     "summary = auto_distiller.generate_summary()\n",
     "print(summary)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### 2.2 Using the Best Distilled Model\n",
     "\n",
     "Now that we've found the best distilled model, let's see how to use it for predictions."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "source": [
     "# Get the best model configuration\n",
     "best_model_config = auto_distiller.find_best_model(metric='test_accuracy')\n",
     "model_type = best_model_config['model_type']\n",
     "temperature = best_model_config['temperature']\n",
     "alpha = best_model_config['alpha']\n",
     "\n",
     "# Get the trained distilled model\n",
     "best_model = auto_distiller.get_trained_model(\n",
     "    model_type=model_type,\n",
     "    temperature=temperature,\n",
     "    alpha=alpha\n",
     ")\n",
     "\n",
     "# Make predictions with the distilled model\n",
     "distilled_probs = best_model.predict_proba(X_test)\n",
     "distilled_preds = (distilled_probs[:, 1] > 0.5).astype(int)\n",
     "\n",
     "# Make predictions with the teacher model for comparison\n",
     "teacher_probs = teacher_model.predict_proba(X_test)\n",
     "teacher_preds = (teacher_probs[:, 1] > 0.5).astype(int)\n",
     "\n",
     "# Compare accuracy\n",
     "distilled_accuracy = (distilled_preds == y_test).mean()\n",
     "teacher_accuracy = (teacher_preds == y_test).mean()\n",
     "\n",
     "print(f\"Teacher model accuracy: {teacher_accuracy:.4f}\")\n",
     "print(f\"Distilled model accuracy: {distilled_accuracy:.4f}\")\n",
     "print(f\"Accuracy retention: {distilled_accuracy/teacher_accuracy:.2%}\")"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "source": [
     "# Save the best model\n",
     "best_model_path = auto_distiller.save_best_model(\n",
     "    metric='test_accuracy',\n",
     "    file_path=os.path.join(temp_dir, 'best_distilled_model.pkl')\n",
     ")\n",
     "\n",
     "# Compare model sizes\n",
     "original_size = os.path.getsize(model_path) / (1024 * 1024)  # in MB\n",
     "distilled_size = os.path.getsize(best_model_path) / (1024 * 1024)  # in MB\n",
     "\n",
     "print(f\"Original model size: {original_size:.2f} MB\")\n",
     "print(f\"Distilled model size: {distilled_size:.2f} MB\")\n",
     "print(f\"Size reduction: {(1 - distilled_size/original_size):.2%}\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 3. Visualizing the Probability Distributions\n",
     "\n",
     "An important aspect of model distillation is how well the student model matches the probability distribution of the teacher model."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "source": [
     "# Create a visualization of the probability distributions\n",
     "plt.figure(figsize=(12, 6))\n",
     "\n",
     "# Get teacher and student probabilities for the positive class\n",
     "teacher_probs_pos = teacher_probs[:, 1]\n",
     "student_probs_pos = distilled_probs[:, 1]\n",
     "\n",
     "# Plot density curves\n",
     "sns.kdeplot(teacher_probs_pos, fill=True, color=\"royalblue\", alpha=0.5, \n",
     "           label=\"Teacher Model\", linewidth=2)\n",
     "sns.kdeplot(student_probs_pos, fill=True, color=\"crimson\", alpha=0.5, \n",
     "           label=\"Distilled Model\", linewidth=2)\n",
     "\n",
     "# Add histogram for additional clarity\n",
     "plt.hist(teacher_probs_pos, bins=20, density=True, alpha=0.3, color=\"blue\")\n",
     "plt.hist(student_probs_pos, bins=20, density=True, alpha=0.3, color=\"red\")\n",
     "\n",
     "# Add titles and labels\n",
     "plt.xlabel(\"Probability Value\", fontsize=12)\n",
     "plt.ylabel(\"Density\", fontsize=12)\n",
     "plt.title(\"Teacher vs Distilled Model Probability Distribution\", fontsize=14, fontweight='bold')\n",
     "\n",
     "# Add distribution similarity metrics\n",
     "from scipy import stats\n",
     "from sklearn.metrics import r2_score\n",
     "\n",
     "# Calculate KS statistic\n",
     "ks_stat, ks_pvalue = stats.ks_2samp(teacher_probs_pos, student_probs_pos)\n",
     "\n",
     "# Calculate R² by sorting both distributions\n",
     "teacher_sorted = np.sort(teacher_probs_pos)\n",
     "student_sorted = np.sort(student_probs_pos)\n",
     "min_len = min(len(teacher_sorted), len(student_sorted))\n",
     "r2 = r2_score(teacher_sorted[:min_len], student_sorted[:min_len])\n",
     "\n",
     "# Add metrics to the plot\n",
     "metrics_text = f\"KS Statistic: {ks_stat:.4f} (p={ks_pvalue:.4f})\\nR² Score: {r2:.4f}\"\n",
     "plt.annotate(metrics_text, xy=(0.02, 0.96), xycoords='axes fraction',\n",
     "            bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", alpha=0.8),\n",
     "            va='top', fontsize=10)\n",
     "\n",
     "plt.legend(loc='best')\n",
     "plt.grid(True, linestyle='--', alpha=0.7)\n",
     "plt.show()"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 4. Robustness Testing of Original and Distilled Models\n",
     "\n",
     "Robustness testing evaluates how well models perform when data is perturbed or corrupted. Let's test both our original teacher model and our distilled model."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "source": [
     "# Initialize the robustness test\n",
     "robustness_test = RobustnessTest()\n",
     "\n",
     "# Create a dictionary of models to test\n",
     "models = {\n",
     "    'Teacher (Random Forest)': teacher_model,\n",
     "    'Distilled': best_model\n",
     "}\n",
     "\n",
     "# Run robustness evaluation\n",
     "results = robustness_test.evaluate_robustness(\n",
     "    models=models,\n",
     "    X=X_test,\n",
     "    y=y_test,\n",
     "    perturb_method='raw',  # Use raw perturbation (add Gaussian noise)\n",
     "    perturb_sizes=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0],  # Different levels of perturbation\n",
     "    metric='AUC',  # Evaluation metric\n",
     "    n_iterations=5,  # Number of iterations per perturbation size\n",
     "    random_state=42\n",
     ")"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "source": [
     "# Calculate robustness indices\n",
     "robustness_indices = RobustnessScore.calculate_robustness_index(\n",
     "    results=results,\n",
     "    metric='AUC'\n",
     ")\n",
     "\n",
     "print(\"Robustness Indices:\")\n",
     "for model_name, index in robustness_indices.items():\n",
     "    print(f\"  {model_name}: {index:.4f}\")"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "source": [
     "# Create a visualization of model performance under perturbation\n",
     "fig = RobustnessViz.plot_models_comparison(\n",
     "    results=results,\n",
     "    metric_name='AUC-ROC',\n",
     "    height=500,\n",
     "    width=800\n",
     ")\n",
     "\n",
     "# Display the figure (note: normally this would be interactive with Plotly)\n",
     "from IPython.display import Image\n",
     "import plotly.io as pio\n",
     "\n",
     "# Convert Plotly figure to static image\n",
     "img_path = os.path.join(temp_dir, 'robustness_comparison.png')\n",
     "pio.write_image(fig, img_path)\n",
     "\n",
     "# Display the image\n",
     "Image(img_path)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 5. Feature Importance for Robustness\n",
     "\n",
     "Let's analyze which features impact model robustness the most."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "source": [
     "# Analyze feature importance for robustness\n",
     "feature_importance = robustness_test.analyze_feature_importance(\n",
     "    model=best_model,\n",
     "    X=X_test,\n",
     "    y=y_test,\n",
     "    perturb_method='raw',\n",
     "    perturb_size=0.5,\n",
     "    metric='AUC',\n",
     "    n_iterations=3,\n",
     "    random_state=42\n",
     ")\n",
     "\n",
     "# Display top 10 important features\n",
     "top_features = feature_importance['sorted_features'][:10]\n",
     "top_impacts = feature_importance['sorted_impacts'][:10]\n",
     "\n",
     "print(\"Top 10 Features by Impact on Robustness:\")\n",
     "for feature, impact in zip(top_features, top_impacts):\n",
     "    print(f\"  {feature}: {impact:.4f}\")"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "source": [
     "# Create a feature importance visualization\n",
     "fig = RobustnessViz.plot_feature_importance(\n",
     "    feature_importance_results=feature_importance,\n",
     "    title=\"Feature Impact on Model Robustness\",\n",
     "    top_n=10,\n",
     "    height=600,\n",
     "    width=800\n",
     ")\n",
     "\n",
     "# Convert Plotly figure to static image\n",
     "img_path = os.path.join(temp_dir, 'feature_importance.png')\n",
     "pio.write_image(fig, img_path)\n",
     "\n",
     "# Display the image\n",
     "Image(img_path)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## Conclusion\n",
     "\n",
     "In this second part of our DeepBridge tutorial, we've explored several advanced capabilities:\n",
     "\n",
     "1. **AutoDistiller**: The automated distillation process makes it easy to find the optimal student model by testing various combinations of model types, temperatures, and alpha values.\n",
     "\n",
     "2. **Robustness Testing**: We evaluated how well both teacher and student models perform under data perturbation, showing that distilled models can maintain similar robustness to the original model.\n",
     "\n",
     "3. **Feature Importance Analysis**: We identified which features have the most impact on model robustness, which can guide feature engineering and model development.\n",
     "\n",
     "4. **Visualization**: We created visualizations for probability distributions and performance under perturbation.\n",
     "\n",
     "Key takeaways:\n",
     "- Model distillation can significantly reduce model size while maintaining most of the performance\n",
     "- Robustness testing helps ensure that distilled models remain stable under perturbation\n",
     "- Feature importance analysis provides insights into which features are most critical for model stability\n",
     "\n",
     "In Part 3, we'll explore comparing multiple distillation methods and creating comprehensive dashboards."
    ]
   }
  ],
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.8.8"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 4
 }