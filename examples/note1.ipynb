{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise do Efeito da Temperatura na Destilação de Conhecimento - Parte 1\n",
    "\n",
    "Este notebook investiga como o parâmetro de temperatura afeta o processo de destilação de conhecimento usando a biblioteca DeepBridge. Exploraremos tanto os aspectos teóricos quanto práticos da temperatura na destilação, com visualizações e experimentos sistemáticos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introdução à Temperatura na Destilação\n",
    "\n",
    "### O que é a Temperatura na Destilação de Conhecimento?\n",
    "\n",
    "Na destilação de conhecimento, a \"temperatura\" (T) é um hiperparâmetro que controla a suavidade das distribuições de probabilidade produzidas pelo modelo professor antes de serem usadas para treinar o modelo aluno.\n",
    "\n",
    "Matematicamente, a temperatura é aplicada às logits (valores pré-softmax) do modelo professor da seguinte forma:\n",
    "\n",
    "$$q_i = \\frac{\\exp(z_i/T)}{\\sum_j \\exp(z_j/T)}$$\n",
    "\n",
    "Onde:\n",
    "- $z_i$ são as logits originais do modelo professor\n",
    "- $T$ é o parâmetro de temperatura\n",
    "- $q_i$ são as probabilidades suavizadas\n",
    "\n",
    "#### Importância da Temperatura\n",
    "\n",
    "- **T = 1**: Comportamento padrão (softmax normal)\n",
    "- **T > 1**: Produz distribuições mais suaves, revelando mais informação sobre as relações entre classes\n",
    "- **T < 1**: Produz distribuições mais acentuadas, aproximando-se de one-hot encoding\n",
    "\n",
    "O efeito da temperatura é crucial porque determina a quantidade de \"conhecimento escuro\" transferido do professor para o aluno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuração do Ambiente e Importação de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
    "from scipy.special import softmax\n",
    "import warnings\n",
    "\n",
    "# Importações da biblioteca DeepBridge\n",
    "from deepbridge.db_data import DBDataset\n",
    "from deepbridge.auto_distiller import AutoDistiller\n",
    "from deepbridge.distillation.classification.model_registry import ModelType\n",
    "from deepbridge.auto.config import DistillationConfig\n",
    "from deepbridge.visualizer.distribution_visualizer import DistributionVisualizer\n",
    "\n",
    "# Configurar o estilo das visualizações\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar o tamanho das figuras\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Efeito Visual da Temperatura nas Distribuições\n",
    "\n",
    "Para entender melhor o efeito da temperatura, vamos visualizar como diferentes valores de temperatura transformam as distribuições de probabilidade:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_temperature_effect(logits, temperatures=[0.5, 1.0, 2.0, 5.0, 10.0]):\n",
    "    \"\"\"Visualiza o efeito da temperatura nas distribuições de probabilidade.\"\"\"\n",
    "    fig, axes = plt.subplots(len(temperatures), 1, figsize=(12, 4*len(temperatures)))\n",
    "    \n",
    "    # Certifique-se de que axes seja sempre uma lista, mesmo com um único subplot\n",
    "    if len(temperatures) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, T in enumerate(temperatures):\n",
    "        # Aplicar temperatura nas logits\n",
    "        scaled_logits = logits / T\n",
    "        probs = softmax(scaled_logits, axis=1)\n",
    "        \n",
    "        # Plotar a distribuição de probabilidades\n",
    "        for j in range(min(5, probs.shape[0])):  # Limitar a 5 exemplos para clareza\n",
    "            axes[i].bar(range(probs.shape[1]), probs[j], alpha=0.7, \n",
    "                      label=f'Exemplo {j+1}' if i == 0 else \"\")\n",
    "            \n",
    "        axes[i].set_title(f'Temperatura T = {T}')\n",
    "        axes[i].set_xlabel('Classes')\n",
    "        axes[i].set_ylabel('Probabilidade')\n",
    "        axes[i].set_ylim(0, 1)\n",
    "        \n",
    "        # Calcular entropia para esta temperatura\n",
    "        entropy = -np.sum(probs * np.log(probs + 1e-10), axis=1).mean()\n",
    "        axes[i].text(0.02, 0.95, f'Entropia Média: {entropy:.4f}', \n",
    "                   transform=axes[i].transAxes, bbox=dict(facecolor='white', alpha=0.8))\n",
    "        \n",
    "    # Legenda apenas no primeiro gráfico\n",
    "    axes[0].legend(loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Gerar logits sintéticos para demonstração\n",
    "np.random.seed(42)\n",
    "synthetic_logits = np.random.randn(5, 10) * 2  # 5 exemplos, 10 classes\n",
    "\n",
    "# Tornar a classe 3 tipicamente a mais forte para demonstração\n",
    "synthetic_logits[:, 3] += 2\n",
    "\n",
    "# Visualizar o efeito\n",
    "visualize_temperature_effect(synthetic_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise do Efeito da Temperatura:\n",
    "\n",
    "Como podemos observar nos gráficos acima:\n",
    "\n",
    "1. **T = 0.5**: Com temperatura baixa, as probabilidades se concentram fortemente na classe dominante, aproximando-se de uma codificação one-hot. Isso resulta em menor entropia.\n",
    "\n",
    "2. **T = 1.0**: Representa o comportamento padrão do softmax, onde a distribuição já mostra alguma incerteza entre classes.\n",
    "\n",
    "3. **T = 2.0**: A distribuição se torna mais suave, revelando mais informação sobre as relações entre classes. A entropia aumenta.\n",
    "\n",
    "4. **T = 5.0** e **T = 10.0**: Com temperaturas muito altas, as distribuições se aproximam de uma distribuição uniforme, onde as diferenças sutis entre classes são amplificadas. A entropia continua aumentando.\n",
    "\n",
    "Este efeito de suavização é essencial para a destilação porque:\n",
    "- Revela o \"conhecimento escuro\" do modelo professor\n",
    "- Expõe relações entre classes que não são evidentes nas previsões binárias\n",
    "- Permite que o modelo aluno aprenda padrões mais sutis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparação dos Dados para Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um conjunto de dados sintético para experimentação\n",
    "X, y = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Dividir em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Treinar um modelo professor (Random Forest)\n",
    "teacher_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "teacher_model.fit(X_train, y_train)\n",
    "\n",
    "# Gerar probabilidades do modelo professor\n",
    "train_probs = teacher_model.predict_proba(X_train)\n",
    "test_probs = teacher_model.predict_proba(X_test)\n",
    "\n",
    "# Criar DataFrames para as probabilidades\n",
    "train_prob_df = pd.DataFrame(train_probs, columns=['prob_class_0', 'prob_class_1'])\n",
    "test_prob_df = pd.DataFrame(test_probs, columns=['prob_class_0', 'prob_class_1'])\n",
    "\n",
    "# Criar DataFrames para os dados de entrada\n",
    "train_df = pd.DataFrame(X_train, columns=[f'feature_{i}' for i in range(X_train.shape[1])])\n",
    "train_df['target'] = y_train\n",
    "train_df = pd.concat([train_df, train_prob_df], axis=1)\n",
    "\n",
    "test_df = pd.DataFrame(X_test, columns=[f'feature_{i}' for i in range(X_test.shape[1])])\n",
    "test_df['target'] = y_test\n",
    "test_df = pd.concat([test_df, test_prob_df], axis=1)\n",
    "\n",
    "# Criar um DBDataset\n",
    "dataset = DBDataset(\n",
    "    train_data=train_df,\n",
    "    test_data=test_df,\n",
    "    target_column='target',\n",
    "    prob_cols=['prob_class_0', 'prob_class_1']\n",
    ")\n",
    "\n",
    "print(f\"Conjunto de dados criado: {len(train_df)} amostras de treino, {len(test_df)} amostras de teste\")\n",
    "print(f\"Performance do modelo professor no conjunto de teste:\")\n",
    "print(f\"  Acurácia: {accuracy_score(y_test, teacher_model.predict(X_test)):.4f}\")\n",
    "print(f\"  AUC-ROC: {roc_auc_score(y_test, test_probs[:, 1]):.4f}\")\n",
    "print(f\"  Log Loss: {log_loss(y_test, test_probs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experimentos com Diferentes Temperaturas\n",
    "\n",
    "Agora, vamos explorar como diferentes valores de temperatura afetam o desempenho dos modelos destilados. Executaremos uma série de experimentos sistemáticos com diferentes combinações de modelos, temperaturas e valores de alpha (peso entre o erro de destilação e o erro de classificação)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar experimentação sistemática com diferentes temperaturas\n",
    "temperatures = [0.5, 1.0, 2.0, 3.0, 5.0, 10.0]\n",
    "alphas = [0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "# Modelos a serem testados\n",
    "model_types = [\n",
    "    ModelType.LOGISTIC_REGRESSION,\n",
    "    ModelType.DECISION_TREE,\n",
    "    ModelType.GBM,\n",
    "]\n",
    "\n",
    "# Diretório para salvar resultados\n",
    "output_dir = \"temperatura_destilacao_resultados\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Configuração do AutoDistiller\n",
    "config = DistillationConfig(\n",
    "    output_dir=output_dir,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    n_trials=10,  # Reduzido para economizar tempo\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Personalizar a configuração\n",
    "config.customize(\n",
    "    model_types=model_types,\n",
    "    temperatures=temperatures,\n",
    "    alphas=alphas\n",
    ")\n",
    "\n",
    "# Criar o AutoDistiller\n",
    "distiller = AutoDistiller(\n",
    "    dataset=dataset,\n",
    "    output_dir=output_dir,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Personalizar a configuração\n",
    "distiller.customize_config(\n",
    "    model_types=model_types,\n",
    "    temperatures=temperatures,\n",
    "    alphas=alphas\n",
    ")\n",
    "\n",
    "# Executar os experimentos\n",
    "print(\"Executando experimentos de destilação com diferentes temperaturas...\")\n",
    "results_df = distiller.run(verbose_output=False)\n",
    "\n",
    "# Salvar resultados em CSV para análise posterior\n",
    "results_df.to_csv(os.path.join(output_dir, \"temperatura_resultados.csv\"), index=False)\n",
    "print(f\"Experimentos concluídos. Resultados salvos em {output_dir}/temperatura_resultados.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}