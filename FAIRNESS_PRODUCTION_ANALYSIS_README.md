# AnÃ¡lise de Fairness - Dados de ProduÃ§Ã£o

Este documento explica como executar a anÃ¡lise de fairness nos seus dados de produÃ§Ã£o.

## ğŸ“Š Sobre os Dados

**Arquivos**:
- `train_predictions.parquet` - Dados de treino com prediÃ§Ãµes
- `test_predictions.parquet` - Dados de teste com prediÃ§Ãµes

**Colunas**:
- **Target**: `in_cmst_fun` (0 ou 1)
- **Probabilidades**: `pred_proba_class_0`, `pred_proba_class_1`
- **Classe Predita**: `pred_class` (usando threshold customizado)
- **Atributos Protegidos**:
  - `nm_tip_gnr` (gÃªnero)
  - `nm_tip_raca` (raÃ§a)
  - `vl_idd_aa` (idade em anos)

---

## ğŸš€ OpÃ§Ã£o 1: AnÃ¡lise Completa (Recomendado)

### Script: `analyze_fairness_production.py`

AnÃ¡lise detalhada com todas as mÃ©tricas, visualizaÃ§Ãµes e relatÃ³rios.

**CaracterÃ­sticas**:
- âœ… 15 mÃ©tricas de fairness
- âœ… Threshold analysis (99 pontos)
- âœ… RelatÃ³rios HTML interativos
- âœ… VisualizaÃ§Ãµes estÃ¡ticas (PNG)
- âœ… AnÃ¡lise exploratÃ³ria completa
- â±ï¸ Tempo: ~10-15 minutos

### Como Executar

```bash
# Executar anÃ¡lise completa
python analyze_fairness_production.py
```

### Output

```
fairness_production_analysis/
â”œâ”€â”€ fairness_report_quick.html      # RelatÃ³rio anÃ¡lise rÃ¡pida
â”œâ”€â”€ fairness_report_full.html       # RelatÃ³rio anÃ¡lise completa
â”œâ”€â”€ distribution_nm_tip_gnr.png     # DistribuiÃ§Ã£o por gÃªnero
â”œâ”€â”€ distribution_nm_tip_raca.png    # DistribuiÃ§Ã£o por raÃ§a
â”œâ”€â”€ distribution_age_group.png      # DistribuiÃ§Ã£o por idade
â”œâ”€â”€ metrics_comparison.png          # ComparaÃ§Ã£o de mÃ©tricas
â””â”€â”€ fairness_radar.png              # Radar chart de fairness
```

---

## âš¡ OpÃ§Ã£o 2: AnÃ¡lise RÃ¡pida

### Script: `analyze_fairness_quick.py`

AnÃ¡lise simplificada focada nos resultados principais.

**CaracterÃ­sticas**:
- âœ… MÃ©tricas principais (Disparate Impact, Statistical Parity, Equal Opportunity)
- âœ… RelatÃ³rio HTML
- âœ… RecomendaÃ§Ãµes diretas
- â±ï¸ Tempo: ~5-8 minutos

### Como Executar

```bash
# Executar anÃ¡lise rÃ¡pida
python analyze_fairness_quick.py
```

### Output

```
fairness_quick_analysis/
â””â”€â”€ fairness_report.html            # RelatÃ³rio completo
```

---

## ğŸ“– InterpretaÃ§Ã£o dos Resultados

### Overall Fairness Score

| Score | InterpretaÃ§Ã£o | AÃ§Ã£o |
|-------|--------------|------|
| **0.90 - 1.00** | âœ… Excelente | Modelo aprovado para produÃ§Ã£o |
| **0.80 - 0.89** | âœ“ Boa | Revisar warnings, considerar melhorias |
| **0.70 - 0.79** | âš ï¸ Moderada | Melhorias recomendadas antes de deploy |
| **< 0.70** | âŒ CrÃ­tica | NÃƒO recomendado para produÃ§Ã£o |

### MÃ©tricas Principais

#### 1. Disparate Impact (EEOC 80% Rule)

**O que mede**: RazÃ£o entre taxa de aprovaÃ§Ã£o do grupo desfavorecido vs. favorecido

**InterpretaÃ§Ã£o**:
- âœ… **â‰¥ 0.80**: EEOC compliant (OK)
- âš ï¸ **0.70-0.79**: Zona de atenÃ§Ã£o
- âŒ **< 0.70**: ViolaÃ§Ã£o EEOC

**Exemplo**:
```
nm_tip_gnr: 0.75 (âœ— VIOLADO)
  â†’ Mulheres tÃªm 75% da taxa de aprovaÃ§Ã£o dos homens
  â†’ Abaixo do limite EEOC de 80%
```

#### 2. Statistical Parity

**O que mede**: DiferenÃ§a absoluta entre taxas de aprovaÃ§Ã£o

**InterpretaÃ§Ã£o**:
- âœ… **|valor| < 0.10**: OK (diferenÃ§a < 10%)
- âš ï¸ **0.10 â‰¤ |valor| < 0.20**: AtenÃ§Ã£o
- âŒ **|valor| â‰¥ 0.20**: CrÃ­tico

**Exemplo**:
```
nm_tip_raca: -0.18 (âš ï¸ ATENÃ‡ÃƒO)
  â†’ Grupo desfavorecido tem 18% menos aprovaÃ§Ãµes
```

#### 3. Equal Opportunity

**O que mede**: DiferenÃ§a na taxa de verdadeiros positivos (entre quem DEVERIA ser aprovado)

**InterpretaÃ§Ã£o**:
- âœ… **|valor| < 0.10**: OK
- âš ï¸ **0.10 â‰¤ |valor| < 0.15**: AtenÃ§Ã£o
- âŒ **|valor| â‰¥ 0.15**: CrÃ­tico

**Exemplo**:
```
nm_tip_gnr: 0.12 (âš ï¸ ATENÃ‡ÃƒO)
  â†’ Entre qualificados, homens tÃªm 12% mais chance de aprovaÃ§Ã£o
```

---

## ğŸ”§ CustomizaÃ§Ã£o

### Ajustar Atributos Protegidos

Edite as variÃ¡veis no inÃ­cio dos scripts:

```python
# Em analyze_fairness_production.py ou analyze_fairness_quick.py

# Exemplo 1: Apenas gÃªnero
PROTECTED_ATTRIBUTES = ['nm_tip_gnr']

# Exemplo 2: GÃªnero e raÃ§a
PROTECTED_ATTRIBUTES = ['nm_tip_gnr', 'nm_tip_raca']

# Exemplo 3: Todos (incluindo idade)
PROTECTED_ATTRIBUTES = ['nm_tip_gnr', 'nm_tip_raca', 'age_group']
```

### Ajustar Grupos EtÃ¡rios

Modifique os bins no script:

```python
# Grupos etÃ¡rios customizados
df['age_group'] = pd.cut(
    df['vl_idd_aa'],
    bins=[0, 25, 35, 45, 55, 100],      # Customize aqui
    labels=['<25', '25-34', '35-44', '45-54', '55+'],  # E aqui
    include_lowest=True
)
```

### Mudar ConfiguraÃ§Ã£o da AnÃ¡lise

```python
# AnÃ¡lise rÃ¡pida (2 mÃ©tricas, ~30 segundos)
result = experiment.run_fairness_tests(config='quick')

# AnÃ¡lise mÃ©dia (5 mÃ©tricas + prÃ©-treino, ~2 minutos)
result = experiment.run_fairness_tests(config='medium')

# AnÃ¡lise completa (15 mÃ©tricas + threshold, ~10 minutos)
result = experiment.run_fairness_tests(config='full')
```

---

## ğŸ› Troubleshooting

### Erro: "Colunas faltando"

**Problema**: Script nÃ£o encontra as colunas esperadas

**SoluÃ§Ã£o**:
1. Verificar nomes das colunas no seu DataFrame:
   ```python
   df = pd.read_parquet("test_predictions.parquet")
   print(df.columns.tolist())
   ```

2. Ajustar variÃ¡veis no script:
   ```python
   TARGET_COL = 'sua_coluna_target'
   PROBA_COLS = ['sua_prob_0', 'sua_prob_1']
   PRED_COL = 'sua_pred_class'
   ```

### Erro: "Feature names mismatch"

**Problema**: Modelo espera features diferentes

**SoluÃ§Ã£o**: O wrapper `PrecomputedPredictionsModel` usa as prediÃ§Ãµes jÃ¡ existentes, nÃ£o re-prediz. NÃ£o deve dar este erro.

### AnÃ¡lise muito lenta

**Problema**: AnÃ¡lise demora muito (> 20 minutos)

**SoluÃ§Ãµes**:

1. **Usar amostragem**:
   ```python
   # Adicionar antes de criar dataset
   df_sample = df.sample(n=10000, random_state=42)
   dataset = DBDataset(data=df_sample, ...)
   ```

2. **Usar config mais leve**:
   ```python
   result = experiment.run_fairness_tests(config='medium')
   ```

3. **Analisar um atributo por vez**:
   ```python
   for attr in ['nm_tip_gnr', 'nm_tip_raca']:
       experiment = Experiment(..., protected_attributes=[attr])
       result = experiment.run_fairness_tests(config='full')
       result.save_html(f'report_{attr}.html')
   ```

### Valores ausentes (NaN)

**Problema**: Dados tÃªm NaN em colunas crÃ­ticas

**SoluÃ§Ã£o**: O script jÃ¡ remove automaticamente linhas com NaN. Para ver o impacto:
```python
# Antes de criar dataset, verificar
print(f"NaN em target: {df[TARGET_COL].isna().sum()}")
print(f"NaN em gÃªnero: {df['nm_tip_gnr'].isna().sum()}")

# Remover ou imputar conforme necessÃ¡rio
df = df.dropna(subset=['nm_tip_gnr', 'nm_tip_raca'])
```

---

## ğŸ“Š Exemplos de Uso

### Exemplo 1: AnÃ¡lise BÃ¡sica

```bash
# Executar anÃ¡lise completa
python analyze_fairness_production.py

# Abrir relatÃ³rio
# file://./fairness_production_analysis/fairness_report_full.html
```

### Exemplo 2: AnÃ¡lise por Atributo

Criar script customizado:

```python
# analyze_by_attribute.py
import pandas as pd
from analyze_fairness_production import *

for attr in ['nm_tip_gnr', 'nm_tip_raca', 'age_group']:
    print(f"\n{'='*80}")
    print(f"Analisando: {attr}")
    print(f"{'='*80}")

    experiment = Experiment(
        dataset=dataset,
        experiment_type="binary_classification",
        tests=["fairness"],
        protected_attributes=[attr],  # Um por vez
        test_size=0.2,
        random_state=42
    )

    result = experiment.run_fairness_tests(config='full')

    # Salvar relatÃ³rio individual
    result.save_html(f'fairness_report_{attr}.html', model_name=f'Analysis - {attr}')

    print(f"Score: {result.overall_fairness_score:.3f}")
    print(f"Critical: {len(result.critical_issues)}")
```

### Exemplo 3: Comparar Train vs. Test

```python
# compare_train_test.py
import pandas as pd

for dataset_name, file_path in [('train', TRAIN_PATH), ('test', TEST_PATH)]:
    df = pd.read_parquet(file_path)

    # Criar modelo e dataset
    model = PrecomputedPredictionsModel(df, PROBA_COLS)
    dataset = DBDataset(data=df, target_column=TARGET_COL, model=model)

    # AnÃ¡lise
    experiment = Experiment(...)
    result = experiment.run_fairness_tests(config='full')

    # Salvar
    result.save_html(f'fairness_report_{dataset_name}.html')

    print(f"{dataset_name}: {result.overall_fairness_score:.3f}")
```

---

## ğŸ’¡ PrÃ³ximos Passos

### Se Score â‰¥ 0.80 (Aprovado)

1. âœ… Revisar relatÃ³rio HTML detalhado
2. âœ… Validar com stakeholders (legal, Ã©tico, negÃ³cio)
3. âœ… Documentar resultados para auditoria
4. âœ… Implementar monitoramento contÃ­nuo
5. âœ… Deploy em produÃ§Ã£o

### Se Score < 0.80 (Melhorias NecessÃ¡rias)

1. âš ï¸ Revisar critical issues e warnings no relatÃ³rio
2. âš ï¸ Identificar fontes de viÃ©s nos dados
3. âš ï¸ Aplicar tÃ©cnicas de mitigaÃ§Ã£o:

   **OpÃ§Ã£o A: Re-balanceamento de Dados**
   ```python
   # Re-balancear por grupo antes de treinar
   from sklearn.utils import resample
   # ... cÃ³digo de re-balanceamento
   ```

   **OpÃ§Ã£o B: Threshold Adjustment**
   ```python
   # Usar threshold Ã³timo da anÃ¡lise
   optimal_threshold = results['threshold_analysis']['optimal_threshold']
   # Retreinar modelo com threshold customizado
   ```

   **OpÃ§Ã£o C: Fairness Constraints**
   ```python
   # Usar Fairlearn ou AIF360
   from fairlearn.reductions import ExponentiatedGradient, DemographicParity
   mitigator = ExponentiatedGradient(estimator=model, constraints=DemographicParity())
   ```

4. âš ï¸ Re-treinar modelo
5. âš ï¸ Re-executar anÃ¡lise de fairness
6. âš ï¸ Repetir atÃ© Score â‰¥ 0.80

---

## ğŸ“š Recursos Adicionais

### DocumentaÃ§Ã£o DeepBridge

- [Tutorial Completo](docs/FAIRNESS_TUTORIAL.md)
- [Guia de Boas PrÃ¡ticas](docs/FAIRNESS_BEST_PRACTICES.md)
- [FAQ](docs/FAIRNESS_FAQ.md)

### Bibliotecas Complementares

- **AIF360** (IBM): https://github.com/Trusted-AI/AIF360
- **Fairlearn** (Microsoft): https://fairlearn.org/
- **What-If Tool** (Google): https://pair-code.github.io/what-if-tool/

### RegulamentaÃ§Ãµes

- **EEOC 80% Rule** (EUA): https://www.eeoc.gov/
- **GDPR** (Europa): https://gdpr.eu/
- **LGPD** (Brasil): https://www.gov.br/cidadania/pt-br/acesso-a-informacao/lgpd

---

## â“ DÃºvidas?

Para questÃµes sobre:
- **Uso dos scripts**: Consultar este README
- **InterpretaÃ§Ã£o de mÃ©tricas**: Consultar `docs/FAIRNESS_FAQ.md`
- **Boas prÃ¡ticas**: Consultar `docs/FAIRNESS_BEST_PRACTICES.md`
- **Issues tÃ©cnicos**: Abrir issue no GitHub

---

**VersÃ£o**: 1.0
**Data**: 2025-11-03
**Autor**: DeepBridge Team
