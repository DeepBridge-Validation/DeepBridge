# An√°lise Profunda e Otimiza√ß√£o dos Testes do DeepBridge

**Data da An√°lise**: 30 de Outubro de 2025
**Log Analisado**: `individual_tests_execution_20251030_104004.log`
**Tempo Total de Execu√ß√£o**: 512.77s (8.55 minutos)

---

## üìä Sum√°rio Executivo

Os testes individuais do DeepBridge est√£o levando **8.55 minutos** para executar com apenas **10% dos dados (1000 amostras)**. Os principais gargalos identificados s√£o:

1. **Cria√ß√£o de Experimento de Resili√™ncia**: 202.75s (39.5% do tempo total) ‚ö†Ô∏è **CR√çTICO**
2. **Execu√ß√£o do Teste de Incerteza**: 216.40s (42.2% do tempo total) ‚ö†Ô∏è **CR√çTICO**
3. **Cria√ß√£o de Experimento de Robustez**: 44.23s (8.6% do tempo total)
4. **Cria√ß√£o de Experimento de Incerteza**: 38.82s (7.6% do tempo total)

**Juntos, esses dois gargalos cr√≠ticos representam 81.7% do tempo total de execu√ß√£o.**

---

## üîç An√°lise Detalhada dos Gargalos

### 1. GARGALO CR√çTICO #1: Cria√ß√£o do Experimento de Resili√™ncia (202.75s)

#### An√°lise do C√≥digo

**Arquivo**: `deepbridge/validation/wrappers/resilience_suite.py`

**Problema Identificado** (linhas 29-140):
```python
def _get_config_templates(self):
    central_configs = {
        config_name: get_test_config(TestType.RESILIENCE.value, config_name)
        for config_name in [ConfigName.QUICK.value, ConfigName.MEDIUM.value, ConfigName.FULL.value]
    }

    # Transform the format to match what the resilience suite expects
    test_configs = {}
    for config_name, config in central_configs.items():
        tests = []
        drift_types = config.get('drift_types', [])
        drift_intensities = config.get('drift_intensities', [])

        # Create test configurations based on drift types and intensities
        for drift_type in drift_types:
            for intensity in drift_intensities:
                # Para CADA combina√ß√£o de drift_type e intensity, cria um teste
                tests.append({...})
```

**Por que √© Lento**:
- Para configura√ß√£o "full", h√° m√∫ltiplos `drift_types` e `drift_intensities`
- Cada combina√ß√£o gera um teste individual
- Cada teste no m√©todo `run()` (linha 1566-1696) executa:
  - `evaluate_distribution_shift()` - an√°lise completa de shift de distribui√ß√£o
  - `evaluate_worst_sample()` - identifica piores amostras
  - `evaluate_worst_cluster()` - clustering com K-means (linha 906)
  - `evaluate_outer_sample()` - detec√ß√£o de outliers com IsolationForest (linha 1134-1140)
  - `evaluate_hard_sample()` - an√°lise de discord√¢ncia entre modelos

**Tempo Medido**: 202.75s para criar o experimento (antes mesmo de executar!)

#### Causa Raiz

A **cria√ß√£o do experimento** est√° demorando porque o construtor da classe `Experiment` j√° executa trabalho pesado:

**Arquivo**: `deepbridge/core/experiment/experiment.py` (linhas 100-177)

```python
def __init__(self, dataset, experiment_type, ...):
    # ...
    # Linha 166: Inicializa componentes
    self._initialize_components(dataset, test_size, random_state)
        # Linha 46: Prepara dados
        # Linha 52: create_alternative_models() - TREINA M√öLTIPLOS MODELOS! ‚ö†Ô∏è

    # Linha 169: Inicializa test runner
    self._initialize_test_runner()

    # Linha 176: Calcula m√©tricas iniciais
    self._process_initial_metrics()
        # Linha 74: run_initial_tests() - EXECUTA TESTES INICIAIS! ‚ö†Ô∏è
```

**O problema**: No script `run_individual_tests.py`, um novo `Experiment` √© criado para **cada tipo de teste**:
```python
# Linha 127-131 em run_individual_tests.py
experimento = Experiment(
    dataset=dataset_complete,
    experiment_type="binary_classification",
    tests=[test_type]  # Criado 3 vezes: robustness, uncertainty, resilience
)
```

Isso significa que:
1. **Modelos alternativos s√£o treinados 3 vezes** (uma vez por teste)
2. **Testes iniciais s√£o executados 3 vezes**
3. **Prepara√ß√£o de dados √© feita 3 vezes**

---

### 2. GARGALO CR√çTICO #2: Execu√ß√£o do Teste de Incerteza (216.40s)

#### An√°lise do C√≥digo

**Arquivo**: `deepbridge/validation/wrappers/uncertainty_suite.py`

**Problema Identificado** (linhas 843-1053):

```python
class CRQR:
    def fit(self, X, y):
        # Linha 896-906: Divide dados em train/calib/test
        X_train, X_temp, y_train, y_temp = train_test_split(...)
        X_calib, X_test, y_calib, y_test = train_test_split(...)

        # Linha 914: Treina modelo base
        self.base_model.fit(X_train, y_train)

        # Linha 948-949: Treina DOIS modelos de regress√£o quantil
        self.quantile_model_lower.fit(X_train, residuals)
        self.quantile_model_upper.fit(X_train, residuals)
```

**Por que √© Lento**:
1. **Treina 3 modelos por itera√ß√£o**:
   - 1 modelo base (HistGradientBoostingRegressor)
   - 1 modelo quantil inferior (GradientBoostingRegressor)
   - 1 modelo quantil superior (GradientBoostingRegressor)

2. **Para cada feature testada** (linhas 362-374 em `uncertainty_suite.py`):
   ```python
   for feature in features_to_test:
       # Treina NOVAMENTE os 3 modelos para cada feature! ‚ö†Ô∏è
       feature_result = self.evaluate_uncertainty(method, params, feature=feature)
   ```

3. **GradientBoostingRegressor √© intrinsecamente lento**:
   - Treina √°rvores de decis√£o iterativamente
   - Cada √°rvore depende da anterior (n√£o paraleliz√°vel)

**Tempo Medido**: 216.40s (3.61 minutos) apenas para executar o teste

---

### 3. Cria√ß√£o do Experimento de Robustez (44.23s)

**Arquivo**: `deepbridge/validation/wrappers/robustness_suite.py`

**Problema Identificado**:
- Menos cr√≠tico que os anteriores, mas ainda significativo
- Linha 52 em `experiment.py`: Cria√ß√£o de modelos alternativos executada novamente
- Linha 255-353 em `robustness_suite.py`: Loop de perturba√ß√µes pode ser otimizado

**Tempo Medido**: 44.23s

---

### 4. Cria√ß√£o do Experimento de Incerteza (38.82s)

**Mesmo problema da cria√ß√£o de experimento de Resili√™ncia**: Modelos alternativos sendo treinados novamente.

**Tempo Medido**: 38.82s

---

## üöÄ Sugest√µes de Otimiza√ß√£o

### PRIORIDADE 1: Reutilizar Experimento Base (Ganho Estimado: 50-60%)

#### Problema
Atualmente, um novo `Experiment` √© criado para cada teste:
```python
# 3 chamadas separadas = 3x overhead
experimento_robustness = Experiment(dataset, tests=["robustness"])
experimento_uncertainty = Experiment(dataset, tests=["uncertainty"])
experimento_resilience = Experiment(dataset, tests=["resilience"])
```

#### Solu√ß√£o Proposta

**Modificar `run_individual_tests.py`** para criar um √∫nico experimento:

```python
# Criar experimento √öNICO com TODOS os testes
experimento = Experiment(
    dataset=dataset_complete,
    experiment_type="binary_classification",
    tests=["robustness", "uncertainty", "resilience"]  # Todos juntos!
)

# Executar cada teste individualmente usando o mesmo experimento
for test_type, test_name in test_configs:
    timings = executar_teste_individual_otimizado(
        experimento=experimento,  # Reutilizar o mesmo
        test_type=test_type,
        test_name=test_name,
        results_path=results_path
    )
```

**Implementa√ß√£o**:
```python
def executar_teste_individual_otimizado(experimento, test_type, test_name, results_path):
    """
    Executa um teste usando um experimento j√° criado (SEM recri√°-lo).
    """
    print_section(f"EXECUTANDO TESTE: {test_name.upper()}")
    logger.info(f"Iniciando teste: {test_name}")

    timings = {}

    # N√ÉO cria novo experimento - usa o existente
    # APENAS executa o teste
    start_time_run = time.time()
    results = experimento.run_tests("full", tests=[test_type])
    timings['executar_teste'] = time.time() - start_time_run

    # Salvar resultados...
    # (resto do c√≥digo permanece igual)

    return timings
```

**Ganho Estimado**:
- Elimina√ß√£o de 2 cria√ß√µes de experimento = ~280s economizados
- Novo tempo total: ~230s (de 512s) = **55% de redu√ß√£o**

---

### PRIORIDADE 2: Paraleliza√ß√£o de Testes (Ganho Estimado: 60-70%)

#### Problema
Testes s√£o executados **sequencialmente**, um de cada vez.

#### Solu√ß√£o Proposta

Usar `concurrent.futures` ou `multiprocessing` para executar testes em paralelo:

```python
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing

def executar_teste_parallel(args):
    """Fun√ß√£o auxiliar para execu√ß√£o paralela"""
    dataset, test_type, test_name, results_path = args
    return executar_teste_individual(dataset, test_type, test_name, results_path)

def executar_testes_individuais(artefatos_path, results_path, data_path, sample_frac=0.1):
    # ... prepara√ß√£o inicial ...

    test_configs = [
        ("robustness", "Robustez"),
        ("uncertainty", "Incerteza"),
        ("resilience", "Resili√™ncia")
    ]

    # Determinar n√∫mero de workers
    n_workers = min(3, multiprocessing.cpu_count())  # M√°ximo 3 testes

    # Preparar argumentos para cada teste
    test_args = [
        (dataset_complete, test_type, test_name, results_path)
        for test_type, test_name in test_configs
    ]

    # Executar testes em paralelo
    test_timings = {}
    with ProcessPoolExecutor(max_workers=n_workers) as executor:
        future_to_test = {
            executor.submit(executar_teste_parallel, args): args[1]
            for args in test_args
        }

        for future in as_completed(future_to_test):
            test_type = future_to_test[future]
            try:
                timings = future.result()
                test_timings[test_type] = timings
            except Exception as e:
                logger.error(f"Erro no teste {test_type}: {str(e)}")
                test_timings[test_type] = None

    return test_timings
```

**Ganho Estimado** (com 3 cores):
- Tempo do teste mais longo (Uncertainty): ~216s
- Tempo total paralelo: ~216s (vs 512s sequencial)
- **58% de redu√ß√£o no tempo total**

**Combinado com PRIORIDADE 1**:
- Tempo total: ~150-180s
- **65-70% de redu√ß√£o total**

---

### PRIORIDADE 3: Otimizar CRQR com Cache de Modelos (Ganho Estimado: 70%)

#### Problema
Para **cada feature**, o CRQR treina 3 modelos do zero (216s / feature).

#### Solu√ß√£o Proposta

**Modificar `uncertainty_suite.py`** para cachear modelos j√° treinados:

```python
class UncertaintySuite:
    def __init__(self, ...):
        # ...
        self._model_cache = {}  # Cache de modelos treinados

    def evaluate_uncertainty(self, method: str, params: Dict, feature=None):
        # ...

        if method == 'crqr':
            alpha = params.get('alpha', 0.1)
            test_size = params.get('test_size', 0.3)
            calib_ratio = params.get('calib_ratio', 1/3)

            # Chave de cache baseada nos par√¢metros
            cache_key = (alpha, test_size, calib_ratio, feature is None)

            # Verificar cache
            if cache_key in self._model_cache and feature is not None:
                # Reutilizar modelo existente para an√°lise de features
                model = self._model_cache[cache_key]

                # Apenas calcular import√¢ncia da feature sem retreinar
                feature_importance = self._calculate_feature_importance_fast(
                    model, X, y, feature
                )

                # Retornar resultados usando modelo cacheado
                return {
                    'method': 'crqr',
                    'alpha': alpha,
                    'feature_importance': {feature: feature_importance},
                    'from_cache': True  # Indicador de cache
                }

            # Se n√£o est√° em cache, criar e cachear
            model = self._create_crqr_model(alpha, test_size, calib_ratio)
            model.fit(X, y)

            if feature is None:
                # Cachear apenas o modelo geral (sem feature espec√≠fica)
                self._model_cache[cache_key] = model

            # ... resto da l√≥gica ...

    def _calculate_feature_importance_fast(self, model, X, y, feature):
        """
        Calcula import√¢ncia de feature SEM retreinar modelos.
        Usa an√°lise de sensibilidade ou permuta√ß√£o.
        """
        from sklearn.inspection import permutation_importance

        # Usar permutation importance (muito mais r√°pido que retreinar)
        result = permutation_importance(
            model.base_model, X, y,
            n_repeats=5,
            random_state=42,
            n_jobs=-1
        )

        # Encontrar √≠ndice da feature
        feature_idx = X.columns.get_loc(feature)
        importance = result.importances_mean[feature_idx]

        return abs(importance)
```

**Ganho Estimado**:
- Sem retreinamento: ~65s (de 216s)
- **70% de redu√ß√£o no tempo do teste de incerteza**

---

### PRIORIDADE 4: Reduzir Configura√ß√µes do Teste de Resili√™ncia (Ganho Estimado: 50-70%)

#### Problema
A configura√ß√£o "full" do teste de resili√™ncia gera **dezenas de testes** devido a todas as combina√ß√µes de:
- drift_types: ['covariate', 'concept', 'label', 'distribution', 'statistical']
- drift_intensities: [0.1, 0.2, 0.3, 0.4, 0.5]
- test_scenarios: ['worst_sample', 'worst_cluster', 'outer_sample', 'hard_sample']

**Total de testes**: 5 √ó 5 √ó 4 = 100+ testes individuais

#### Solu√ß√£o Proposta

**Op√ß√£o 1: Configura√ß√£o "full" Mais Inteligente**

Modificar `parameter_standards.py` para reduzir combina√ß√µes redundantes:

```python
# Em vez de todas as combina√ß√µes (5√ó5=25 testes)
drift_types = ['covariate', 'concept', 'label', 'distribution', 'statistical']
drift_intensities = [0.1, 0.2, 0.3, 0.4, 0.5]

# Usar amostragem estrat√©gica (apenas 9 testes)
test_combinations = [
    ('covariate', 0.1),   # Baixa intensidade
    ('covariate', 0.3),   # M√©dia intensidade
    ('covariate', 0.5),   # Alta intensidade
    ('concept', 0.2),
    ('label', 0.2),
    ('distribution', 0.3),
    ('statistical', 0.3),
    ('covariate', 0.4),   # Combina√ß√£o adicional para drift comum
    ('concept', 0.4),
]
```

**Op√ß√£o 2: Modo "Adaptive Testing"**

Implementar teste adaptativo que executa apenas combina√ß√µes relevantes:

```python
class ResilienceSuite:
    def run_adaptive(self):
        """
        Executa testes de forma adaptativa:
        1. Testa com intensidade baixa (0.1)
        2. Se impacto > threshold, testa intensidades maiores
        3. Caso contr√°rio, pula para pr√≥ximo drift_type
        """
        results = {}

        for drift_type in drift_types:
            # Sempre testa intensidade baixa
            low_result = self.evaluate_distribution_shift(drift_type, 0.1)

            if low_result['impact'] > 0.1:  # Threshold configur√°vel
                # Se impacto significativo, testa intensidades maiores
                med_result = self.evaluate_distribution_shift(drift_type, 0.3)

                if med_result['impact'] > 0.2:
                    # Se impacto alto, testa intensidade m√°xima
                    high_result = self.evaluate_distribution_shift(drift_type, 0.5)
            else:
                # Se impacto baixo, pula para pr√≥ximo drift_type
                logger.info(f"Impacto baixo para {drift_type}, pulando intensidades maiores")

        return results
```

**Ganho Estimado**:
- Redu√ß√£o de 100+ testes para 20-30 testes
- Tempo: ~60-80s (de 202s)
- **60-70% de redu√ß√£o**

---

### PRIORIDADE 5: Usar Algoritmos Mais R√°pidos (Ganho Estimado: 30-40%)

#### Problema
`GradientBoostingRegressor` √© lento por natureza (√°rvores sequenciais).

#### Solu√ß√£o Proposta

**Substituir por HistGradientBoostingRegressor** (nativo no scikit-learn >= 1.0):

```python
class CRQR:
    def fit(self, X, y):
        # Antes (lento):
        # from sklearn.ensemble import GradientBoostingRegressor
        # self.quantile_model_lower = GradientBoostingRegressor(...)

        # Depois (r√°pido):
        from sklearn.ensemble import HistGradientBoostingRegressor

        self.quantile_model_lower = HistGradientBoostingRegressor(
            loss='quantile',
            quantile=self.alpha/2,
            max_depth=5,
            max_iter=100,  # Limitar itera√ß√µes para velocidade
            early_stopping=True,  # Parar quando n√£o melhora mais
            random_state=self.random_state
        )

        self.quantile_model_upper = HistGradientBoostingRegressor(
            loss='quantile',
            quantile=1-self.alpha/2,
            max_depth=5,
            max_iter=100,
            early_stopping=True,
            random_state=self.random_state
        )
```

**Alternativa: LightGBM** (ainda mais r√°pido):

```python
try:
    import lightgbm as lgb

    self.quantile_model_lower = lgb.LGBMRegressor(
        objective='quantile',
        alpha=self.alpha/2,
        max_depth=5,
        n_estimators=100,
        n_jobs=-1,  # Paraleliza√ß√£o autom√°tica
        random_state=self.random_state
    )
except ImportError:
    # Fallback para HistGradientBoostingRegressor
    pass
```

**Ganho Estimado**:
- HistGradientBoostingRegressor: ~150s (de 216s) = 30% mais r√°pido
- LightGBM: ~130s (de 216s) = 40% mais r√°pido

---

### PRIORIDADE 6: Lazy Evaluation e Data Sharing (Ganho Estimado: 20-30%)

#### Problema
Dados s√£o copiados m√∫ltiplas vezes desnecessariamente.

#### Solu√ß√£o Proposta

**1. Usar views do Pandas em vez de c√≥pias**:

```python
# Antes (faz c√≥pia):
X_subset = X[feature_columns].copy()

# Depois (view sem c√≥pia):
X_subset = X[feature_columns]  # Remove .copy()
# OU
X_subset = X.loc[:, feature_columns]  # View expl√≠cita
```

**2. Compartilhar dados entre testes**:

```python
class Experiment:
    def __init__(self, ...):
        # ...
        # Criar dados compartilhados em mem√≥ria
        self._shared_data = {
            'X_train': self.X_train,
            'X_test': self.X_test,
            'y_train': self.y_train,
            'y_test': self.y_test
        }

    def run_tests(self, config, tests=None):
        # Passar refer√™ncias aos dados (n√£o c√≥pias)
        for test_type in tests:
            test_runner = self._get_test_runner(test_type)
            test_runner.set_shared_data(self._shared_data)  # Compartilha
            results = test_runner.run(config)
```

**Ganho Estimado**: 20-30% de redu√ß√£o em overhead de mem√≥ria e c√≥pias

---

### PRIORIDADE 7: Implementar Progressive Testing (Ganho Estimado: 40-60% para explora√ß√£o)

#### Conceito
Executar testes de forma progressiva: quick ‚Üí medium ‚Üí full (apenas se necess√°rio).

#### Implementa√ß√£o

```python
def executar_testes_progressivos(dataset, results_path, max_time_seconds=300):
    """
    Executa testes progressivamente at√© atingir limite de tempo.
    """
    configs = ['quick', 'medium', 'full']
    results = {}

    start_time = time.time()

    for config in configs:
        elapsed = time.time() - start_time
        if elapsed > max_time_seconds:
            logger.info(f"Limite de tempo atingido, parando em config '{config}'")
            break

        logger.info(f"Executando config '{config}'...")
        experimento = Experiment(dataset, tests=["robustness", "uncertainty", "resilience"])

        config_results = experimento.run_tests(config)
        results[config] = config_results

        # An√°lise adaptativa: se resultados quick s√£o bons, pular full
        if config == 'quick' and _resultados_satisfatorios(config_results):
            logger.info("Resultados satisfat√≥rios em 'quick', pulando 'medium' e 'full'")
            break

    return results

def _resultados_satisfatorios(results):
    """Determina se resultados quick s√£o suficientes"""
    # Exemplo: se todos os testes t√™m baixo impacto
    avg_impact = np.mean([
        results.get('robustness', {}).get('avg_overall_impact', 1.0),
        results.get('uncertainty', {}).get('avg_coverage_error', 1.0),
        results.get('resilience', {}).get('resilience_score', 0.0)
    ])

    return avg_impact < 0.15  # Threshold configur√°vel
```

**Ganho Estimado**:
- Para casos explorat√≥rios: 40-60% de economia
- Para valida√ß√£o completa: Nenhum ganho (ainda executa full)

---

## üìà Resumo de Ganhos Esperados

### Aplicando Todas as Otimiza√ß√µes

| Otimiza√ß√£o | Ganho Individual | Tempo Reduzido |
|------------|------------------|----------------|
| **Baseline** | - | 512.77s |
| 1. Reutilizar Experimento | 55% | ~230s |
| 2. Paraleliza√ß√£o | 65% (cumulativo) | ~180s |
| 3. Cache CRQR | 70% no uncertainty | ~150s |
| 4. Reduzir Config Resilience | 60% no resilience | ~120s |
| 5. Algoritmos R√°pidos | 30% adicional | ~100s |
| 6. Lazy Evaluation | 20% adicional | ~80s |
| 7. Progressive Testing | Vari√°vel | ~50-80s |

### **Tempo Final Estimado: 80-100 segundos (~1.5 minutos)**

**Redu√ß√£o Total: 80-85% do tempo original**

---

## üéØ Plano de Implementa√ß√£o Recomendado

### Fase 1: Ganhos R√°pidos (1-2 dias)
1. **Reutilizar Experimento Base** (PRIORIDADE 1)
   - Modificar `run_individual_tests.py`
   - Ganho: 55%

2. **Usar HistGradientBoostingRegressor** (PRIORIDADE 5)
   - Modificar `uncertainty_suite.py` (linha 923-945)
   - Ganho adicional: 30%

**Ganho Fase 1**: ~70% de redu√ß√£o (tempo: 150-180s)

### Fase 2: Otimiza√ß√µes M√©dias (3-5 dias)
3. **Cache de Modelos CRQR** (PRIORIDADE 3)
   - Modificar `uncertainty_suite.py`
   - Adicionar `_model_cache` e `_calculate_feature_importance_fast()`
   - Ganho adicional: 40-50%

4. **Reduzir Configura√ß√µes Resilience** (PRIORIDADE 4)
   - Modificar `parameter_standards.py`
   - Implementar amostragem estrat√©gica
   - Ganho adicional: 30-40%

**Ganho Fase 2**: ~80% de redu√ß√£o total (tempo: 80-100s)

### Fase 3: Otimiza√ß√µes Avan√ßadas (1 semana)
5. **Paraleliza√ß√£o** (PRIORIDADE 2)
   - Refatorar `run_individual_tests.py`
   - Implementar `ProcessPoolExecutor`
   - Ganho adicional: 20-30% (com overhead de paraleliza√ß√£o)

6. **Lazy Evaluation** (PRIORIDADE 6)
   - Revisar uso de `.copy()` em todo o c√≥digo
   - Implementar shared data structures
   - Ganho adicional: 10-15%

**Ganho Fase 3**: ~85% de redu√ß√£o total (tempo: 60-80s)

---

## üîß C√≥digo de Refer√™ncia: Implementa√ß√£o Completa de PRIORIDADE 1

```python
#!/usr/bin/env python3
"""
Pipeline de Testes Individuais OTIMIZADO com Reutiliza√ß√£o de Experimento
"""

import argparse
import os
import sys
import time
import logging
from datetime import datetime
from pathlib import Path

import pandas as pd
import numpy as np
import joblib
from sklearn.model_selection import train_test_split

from deepbridge.core.db_data import DBDataset
from deepbridge.core.experiment import Experiment

# Logger global
logger = None


def executar_teste_individual_otimizado(experimento, test_type, test_name, results_path):
    """
    Executa um teste individual usando um experimento J√Å CRIADO (reutiliza).

    Esta fun√ß√£o N√ÉO cria um novo experimento, apenas executa o teste espec√≠fico.

    Args:
        experimento: Objeto Experiment j√° inicializado
        test_type: Tipo do teste (robustness, uncertainty, resilience)
        test_name: Nome do teste para exibi√ß√£o
        results_path: Caminho para salvar os resultados

    Returns:
        dict: Dicion√°rio com tempos de cada etapa
    """
    print_section(f"EXECUTANDO TESTE: {test_name.upper()}")
    logger.info(f"Iniciando teste: {test_name}")
    logger.debug(f"Tipo: {test_type}")

    timings = {}

    # ========== ETAPA 1: Executar o teste ==========
    # N√ÉO cria experimento - apenas executa!
    print(f"\nüß™ Executando teste de {test_name}...")
    print("‚è≥ Aguarde...\n")

    logger.info(f"[{test_type}] Iniciando execu√ß√£o do teste...")
    start_time_run = time.time()

    try:
        # Executa apenas este teste espec√≠fico
        results = experimento.run_tests("full", tests=[test_type])
        timings['executar_teste'] = time.time() - start_time_run

        logger.info(f"[{test_type}] Teste conclu√≠do com sucesso")
        logger.info(f"[{test_type}] ‚è±Ô∏è  Tempo de execu√ß√£o do teste: {timings['executar_teste']:.2f}s")
        print(f"‚úÖ Teste de {test_name} conclu√≠do!")
        print(f"   ‚è±Ô∏è  Execu√ß√£o do teste: {timings['executar_teste']:.2f}s")

    except Exception as e:
        timings['executar_teste'] = time.time() - start_time_run
        logger.error(f"[{test_type}] Erro durante execu√ß√£o: {str(e)}", exc_info=True)
        raise

    # ========== ETAPA 2: Salvar resultados ==========
    print(f"\nüíæ Salvando resultados de {test_name}...")
    logger.info(f"[{test_type}] Salvando resultados...")

    report_path = os.path.join(results_path, f'report_{test_type}_individual.html')
    json_path = os.path.join(results_path, f'{test_type}_results_individual.json')

    start_time_save = time.time()
    try:
        # Salvar HTML
        start_time_html = time.time()
        results.save_html(test_type, report_path, report_type="interactive")
        timings['salvar_html'] = time.time() - start_time_html
        logger.debug(f"[{test_type}] HTML salvo: {report_path}")

        # Salvar JSON
        start_time_json = time.time()
        results.save_json(test_type, json_path)
        timings['salvar_json'] = time.time() - start_time_json
        logger.debug(f"[{test_type}] JSON salvo: {json_path}")

        timings['salvar_total'] = time.time() - start_time_save

        if os.path.exists(report_path):
            file_size = os.path.getsize(report_path)
            print(f"  ‚úÖ HTML: {file_size:,} bytes")

        if os.path.exists(json_path):
            json_size = os.path.getsize(json_path)
            print(f"  ‚úÖ JSON: {json_size:,} bytes")

        print(f"  ‚è±Ô∏è  Tempo total de salvamento: {timings['salvar_total']:.2f}s")
        logger.info(f"[{test_type}] ‚è±Ô∏è  Tempo total para salvar resultados: {timings['salvar_total']:.2f}s")

    except Exception as e:
        timings['salvar_total'] = time.time() - start_time_save
        logger.warning(f"[{test_type}] Erro ao salvar resultados: {str(e)}")
        print(f"  ‚ö†Ô∏è Erro ao salvar: {str(e)}")

    # Calcular tempo total
    timings['total'] = timings.get('executar_teste', 0) + timings.get('salvar_total', 0)

    # Resumo de tempos
    logger.info(f"[{test_type}] ===== RESUMO DE TEMPOS =====")
    logger.info(f"[{test_type}]   1. Executar teste:     {timings.get('executar_teste', 0):8.2f}s")
    logger.info(f"[{test_type}]   2. Salvar resultados:  {timings.get('salvar_total', 0):8.2f}s")
    logger.info(f"[{test_type}]   TOTAL:                 {timings['total']:8.2f}s")
    logger.info(f"[{test_type}] ============================")

    return timings


def executar_testes_individuais_otimizado(artefatos_path, results_path, data_path, sample_frac=0.1):
    """
    Executa os testes de forma individual OTIMIZADA, reutilizando um √∫nico experimento.

    OTIMIZA√á√ÉO PRINCIPAL: Cria apenas UM experimento para todos os testes.
    """
    print_section("TESTES INDIVIDUAIS OTIMIZADOS - EXPERIMENTO √öNICO")
    logger.info("Iniciando execu√ß√£o de testes individuais otimizados")

    prep_timings = {}

    # ========== Carregar modelo e dados (igual ao original) ==========
    # [c√≥digo de carregamento id√™ntico ao original...]

    # ========== OTIMIZA√á√ÉO: Criar √öNICO experimento com TODOS os testes ==========
    logger.info("Criando experimento √∫nico para todos os testes...")
    print(f"\nüî¨ Criando experimento √∫nico para todos os testes...")

    start_time_experiment = time.time()
    try:
        # ‚ú® MUDAN√áA PRINCIPAL: Um experimento para TODOS os testes
        experimento = Experiment(
            dataset=dataset_complete,
            experiment_type="binary_classification",
            tests=["robustness", "uncertainty", "resilience"]  # Todos juntos!
        )
        prep_timings['criar_experimento'] = time.time() - start_time_experiment

        logger.info(f"Experimento √∫nico criado com sucesso")
        logger.info(f"‚è±Ô∏è  Tempo para criar experimento: {prep_timings['criar_experimento']:.2f}s")
        print(f"‚úÖ Experimento √∫nico criado")
        print(f"   ‚è±Ô∏è  Tempo: {prep_timings['criar_experimento']:.2f}s")

    except Exception as e:
        prep_timings['criar_experimento'] = time.time() - start_time_experiment
        logger.error(f"Erro ao criar experimento: {str(e)}", exc_info=True)
        raise

    # Resumo de tempos de prepara√ß√£o
    prep_total = sum(prep_timings.values())
    logger.info("===== RESUMO DE TEMPOS DE PREPARA√á√ÉO =====")
    logger.info(f"  Criar experimento √∫nico:   {prep_timings['criar_experimento']:8.2f}s")
    logger.info(f"  TOTAL PREPARA√á√ÉO:          {prep_total:8.2f}s")
    logger.info("==========================================")

    # Definir testes a serem executados
    test_configs = [
        ("robustness", "Robustez"),
        ("uncertainty", "Incerteza"),
        ("resilience", "Resili√™ncia")
    ]

    # Dicion√°rio para armazenar tempos de cada teste
    test_timings = {}

    # ========== Executar cada teste usando o MESMO experimento ==========
    logger.info("=" * 70)
    logger.info("INICIANDO EXECU√á√ÉO INDIVIDUAL DOS TESTES (EXPERIMENTO REUTILIZADO)")
    logger.info("=" * 70)

    for test_type, test_name in test_configs:
        try:
            # ‚ú® USA O MESMO EXPERIMENTO para cada teste
            timings = executar_teste_individual_otimizado(
                experimento=experimento,  # Reutilizar!
                test_type=test_type,
                test_name=test_name,
                results_path=results_path
            )
            test_timings[test_type] = timings

        except Exception as e:
            logger.error(f"Erro no teste {test_name}: {str(e)}", exc_info=True)
            print(f"\n‚ùå Erro no teste {test_name}: {str(e)}")
            test_timings[test_type] = None

    # ========== RESUMO FINAL ==========
    print_section("RESUMO COMPLETO - VERS√ÉO OTIMIZADA")

    # Calcular totais
    test_total = sum([t.get('total', 0) for t in test_timings.values() if t is not None])
    grand_total = prep_total + test_total

    print(f"\nüìã TEMPOS:")
    print(f"   Prepara√ß√£o (experimento √∫nico):  {prep_total:8.2f}s")
    print(f"   Testes (reutilizando):           {test_total:8.2f}s")
    print(f"   {'='*50}")
    print(f"   TOTAL:                           {grand_total:8.2f}s ({grand_total/60:6.2f}min)")

    logger.info("=" * 70)
    logger.info(f"TEMPO TOTAL (OTIMIZADO): {grand_total:.2f}s ({grand_total/60:.2f}min)")
    logger.info("=" * 70)

    return {'preparacao': prep_timings, 'testes': test_timings}
```

---

## üìä M√©tricas de Sucesso

Para validar as otimiza√ß√µes, monitorar:

1. **Tempo Total de Execu√ß√£o**
   - Meta: < 100s (de 512s) = 80% de redu√ß√£o

2. **Tempo por Fase**
   - Prepara√ß√£o: < 10s (de 0.04s - sem mudan√ßa esperada)
   - Cria√ß√£o de Experimento: < 10s (de 285s total)
   - Execu√ß√£o de Testes: < 80s (de 227s total)

3. **Uso de Mem√≥ria**
   - Meta: < 2GB RAM (evitando c√≥pias desnecess√°rias)

4. **Throughput**
   - Meta: Processar 10k amostras em < 5 minutos

---

## ‚ö†Ô∏è Considera√ß√µes e Trade-offs

### Paraleliza√ß√£o
**Pr√≥s**:
- Ganho significativo de tempo (60-70%)
- Usa melhor recursos multi-core

**Contras**:
- Overhead de processos (~10-20%)
- Maior uso de mem√≥ria (3x)
- Mais complexo para debug

### Cache de Modelos
**Pr√≥s**:
- Elimina retreinamento desnecess√°rio
- Ganho de 70% no uncertainty

**Contras**:
- Uso de mem√≥ria (modelos cacheados)
- Precisa invalidar cache corretamente

### Progressive Testing
**Pr√≥s**:
- √ìtimo para explora√ß√£o r√°pida
- Reduz tempo em 40-60%

**Contras**:
- Pode perder insights de config "full"
- Requer boa heur√≠stica de stopping

---

## üéì Conclus√£o

A an√°lise identificou que **os gargalos principais s√£o**:
1. Recria√ß√£o desnecess√°ria de experimentos (3x overhead)
2. Treino repetitivo de modelos CRQR para cada feature
3. Excesso de combina√ß√µes de testes no resilience

**Implementando as PRIORIDADES 1, 3 e 4**:
- Ganho esperado: **75-80% de redu√ß√£o**
- Tempo final: **~100 segundos** (de 512s)
- Esfor√ßo: **1-2 semanas**

**Com todas as otimiza√ß√µes**:
- Ganho esperado: **85% de redu√ß√£o**
- Tempo final: **~80 segundos** (de 512s)
- Esfor√ßo: **2-3 semanas**

---

**Documento gerado automaticamente em**: 30/10/2025
**Autor**: An√°lise Profunda do DeepBridge
**Vers√£o**: 1.0
