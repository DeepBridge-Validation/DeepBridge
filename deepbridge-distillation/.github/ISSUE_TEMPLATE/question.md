---
name: Question
about: Ask a question about DeepBridge Distillation
title: '[QUESTION] '
labels: question
assignees: ''
---

## â“ Question

Please describe your question about knowledge distillation clearly and concisely.

## ğŸ“‹ Context

Provide context to help us understand your question:

- **What distillation task are you working on?**
- **What have you tried so far?**
- **Where did you look for answers?** (docs, examples, papers, etc.)

## ğŸ’» Code Example (Optional)

If relevant, provide a minimal code example:

```python
from deepbridge.distillation import ...

# Your code here
```

## ğŸ—ï¸ Model Details (If Relevant)

- **Teacher Model:** [e.g., ResNet50, BERT-large]
- **Student Model:** [e.g., ResNet18, DistilBERT]
- **Distillation Method:** [e.g., KD, FitNets, Attention Transfer]
- **Dataset:** [e.g., ImageNet, GLUE]

## ğŸ“š Environment (If Relevant)

- **deepbridge-distillation Version:** [e.g., 2.0.0]
- **Python Version:** [e.g., 3.10.5]
- **Framework:** [e.g., PyTorch 2.0, TensorFlow 2.13]

## ğŸ” What I've Tried

Describe what you've already tried or researched:

- [ ] Checked the [documentation](https://deepbridge-distillation.readthedocs.io/)
- [ ] Searched existing issues
- [ ] Looked at examples in the repository
- [ ] Reviewed relevant papers
- [ ] Other: ___

## ğŸ’¡ Expected Answer

What kind of answer or guidance are you looking for?

---

**Note:** For bug reports, please use the "Bug Report" template instead.
