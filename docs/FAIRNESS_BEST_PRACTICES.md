# Guia de Boas Pr√°ticas: An√°lise de Fairness no DeepBridge

## üìã √çndice

1. [Princ√≠pios Fundamentais](#princ√≠pios-fundamentais)
2. [Antes de Come√ßar](#antes-de-come√ßar)
3. [Prepara√ß√£o de Dados](#prepara√ß√£o-de-dados)
4. [Sele√ß√£o de M√©tricas](#sele√ß√£o-de-m√©tricas)
5. [Interpreta√ß√£o de Resultados](#interpreta√ß√£o-de-resultados)
6. [Mitiga√ß√£o de Vi√©s](#mitiga√ß√£o-de-vi√©s)
7. [Monitoramento Cont√≠nuo](#monitoramento-cont√≠nuo)
8. [Considera√ß√µes Legais e √âticas](#considera√ß√µes-legais-e-√©ticas)
9. [Checklist de Valida√ß√£o](#checklist-de-valida√ß√£o)

---

## Princ√≠pios Fundamentais

### 1. Fairness √© Multidimensional

N√£o existe uma √∫nica defini√ß√£o de fairness. Diferentes contextos exigem diferentes trade-offs:

- **Statistical Parity**: Resultados iguais entre grupos
- **Equal Opportunity**: Igualdade na taxa de verdadeiros positivos
- **Equalized Odds**: Igualdade em TPR e FPR
- **Individual Fairness**: Indiv√≠duos similares tratados similarmente

‚ö†Ô∏è **IMPORTANTE**: √â matematicamente imposs√≠vel satisfazer todas as defini√ß√µes simultaneamente (Impossibility Theorem).

### 2. Contexto Importa

O que √© "justo" depende do dom√≠nio:

- **Cr√©dito**: EEOC 80% rule, Equal Opportunity
- **Recrutamento**: Disparate Impact, Statistical Parity
- **Sa√∫de**: Equal Opportunity (evitar falsos negativos)
- **Justi√ßa Criminal**: Equalized Odds, False Positive Rate

### 3. Transpar√™ncia e Documenta√ß√£o

Sempre documente:
- Por que certos atributos foram considerados sens√≠veis
- Quais m√©tricas foram priorizadas e por qu√™
- Trade-offs feitos entre fairness e performance
- Limita√ß√µes conhecidas

---

## Antes de Come√ßar

### ‚úÖ Checklist Pr√©-An√°lise

- [ ] **Definir stakeholders**: Quem ser√° afetado pelo modelo?
- [ ] **Identificar grupos protegidos**: Quais atributos s√£o legalmente/eticamente sens√≠veis?
- [ ] **Estabelecer m√©tricas de sucesso**: O que significa "justo" neste contexto?
- [ ] **Revisar regulamenta√ß√µes**: GDPR, CCPA, LGPD, EEOC, etc.
- [ ] **Obter consentimento**: Dados sens√≠veis foram coletados eticamente?

### ‚ùå Armadilhas Comuns

1. **Remover atributos protegidos n√£o elimina vi√©s**
   - Features correlacionadas (proxies) mant√™m o vi√©s
   - Exemplo: CEP correlacionado com ra√ßa

2. **Alta acur√°cia ‚â† Fairness**
   - Modelo pode ter 95% de acur√°cia mas vi√©s severo em grupos minorit√°rios

3. **Vi√©s no treinamento = Vi√©s no modelo**
   - Dados hist√≥ricos frequentemente refletem discrimina√ß√£o passada

---

## Prepara√ß√£o de Dados

### 1. Identifica√ß√£o de Atributos Sens√≠veis

#### Atributos Expl√≠citos

Use sempre que poss√≠vel especificar explicitamente:

```python
# ‚úÖ RECOMENDADO: Expl√≠cito
experiment = Experiment(
    dataset=dataset,
    experiment_type="binary_classification",
    tests=["fairness"],
    protected_attributes=['gender', 'race', 'age_group']  # Expl√≠cito
)
```

```python
# ‚ö†Ô∏è  CUIDADO: Auto-detec√ß√£o (apenas para explora√ß√£o)
experiment = Experiment(
    dataset=dataset,
    experiment_type="binary_classification",
    tests=["fairness"]  # Auto-detecta
)
```

#### Atributos Comuns

| Categoria | Exemplos |
|-----------|----------|
| **Demogr√°ficos** | gender, sex, race, ethnicity, nationality |
| **Idade** | age, age_group, birth_year |
| **Socioecon√¥micos** | income_bracket, education_level, marital_status |
| **Localiza√ß√£o** | zip_code, neighborhood, state |
| **Sa√∫de** | disability, medical_condition |
| **Outros** | religion, sexual_orientation, veteran_status |

### 2. Detec√ß√£o de Proxies

Verifique correla√ß√µes entre features e atributos protegidos:

```python
import pandas as pd

# Calcular correla√ß√µes
correlations = df[feature_cols].corrwith(df['protected_attribute'].astype('category').cat.codes)

# Identificar proxies (correla√ß√£o > 0.5)
proxies = correlations[abs(correlations) > 0.5]
print(f"Poss√≠veis proxies: {proxies.to_dict()}")
```

**Exemplos de Proxies**:
- CEP ‚Üí ra√ßa/renda
- Nome ‚Üí etnia/g√™nero
- Tipo de escola ‚Üí renda/ra√ßa

### 3. Balanceamento de Dados

```python
# Verificar balanceamento por grupo
for attr in protected_attributes:
    distribution = df[attr].value_counts(normalize=True)
    print(f"\n{attr}:")
    print(distribution)

    # Verificar target por grupo
    for group in df[attr].unique():
        target_rate = df[df[attr] == group]['target'].mean()
        print(f"  {group}: {target_rate:.1%}")
```

**Limiares de Aten√ß√£o**:
- Grupo < 5% da popula√ß√£o: Risco de underfitting
- Diferen√ßa > 20% na taxa de target: Poss√≠vel vi√©s nos dados

---

## Sele√ß√£o de M√©tricas

### Configura√ß√µes Recomendadas por Cen√°rio

#### 1. Explora√ß√£o Inicial (config='quick')

**Quando usar**: Primeira an√°lise, prototipagem r√°pida

**M√©tricas**:
- Statistical Parity
- Disparate Impact

```python
fairness_result = experiment.run_fairness_tests(config='quick')
```

**Tempo**: ~10-30 segundos

---

#### 2. Valida√ß√£o Intermedi√°ria (config='medium')

**Quando usar**: Ap√≥s ajustes iniciais, antes de produ√ß√£o

**M√©tricas**:
- 5 m√©tricas p√≥s-treino (Statistical Parity, Disparate Impact, Equal Opportunity, Equalized Odds, Precision Difference)
- 4 m√©tricas pr√©-treino (Class Balance, Concept Balance, KL Divergence, JS Divergence)
- Confusion Matrix por grupo

```python
fairness_result = experiment.run_fairness_tests(config='medium')
```

**Tempo**: ~1-3 minutos

---

#### 3. An√°lise Completa (config='full')

**Quando usar**: Auditoria final, produ√ß√£o, compliance

**M√©tricas**:
- 11 m√©tricas p√≥s-treino
- 4 m√©tricas pr√©-treino
- Confusion Matrix por grupo
- Threshold Analysis (99 pontos)

```python
fairness_result = experiment.run_fairness_tests(config='full')
```

**Tempo**: ~5-10 minutos

---

### M√©tricas por Dom√≠nio

| Dom√≠nio | M√©tricas Prim√°rias | M√©tricas Secund√°rias |
|---------|-------------------|---------------------|
| **Cr√©dito/Financeiro** | Disparate Impact, Equal Opportunity | Statistical Parity, Conditional Acceptance |
| **Recrutamento** | Statistical Parity, Disparate Impact | Equal Opportunity, Conditional Acceptance |
| **Sa√∫de** | Equal Opportunity, False Negative Rate | Equalized Odds, Precision Difference |
| **Justi√ßa Criminal** | Equalized Odds, False Positive Rate | Statistical Parity, Treatment Equality |
| **Educa√ß√£o** | Equal Opportunity, Statistical Parity | Disparate Impact, Accuracy Difference |

---

## Interpreta√ß√£o de Resultados

### 1. Overall Fairness Score

```python
score = fairness_result.overall_fairness_score
```

**Interpreta√ß√£o**:
- **0.90 - 1.00**: ‚úÖ Excelente - Deploy recomendado
- **0.80 - 0.89**: ‚úì Boa - Revisar warnings
- **0.70 - 0.79**: ‚ö†Ô∏è  Moderada - Melhorias recomendadas
- **< 0.70**: ‚ùå Cr√≠tica - N√ÉO deploy

### 2. An√°lise de Issues

```python
critical = fairness_result.critical_issues
warnings = fairness_result.warnings

print(f"Critical: {len(critical)}")
print(f"Warnings: {len(warnings)}")

# Revisar cada issue
for issue in critical:
    print(f"  - {issue}")
```

**Prioriza√ß√£o**:
1. **Critical Issues**: Resolver antes de deploy
2. **Warnings**: Documentar e monitorar
3. **OK**: Verificar periodicamente

### 3. M√©tricas Individuais

#### Statistical Parity (Paridade Estat√≠stica)

```
Valor: -0.15
Interpreta√ß√£o: ‚ö†Ô∏è  Amarelo (Warning)
```

**Significado**: Grupo desfavorecido tem 15 pontos percentuais a menos de outcomes positivos

**A√ß√£o**:
- Se |valor| < 0.10: OK
- Se 0.10 ‚â§ |valor| < 0.20: Investigar
- Se |valor| ‚â• 0.20: Mitiga√ß√£o necess√°ria

#### Disparate Impact (Impacto Desproporcional)

```
Valor: 0.72
Interpreta√ß√£o: ‚úó Vermelho (Critical)
EEOC: 0.80 (n√£o atende)
```

**Significado**: Taxa de aprova√ß√£o do grupo desfavorecido √© 72% da taxa do grupo favorecido

**A√ß√£o**:
- Se valor ‚â• 0.80: OK (EEOC compliant)
- Se 0.70 ‚â§ valor < 0.80: Revisar
- Se valor < 0.70: Viola√ß√£o EEOC - mitiga√ß√£o urgente

#### Equal Opportunity (Oportunidade Igual)

```
Valor: 0.08
Interpreta√ß√£o: ‚úì Verde (OK)
```

**Significado**: Diferen√ßa de 8% na taxa de verdadeiros positivos entre grupos

**A√ß√£o**:
- Se |valor| < 0.10: OK
- Se 0.10 ‚â§ |valor| < 0.15: Aten√ß√£o
- Se |valor| ‚â• 0.15: Mitiga√ß√£o necess√°ria

### 4. Threshold Analysis

```python
# Se dispon√≠vel (config='full')
if 'threshold_analysis' in fairness_result._results:
    optimal = fairness_result._results['threshold_analysis']['optimal_threshold']
    print(f"Threshold √≥timo: {optimal:.3f}")
```

**Uso**:
- Ajustar threshold de decis√£o para melhorar fairness
- Trade-off: Pode reduzir performance geral
- Testar em valida√ß√£o antes de aplicar em produ√ß√£o

---

## Mitiga√ß√£o de Vi√©s

### 1. Pr√©-processamento (Antes do Treinamento)

#### Re-balanceamento

```python
from imblearn.over_sampling import SMOTE

# Re-balancear por grupo
for group in df['protected_attr'].unique():
    group_data = df[df['protected_attr'] == group]
    # Aplicar SMOTE ou undersampling
```

#### Remo√ß√£o de Proxies

```python
# Identificar e remover features correlacionadas
high_corr_features = ['zip_code', 'school_type']  # Exemplo
X_clean = X.drop(columns=high_corr_features)
```

#### Reweighting

```python
from sklearn.utils.class_weight import compute_sample_weight

# Computar pesos para balancear grupos
sample_weights = compute_sample_weight(
    class_weight='balanced',
    y=df['protected_attr']
)

model.fit(X, y, sample_weight=sample_weights)
```

---

### 2. In-processing (Durante o Treinamento)

#### Fairness Constraints

```python
# Exemplo conceitual (requer bibliotecas espec√≠ficas)
# AIF360, Fairlearn, etc.

from fairlearn.reductions import ExponentiatedGradient, DemographicParity

mitigator = ExponentiatedGradient(
    estimator=base_model,
    constraints=DemographicParity()
)

mitigator.fit(X, y, sensitive_features=df['protected_attr'])
```

#### Adversarial Debiasing

```python
# Treinar com adversarial network
# que tenta prever atributo protegido
# (for√ßa o modelo a ser independente)
```

---

### 3. P√≥s-processamento (Ap√≥s o Treinamento)

#### Threshold Adjustment

```python
# Usar threshold analysis para encontrar threshold √≥timo
if 'threshold_analysis' in results:
    optimal_threshold = results['threshold_analysis']['optimal_threshold']

    # Aplicar threshold customizado
    y_pred_fair = (y_pred_proba >= optimal_threshold).astype(int)
```

#### Calibra√ß√£o por Grupo

```python
from sklearn.calibration import CalibratedClassifierCV

# Calibrar probabilidades separadamente por grupo
calibrated_models = {}

for group in df['protected_attr'].unique():
    group_mask = df['protected_attr'] == group

    calibrated = CalibratedClassifierCV(model, cv=5)
    calibrated.fit(X[group_mask], y[group_mask])

    calibrated_models[group] = calibrated
```

---

### Compara√ß√£o de Abordagens

| Abordagem | Vantagens | Desvantagens | Quando Usar |
|-----------|-----------|--------------|-------------|
| **Pr√©-processamento** | Simples, independente do modelo | Pode perder informa√ß√£o | Dados desbalanceados |
| **In-processing** | Integrado no treinamento | Requer modelos espec√≠ficos | Novo desenvolvimento |
| **P√≥s-processamento** | Aplic√°vel a modelos existentes | Pode reduzir performance | Modelos j√° em produ√ß√£o |

---

## Monitoramento Cont√≠nuo

### 1. Frequ√™ncia de Re-avalia√ß√£o

| Cen√°rio | Frequ√™ncia | Configura√ß√£o |
|---------|-----------|--------------|
| **Alto Risco** (cr√©dito, sa√∫de) | Semanal/Mensal | config='full' |
| **M√©dio Risco** (recrutamento) | Mensal/Trimestral | config='medium' |
| **Baixo Risco** (recomenda√ß√µes) | Trimestral/Anual | config='quick' |

### 2. Pipeline de Monitoramento

```python
def monitor_fairness(model, new_data, protected_attrs):
    """
    Pipeline de monitoramento cont√≠nuo de fairness.
    """
    # 1. Criar dataset
    dataset = DBDataset(data=new_data, target_column='target', model=model)

    # 2. Executar an√°lise
    experiment = Experiment(
        dataset=dataset,
        experiment_type="binary_classification",
        tests=["fairness"],
        protected_attributes=protected_attrs
    )

    result = experiment.run_fairness_tests(config='medium')

    # 3. Verificar degrada√ß√£o
    if result.overall_fairness_score < 0.75:
        send_alert(f"Fairness score dropped to {result.overall_fairness_score:.3f}")

    # 4. Gerar relat√≥rio
    result.save_html(
        f'monitoring/fairness_{datetime.now().strftime("%Y%m%d")}.html',
        model_name=f'Model Monitoring - {datetime.now().strftime("%Y-%m-%d")}'
    )

    return result
```

### 3. Alertas e Thresholds

```python
# Configurar alertas
ALERT_THRESHOLDS = {
    'overall_fairness_score': 0.75,
    'disparate_impact_min': 0.80,
    'statistical_parity_max': 0.15,
    'critical_issues_max': 0
}

def check_alerts(result):
    alerts = []

    if result.overall_fairness_score < ALERT_THRESHOLDS['overall_fairness_score']:
        alerts.append(f"Overall score: {result.overall_fairness_score:.3f}")

    if len(result.critical_issues) > ALERT_THRESHOLDS['critical_issues_max']:
        alerts.append(f"Critical issues: {len(result.critical_issues)}")

    return alerts
```

---

## Considera√ß√µes Legais e √âticas

### 1. Regulamenta√ß√µes por Regi√£o

#### Estados Unidos

- **Equal Employment Opportunity Commission (EEOC)**: 80% rule
- **Fair Credit Reporting Act (FCRA)**: Transpar√™ncia em decis√µes de cr√©dito
- **Fair Housing Act**: Proibi√ß√£o de discrimina√ß√£o em habita√ß√£o

#### Europa

- **GDPR (General Data Protection Regulation)**:
  - Art. 22: Direito a explica√ß√µes sobre decis√µes automatizadas
  - Art. 9: Proibi√ß√£o de processar dados sens√≠veis sem consentimento

#### Brasil

- **LGPD (Lei Geral de Prote√ß√£o de Dados)**:
  - Art. 20: Direito de revis√£o de decis√µes automatizadas
  - Proibi√ß√£o de discrimina√ß√£o il√≠cita

### 2. Documenta√ß√£o Legal

Sempre documente:

```markdown
# Documenta√ß√£o de Fairness - [Nome do Modelo]

## 1. Atributos Protegidos
- Gender (justificativa: EEOC protected class)
- Race (justificativa: EEOC protected class)
- Age (justificativa: ADEA - Age Discrimination in Employment Act)

## 2. M√©tricas e Thresholds
- Disparate Impact ‚â• 0.80 (EEOC compliance)
- Statistical Parity ‚â§ 0.10 (internal policy)

## 3. Resultados
- Overall Fairness Score: 0.85
- Disparate Impact: 0.82 ‚úì (EEOC compliant)
- Critical Issues: 0

## 4. Mitiga√ß√µes Aplicadas
- Re-balanceamento de dados por grupo
- Threshold adjustment (0.45 ‚Üí 0.42)

## 5. Limita√ß√µes Conhecidas
- Grupos < 5% da popula√ß√£o: Asian (3.2%)
- Dados de treinamento: 2020-2023 (pode n√£o refletir mudan√ßas recentes)

## 6. Respons√°veis
- Data Scientist: [Nome]
- Legal Review: [Nome]
- Aprova√ß√£o: [Nome, Data]
```

### 3. Explicabilidade

Combine fairness com explicabilidade:

```python
# SHAP values por grupo
import shap

for group in protected_attributes:
    explainer = shap.TreeExplainer(model)

    group_data = df[df[group] == 'specific_value']
    shap_values = explainer.shap_values(group_data[feature_cols])

    shap.summary_plot(shap_values, group_data[feature_cols])
```

---

## Checklist de Valida√ß√£o

### ‚úÖ Antes do Deploy

- [ ] **An√°lise completa executada** (config='full')
- [ ] **Overall Fairness Score ‚â• 0.80**
- [ ] **Zero critical issues**
- [ ] **Disparate Impact ‚â• 0.80** (se aplic√°vel EEOC)
- [ ] **Documenta√ß√£o legal completa**
- [ ] **Aprova√ß√£o de stakeholders legais/√©ticos**
- [ ] **Plano de monitoramento definido**
- [ ] **Processo de re-treinamento documentado**

### ‚úÖ Em Produ√ß√£o

- [ ] **Monitoramento ativo** (frequ√™ncia definida)
- [ ] **Alertas configurados**
- [ ] **Relat√≥rios arquivados**
- [ ] **Logs de decis√µes mantidos**
- [ ] **Processo de revis√£o humana dispon√≠vel**
- [ ] **Canal para reportar vi√©s**

### ‚úÖ Manuten√ß√£o

- [ ] **Re-an√°lise ap√≥s cada re-treinamento**
- [ ] **Revis√£o trimestral de m√©tricas**
- [ ] **Atualiza√ß√£o de documenta√ß√£o**
- [ ] **Auditoria anual completa**

---

## Recursos Adicionais

### Bibliotecas Complementares

- **AIF360** (IBM): T√©cnicas de mitiga√ß√£o
- **Fairlearn** (Microsoft): Fairness-aware learning
- **What-If Tool** (Google): An√°lise interativa
- **SHAP**: Explicabilidade

### Refer√™ncias Acad√™micas

1. **Barocas, S., Hardt, M., & Narayanan, A.** (2019). *Fairness and Machine Learning*
2. **Mehrabi, N., et al.** (2021). *A Survey on Bias and Fairness in Machine Learning*
3. **IEEE P7003**: *Algorithmic Bias Considerations*

### Frameworks de Governan√ßa

- **EU AI Act**: Regulamenta√ß√£o de IA de alto risco
- **NIST AI Risk Management Framework**
- **ISO/IEC 23894**: *AI Risk Management*

---

## Conclus√£o

**Princ√≠pios-chave para lembrar**:

1. üéØ **Fairness √© um processo, n√£o um destino** - Monitoramento cont√≠nuo √© essencial
2. üìä **M√∫ltiplas m√©tricas s√£o necess√°rias** - Nenhuma m√©trica √∫nica captura tudo
3. ü§ù **Envolva stakeholders** - Decis√µes de fairness s√£o sociot√©cnicas
4. üìù **Documente tudo** - Transpar√™ncia √© fundamental
5. ‚öñÔ∏è **Trade-offs s√£o inevit√°veis** - Balance fairness, performance e complexidade

**Lembre-se**: Tecnologia sozinha n√£o resolve problemas de fairness. √â necess√°rio combinar ferramentas t√©cnicas com processos organizacionais, supervis√£o humana e governan√ßa adequada.

---

**Vers√£o**: 1.0
**√öltima atualiza√ß√£o**: 2025-11-03
**Autores**: DeepBridge Team
