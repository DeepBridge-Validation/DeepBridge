# FAQ: Fairness no DeepBridge

## üìã √çndice

- [Conceitos B√°sicos](#conceitos-b√°sicos)
- [Uso do M√≥dulo](#uso-do-m√≥dulo)
- [M√©tricas](#m√©tricas)
- [Interpreta√ß√£o](#interpreta√ß√£o)
- [Mitiga√ß√£o](#mitiga√ß√£o)
- [Quest√µes T√©cnicas](#quest√µes-t√©cnicas)
- [Quest√µes Legais](#quest√µes-legais)
- [Troubleshooting](#troubleshooting)

---

## Conceitos B√°sicos

### O que √© fairness em Machine Learning?

**Resposta**: Fairness em ML refere-se √† aus√™ncia de vi√©s ou discrimina√ß√£o injusta contra grupos ou indiv√≠duos baseado em atributos sens√≠veis (ra√ßa, g√™nero, idade, etc.). Um modelo "justo" toma decis√µes que n√£o discriminam sistematicamente contra grupos protegidos.

**Exemplo**: Um modelo de aprova√ß√£o de cr√©dito que aprova homens a uma taxa significativamente maior que mulheres, mesmo quando t√™m perfis financeiros similares, √© considerado injusto.

---

### Qual a diferen√ßa entre bias e fairness?

**Resposta**:
- **Bias (Vi√©s)**: Desvio sistem√°tico que favorece ou prejudica certos grupos. Pode ser estat√≠stico ou social.
- **Fairness**: Conceito normativo sobre o que √© "justo". Fairness busca mitigar bias social/discriminat√≥rio.

**Exemplo**: Um modelo pode ter bias estat√≠stico (regulariza√ß√£o) sem problemas de fairness. Mas bias social (prever que mulheres s√£o piores programadoras) √© uma quest√£o de fairness.

---

### Por que simplesmente remover atributos sens√≠veis n√£o funciona?

**Resposta**: Porque existem **proxies** - features correlacionadas com atributos sens√≠veis que permitem ao modelo "inferir" informa√ß√µes protegidas.

**Exemplo**:
- Remover "ra√ßa" do dataset
- Mas manter "CEP" (zip code)
- CEP √© altamente correlacionado com ra√ßa nos EUA devido a segrega√ß√£o hist√≥rica
- Modelo usa CEP como proxy para ra√ßa

**Solu√ß√£o**: An√°lise de correla√ß√µes + t√©cnicas de mitiga√ß√£o espec√≠ficas.

---

### Fairness vs. Acur√°cia: Sempre h√° trade-off?

**Resposta**: **Nem sempre**, mas frequentemente sim.

**Quando N√ÉO h√° trade-off**:
- Se o vi√©s vem de dados ruins/enviesados, corrigir os dados pode AUMENTAR acur√°cia E fairness
- Se o modelo est√° overfitting em correla√ß√µes esp√∫rias

**Quando H√Å trade-off**:
- Quando a distribui√ß√£o real tem diferen√ßas entre grupos
- Quando otimizar para uma defini√ß√£o de fairness prejudica outra

**Recomenda√ß√£o**: Sempre medir o trade-off. Pequenas perdas em acur√°cia (1-2%) geralmente s√£o aceit√°veis para ganhos significativos em fairness.

---

## Uso do M√≥dulo

### Como come√ßar a usar o m√≥dulo de Fairness?

**Resposta**: H√° duas formas principais:

#### 1. Via Experiment (Recomendado - Mais Simples)

```python
from deepbridge.core.db_data import DBDataset
from deepbridge.core.experiment.experiment import Experiment

# Criar dataset
dataset = DBDataset(data=df, target_column='target', model=model)

# Criar experiment
experiment = Experiment(
    dataset=dataset,
    experiment_type="binary_classification",
    tests=["fairness"],
    protected_attributes=['gender', 'race']
)

# Executar testes
result = experiment.run_fairness_tests(config='full')

# Gerar relat√≥rio
result.save_html('fairness_report.html', model_name='My Model')
```

#### 2. Via FairnessSuite (Avan√ßado - Mais Controle)

```python
from deepbridge.validation.wrappers import FairnessSuite

# Criar suite
fairness = FairnessSuite(dataset, protected_attributes=['gender', 'race'])

# Executar com configura√ß√£o espec√≠fica
results = fairness.config('full').run()

# Gerar relat√≥rio manualmente
from deepbridge.core.experiment.report.report_manager import ReportManager
report_manager = ReportManager()
report_manager.generate_report(
    test_type='fairness',
    results=results,
    file_path='fairness_report.html'
)
```

---

### Qual configura√ß√£o devo usar: quick, medium ou full?

**Resposta**: Depende do est√°gio do projeto:

| Configura√ß√£o | Quando Usar | Tempo | M√©tricas |
|--------------|-------------|-------|----------|
| **quick** | Explora√ß√£o inicial, prot√≥tipos, testes r√°pidos | 10-30s | 2 m√©tricas |
| **medium** | Valida√ß√£o intermedi√°ria, desenvolvimento | 1-3min | 5 p√≥s + 4 pr√© + CM |
| **full** | Auditoria final, produ√ß√£o, compliance | 5-10min | 11 p√≥s + 4 pr√© + CM + threshold |

**Recomenda√ß√£o**:
- Desenvolvimento: `quick` ‚Üí `medium` ‚Üí `full`
- Produ√ß√£o: Sempre `full`
- Monitoramento cont√≠nuo: `medium`

---

### Como especificar atributos protegidos?

**Resposta**:

#### Op√ß√£o 1: Expl√≠cito (RECOMENDADO para produ√ß√£o)

```python
experiment = Experiment(
    dataset=dataset,
    experiment_type="binary_classification",
    tests=["fairness"],
    protected_attributes=['gender', 'race', 'age_group']  # Lista expl√≠cita
)
```

#### Op√ß√£o 2: Auto-detec√ß√£o (Apenas para explora√ß√£o)

```python
experiment = Experiment(
    dataset=dataset,
    experiment_type="binary_classification",
    tests=["fairness"]  # N√£o especifica - detecta automaticamente
)
# Auto-detecta usando fuzzy matching: 'gender', 'race', 'age', etc.
```

**‚ö†Ô∏è Warning**: Auto-detec√ß√£o √© conveniente mas pode:
- Detectar atributos incorretos
- Perder atributos sens√≠veis n√£o-√≥bvios
- N√£o ser determin√≠stico entre execu√ß√µes

**Para produ√ß√£o**: SEMPRE especifique explicitamente.

---

### Posso usar com modelos de regress√£o ou multiclass?

**Resposta**: **Atualmente, apenas classifica√ß√£o bin√°ria est√° suportada** (Fase 1-6 focou em binary classification).

**Suporte planejado futuro**:
- ‚úÖ Classifica√ß√£o Bin√°ria (dispon√≠vel)
- üîú Classifica√ß√£o Multiclass (planejado)
- üîú Regress√£o (planejado - m√©tricas diferentes)

**Workaround para multiclass**:
- Converter para one-vs-rest (m√∫ltiplas an√°lises bin√°rias)
- Avaliar cada classe separadamente

---

## M√©tricas

### Quantas m√©tricas de fairness existem?

**Resposta**: O DeepBridge implementa **15 m√©tricas** divididas em:

**Pr√©-treino** (4 - independentes do modelo):
1. Class Balance (BCL)
2. Concept Balance (BCE)
3. KL Divergence
4. JS Divergence

**P√≥s-treino** (11 - dependentes do modelo):
1. Statistical Parity (PE)
2. Disparate Impact (ID)
3. Equal Opportunity (IO)
4. Equalized Odds (CP)
5. False Negative Rate Difference (TFN)
6. Conditional Acceptance (TAC)
7. Conditional Rejection (TRJ)
8. Precision Difference (DP)
9. Accuracy Difference (DA)
10. Treatment Equality (IT)
11. Entropy Index (IE)

---

### Qual a diferen√ßa entre Statistical Parity e Equal Opportunity?

**Resposta**:

#### Statistical Parity (Paridade Estat√≠stica)
- **F√≥rmula**: P(≈∂=1 | A=a) - P(≈∂=1 | A=b)
- **Significado**: Taxa de predi√ß√µes positivas deve ser igual entre grupos
- **Foco**: Resultados iguais (outcome-based)
- **Exemplo**: 50% homens aprovados ‚Üí 50% mulheres aprovadas

#### Equal Opportunity (Oportunidade Igual)
- **F√≥rmula**: TPR(A=a) - TPR(A=b)
- **Significado**: Entre indiv√≠duos QUALIFICADOS, taxa de aprova√ß√£o deve ser igual
- **Foco**: Igualdade para qualificados (merit-based)
- **Exemplo**: Dos homens QUE DEVEM ser aprovados, 80% s√£o ‚Üí Das mulheres QUE DEVEM ser aprovadas, 80% s√£o

**Quando usar**:
- **Statistical Parity**: Quando queremos resultados proporcionais (recrutamento, admiss√µes)
- **Equal Opportunity**: Quando queremos igualdade para qualificados (cr√©dito, promo√ß√µes)

---

### O que √© Disparate Impact e por que √© importante?

**Resposta**:

**Defini√ß√£o**: Raz√£o entre a taxa de outcomes positivos do grupo desfavorecido e do grupo favorecido.

**F√≥rmula**:
```
Disparate Impact = P(≈∂=1 | A=desfavorecido) / P(≈∂=1 | A=favorecido)
```

**Exemplo**:
- 60% dos homens aprovados
- 45% das mulheres aprovadas
- Disparate Impact = 45% / 60% = 0.75

**Import√¢ncia Legal**:
- **EEOC 80% Rule** (EUA): Disparate Impact < 0.80 √© evid√™ncia prima facie de discrimina√ß√£o
- Usado em processos de emprego, cr√©dito, habita√ß√£o

**Interpreta√ß√£o DeepBridge**:
- ‚úÖ Verde: ‚â• 0.80 (EEOC compliant)
- ‚ö†Ô∏è Amarelo: 0.70 - 0.79 (aten√ß√£o)
- ‚ùå Vermelho: < 0.70 (cr√≠tico)

---

### Como s√£o calculadas as m√©tricas pr√©-treino?

**Resposta**: M√©tricas pr√©-treino analisam APENAS os dados, sem considerar o modelo.

#### Class Balance (BCL)
```python
# Diferen√ßa na propor√ß√£o de classes por grupo
P(Y=1 | A=male) - P(Y=1 | A=female)
```

#### Concept Balance (BCE)
```python
# Diferen√ßa nas features m√©dias entre grupos (classe positiva)
mean(X | Y=1, A=male) - mean(X | Y=1, A=female)
```

#### KL/JS Divergence
```python
# Diverg√™ncia entre distribui√ß√µes de features por grupo
KL(P(X|A=male) || P(X|A=female))
JS = 0.5 * KL(P||M) + 0.5 * KL(Q||M)  # M = m√©dia
```

**Utilidade**:
- Detectar vi√©s NOS DADOS antes do treinamento
- Independente do modelo
- √ötil para diagn√≥stico inicial

---

### O que significa "Threshold Analysis"?

**Resposta**: An√°lise de como diferentes thresholds de decis√£o afetam fairness e performance.

**Como funciona**:
1. Modelo produz probabilidades: `P(Y=1) = 0.65`
2. Threshold converte em decis√£o: `if P(Y=1) >= 0.5 then ≈∂=1`
3. Threshold Analysis testa 99 valores (0.01 a 0.99)
4. Para cada threshold, calcula: Disparate Impact, Statistical Parity, F1 Score

**Resultado**: Threshold √≥timo que maximiza fairness (ou balance com F1)

**Exemplo**:
```
Threshold padr√£o: 0.50
- Disparate Impact: 0.72 ‚ùå
- F1 Score: 0.82

Threshold √≥timo: 0.42
- Disparate Impact: 0.81 ‚úÖ
- F1 Score: 0.80 (perda de 2%)
```

**Quando usar**: Quando voc√™ pode aceitar pequena perda de performance para ganho significativo em fairness.

---

## Interpreta√ß√£o

### O que √© Overall Fairness Score?

**Resposta**: M√©trica agregada (0-1) que resume fairness geral do modelo.

**C√°lculo**:
```python
# M√©dia ponderada de:
# 1. M√©tricas pr√©-treino normalizadas
# 2. M√©tricas p√≥s-treino normalizadas
# 3. Penalidade por critical issues

score = (
    0.3 * pretrain_score +
    0.7 * posttrain_score -
    0.05 * num_critical_issues
)
```

**Interpreta√ß√£o**:
- **0.90-1.00**: ‚úÖ Excelente
- **0.80-0.89**: ‚úì Boa
- **0.70-0.79**: ‚ö†Ô∏è Moderada
- **< 0.70**: ‚ùå Cr√≠tica

**Limita√ß√µes**:
- Score √∫nico esconde nuances
- Sempre revisar m√©tricas individuais
- Considerar contexto espec√≠fico

---

### Meu modelo tem score 0.65. Posso coloc√°-lo em produ√ß√£o?

**Resposta**: **N√ÉO recomendado** sem mitiga√ß√µes.

**Score 0.65 indica**:
- Problemas significativos de fairness
- Provavelmente m√∫ltiplos critical issues
- Risco legal e reputacional

**Pr√≥ximos passos**:
1. Revisar `critical_issues` e `warnings`
2. Identificar m√©tricas espec√≠ficas problem√°ticas
3. Aplicar t√©cnicas de mitiga√ß√£o
4. Re-treinar e re-avaliar
5. S√≥ deploy quando score ‚â• 0.80

**Exce√ß√µes** (com documenta√ß√£o legal):
- Contexto de baixo risco (recomenda√ß√µes n√£o-cr√≠ticas)
- Supervis√£o humana obrigat√≥ria
- Plano claro de melhoria cont√≠nua

---

### Como interpretar "Confusion Matrix por Grupo"?

**Resposta**: Mostra a matriz de confus√£o separadamente para cada grupo demogr√°fico.

**Exemplo**:

| | Male | | Female | |
|---|---|---|---|---|
| | Pred 0 | Pred 1 | Pred 0 | Pred 1 |
| **Real 0** | 850 (TN) | 50 (FP) | 420 (TN) | 80 (FP) |
| **Real 1** | 30 (FN) | 70 (TP) | 40 (FN) | 60 (TP) |

**M√©tricas derivadas**:
- **Male**: TPR = 70/(70+30) = 70%, FPR = 50/(850+50) = 5.6%
- **Female**: TPR = 60/(60+40) = 60%, FPR = 80/(420+80) = 16%

**Insights**:
- Modelo tem MENOR TPR para mulheres (60% vs 70%) ‚Üí Perde mais mulheres qualificadas
- Modelo tem MAIOR FPR para mulheres (16% vs 5.6%) ‚Üí Aprova mais mulheres n√£o-qualificadas erroneamente

**A√ß√£o**: Investigar por que modelo performa diferente por grupo.

---

## Mitiga√ß√£o

### Quais t√©cnicas posso usar para mitigar vi√©s?

**Resposta**: T√©cnicas dividem-se em 3 categorias:

#### 1. Pr√©-processamento (Antes do Treinamento)

**Re-balanceamento**:
```python
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)
```

**Reweighting**:
```python
from sklearn.utils.class_weight import compute_sample_weight
weights = compute_sample_weight('balanced', y)
model.fit(X, y, sample_weight=weights)
```

**Remo√ß√£o de proxies**:
```python
# Identificar features correlacionadas
high_corr = ['zip_code', 'first_name']
X_clean = X.drop(columns=high_corr)
```

---

#### 2. In-processing (Durante o Treinamento)

**Fairness Constraints** (via Fairlearn):
```python
from fairlearn.reductions import ExponentiatedGradient, DemographicParity

mitigator = ExponentiatedGradient(
    estimator=base_model,
    constraints=DemographicParity()
)
mitigator.fit(X, y, sensitive_features=df['gender'])
```

**Adversarial Debiasing** (via AIF360):
```python
from aif360.algorithms.inprocessing import AdversarialDebiasing
model = AdversarialDebiasing(...)
model.fit(dataset)
```

---

#### 3. P√≥s-processamento (Ap√≥s o Treinamento)

**Threshold Optimization**:
```python
# Usar threshold analysis do DeepBridge
result = experiment.run_fairness_tests(config='full')
optimal_threshold = result._results['threshold_analysis']['optimal_threshold']

# Aplicar threshold
y_pred = (model.predict_proba(X)[:, 1] >= optimal_threshold).astype(int)
```

**Calibra√ß√£o por Grupo**:
```python
from sklearn.calibration import CalibratedClassifierCV

# Calibrar separadamente
for group in groups:
    group_data = X[df['gender'] == group]
    calibrated = CalibratedClassifierCV(model, cv=5)
    calibrated.fit(group_data, y[df['gender'] == group])
```

**Recomenda√ß√£o**: Comece com pr√©-processamento (mais simples), depois tente in-processing se necess√°rio.

---

### Aplicar t√©cnicas de mitiga√ß√£o sempre melhora fairness?

**Resposta**: **N√£o necessariamente** - pode ter efeitos colaterais.

**Poss√≠veis problemas**:
1. **Redu√ß√£o de performance geral**: Acur√°cia pode cair 5-10%
2. **Fairness em uma m√©trica, bias em outra**: Melhorar Statistical Parity pode piorar Equal Opportunity
3. **Overfitting**: Re-balanceamento excessivo pode causar overfitting no grupo minorit√°rio

**Recomenda√ß√µes**:
- Sempre avaliar ANTES e DEPOIS
- Validar em conjunto de teste independente
- Medir trade-offs explicitamente
- Documentar decis√µes

**Pipeline recomendado**:
```python
# 1. Baseline
baseline_result = experiment.run_fairness_tests(config='full')

# 2. Aplicar mitiga√ß√£o
# [seu c√≥digo de mitiga√ß√£o]

# 3. Re-avaliar
mitigated_result = experiment.run_fairness_tests(config='full')

# 4. Comparar
print(f"Baseline: {baseline_result.overall_fairness_score:.3f}")
print(f"Mitigated: {mitigated_result.overall_fairness_score:.3f}")
print(f"Acur√°cia Baseline: {baseline_acc:.3f}")
print(f"Acur√°cia Mitigada: {mitigated_acc:.3f}")
```

---

## Quest√µes T√©cnicas

### Posso usar com qualquer tipo de modelo?

**Resposta**: **Sim**, desde que o modelo tenha interface sklearn-compatible.

**Modelos suportados**:
- ‚úÖ Scikit-learn (RandomForest, LogisticRegression, SVM, etc.)
- ‚úÖ XGBoost
- ‚úÖ LightGBM
- ‚úÖ CatBoost
- ‚úÖ Redes Neurais (Keras/TensorFlow/PyTorch com wrapper sklearn)

**Requisitos**:
1. M√©todo `predict(X)` que retorna classes
2. (Opcional) M√©todo `predict_proba(X)` para threshold analysis

**Exemplo com XGBoost**:
```python
import xgboost as xgb

# Treinar XGBoost
model = xgb.XGBClassifier(...)
model.fit(X_train, y_train)

# Usar com DeepBridge (funciona diretamente)
dataset = DBDataset(data=df, target_column='target', model=model)
experiment = Experiment(
    dataset=dataset,
    experiment_type="binary_classification",
    tests=["fairness"],
    protected_attributes=['gender']
)
```

---

### Como lidar com m√∫ltiplos atributos protegidos?

**Resposta**: DeepBridge analisa cada atributo **separadamente**.

**Exemplo**:
```python
protected_attributes = ['gender', 'race', 'age_group']
```

**An√°lise produzida**:
- M√©tricas para `gender` (comparando Male vs Female)
- M√©tricas para `race` (comparando White vs Black vs Hispanic vs Asian)
- M√©tricas para `age_group` (comparando Young vs Adult vs Middle-Aged vs Senior)

**Limita√ß√£o atual**: N√£o analisa **interse√ß√µes** (ex: Mulheres Negras vs Homens Brancos).

**Workaround para interse√ß√µes**:
```python
# Criar atributo combinado
df['gender_race'] = df['gender'] + '_' + df['race']
# Resultado: 'Male_White', 'Female_Black', etc.

# Analisar interse√ß√£o
protected_attributes = ['gender_race']
```

---

### Quanto tempo demora a an√°lise?

**Resposta**: Depende de:
1. Configura√ß√£o (quick/medium/full)
2. Tamanho do dataset
3. N√∫mero de atributos protegidos
4. N√∫mero de grupos por atributo

**Benchmarks t√≠picos**:

| Dataset | Config | Atributos | Tempo |
|---------|--------|-----------|-------|
| 1K samples | quick | 2 | ~5s |
| 1K samples | medium | 2 | ~30s |
| 1K samples | full | 2 | ~2min |
| 10K samples | full | 3 | ~5min |
| 100K samples | full | 3 | ~15min |

**Componente mais lento**: Threshold Analysis (testa 99 thresholds)

**Dica de performance**:
```python
# Para datasets grandes, use amostragem
import numpy as np

sample_size = 10000
sample_idx = np.random.choice(len(df), sample_size, replace=False)
df_sample = df.iloc[sample_idx]

# An√°lise na amostra (muito mais r√°pido)
dataset = DBDataset(data=df_sample, ...)
```

---

### Os resultados s√£o determin√≠sticos?

**Resposta**: **Sim**, se voc√™ controlar seeds.

**Fontes de aleatoriedade**:
1. Split train/test no Experiment
2. Modelo treinado (se usar random_state)
3. Re-balanceamento de dados (SMOTE, etc.)

**Como garantir reprodutibilidade**:
```python
import numpy as np
import random

# Fixar seeds
np.random.seed(42)
random.seed(42)

# Usar random_state no Experiment
experiment = Experiment(
    dataset=dataset,
    experiment_type="binary_classification",
    tests=["fairness"],
    protected_attributes=['gender'],
    random_state=42  # Importante!
)

# Usar random_state no modelo
model = RandomForestClassifier(random_state=42)
```

**Com seeds fixadas**: Resultados s√£o 100% reprodut√≠veis.

---

## Quest√µes Legais

### Meu modelo √© EEOC compliant?

**Resposta**: Verifique a m√©trica **Disparate Impact**.

**Regra EEOC 80%**:
- Se Disparate Impact ‚â• 0.80 para TODOS os atributos protegidos ‚Üí **Provavelmente compliant**
- Se Disparate Impact < 0.80 para QUALQUER atributo ‚Üí **Risco de viola√ß√£o**

**No relat√≥rio DeepBridge**:
```
Disparate Impact (ID)
  Valor: 0.85
  Interpreta√ß√£o: ‚úì Verde (OK)
  EEOC: 0.80 (atende)
```

**‚ö†Ô∏è IMPORTANTE**:
- EEOC 80% √© uma **heur√≠stica**, n√£o garantia legal
- Compliance real depende de contexto, jurisdi√ß√£o, documenta√ß√£o
- **Sempre consulte advogado** antes de decis√µes legais

**Documenta√ß√£o recomendada**:
1. Salvar relat√≥rio HTML de cada an√°lise
2. Manter logs de todas as decis√µes
3. Documentar justificativas t√©cnicas
4. Revisar com equipe legal

---

### Quais atributos s√£o legalmente protegidos?

**Resposta**: Varia por jurisdi√ß√£o.

#### Estados Unidos (EEOC)
- Race (ra√ßa)
- Color (cor)
- Religion (religi√£o)
- Sex (sexo/g√™nero)
- National Origin (origem nacional)
- Age (40+) (idade)
- Disability (defici√™ncia)
- Genetic Information (informa√ß√£o gen√©tica)

#### Europa (GDPR - Artigo 9)
- Racial/ethnic origin
- Political opinions
- Religious/philosophical beliefs
- Trade union membership
- Genetic data
- Biometric data
- Health data
- Sex life/sexual orientation

#### Brasil (LGPD + Constitui√ß√£o)
- Origem racial/√©tnica
- Convic√ß√£o religiosa
- Opini√£o pol√≠tica
- Filia√ß√£o sindical
- Dados gen√©ticos
- Dados de sa√∫de
- Orienta√ß√£o sexual
- Dados biom√©tricos

**Recomenda√ß√£o**: Consultar legisla√ß√£o local + advogado especializado.

---

### Posso processar dados sens√≠veis para an√°lise de fairness?

**Resposta**: **Depende da jurisdi√ß√£o e consentimento**.

#### GDPR (Europa)
- Art. 9: Processamento de dados sens√≠veis √© **proibido por padr√£o**
- **Exce√ß√µes**: Consentimento expl√≠cito, obriga√ß√£o legal, interesse p√∫blico substancial
- **Para fairness**: Geralmente permitido sob "interesse p√∫blico substancial" (prevenir discrimina√ß√£o)
- **Requisitos**: Documentar necessidade, minimizar dados, garantir seguran√ßa

#### LGPD (Brasil)
- Art. 11: Dados sens√≠veis requerem **consentimento espec√≠fico**
- **Exce√ß√µes**: Cumprimento de obriga√ß√£o legal, exerc√≠cio regular de direitos
- **Para fairness**: Argumento de "preven√ß√£o de discrimina√ß√£o" pode se aplicar
- **Requisitos**: Base legal clara, relat√≥rio de impacto

**Recomenda√ß√µes pr√°ticas**:
1. **Obter consentimento** quando poss√≠vel
2. **Minimizar dados**: Apenas atributos necess√°rios
3. **Anonimizar**: Remover identificadores diretos
4. **Documentar**: Justificar necessidade de cada atributo
5. **Limitar acesso**: Apenas equipes autorizadas
6. **Auditoria**: Manter logs de acesso

---

## Troubleshooting

### Erro: "No protected attributes detected"

**Problema**: Auto-detec√ß√£o n√£o encontrou atributos sens√≠veis.

**Causas**:
1. Dataset n√£o tem colunas com nomes √≥bvios ('gender', 'race', etc.)
2. Nomes das colunas muito diferentes dos keywords
3. Atributos protegidos codificados numericamente (0/1 ao inv√©s de 'Male'/'Female')

**Solu√ß√£o**:
```python
# Op√ß√£o 1: Especificar explicitamente
protected_attributes = ['column_x', 'column_y']  # Use os nomes reais

# Op√ß√£o 2: Renomear colunas
df = df.rename(columns={'column_x': 'gender', 'column_y': 'race'})

# Op√ß√£o 3: Ajustar threshold de fuzzy matching
detected = Experiment.detect_sensitive_attributes(dataset, threshold=0.5)  # Mais permissivo
```

---

### Erro: "Feature names mismatch"

**Problema**: Modelo foi treinado com features diferentes das fornecidas.

**Causa comum**: Treinou modelo SEM atributos protegidos, mas dataset inclui atributos protegidos.

**Exemplo do erro**:
```
Modelo treinado com: ['income', 'credit_score', 'debt_ratio']
Dataset fornecido com: ['income', 'credit_score', 'debt_ratio', 'gender', 'race']
```

**Solu√ß√£o**:
```python
# Garantir que DBDataset usa mesmas features do treinamento
feature_cols = ['income', 'credit_score', 'debt_ratio']  # SEM atributos protegidos

# Separar features para predi√ß√£o
X = df[feature_cols]

# Mas manter atributos protegidos no DataFrame completo
dataset = DBDataset(data=df, target_column='target', model=model)
# DeepBridge internamente filtra features corretas
```

---

### Warning: "For production, explicitly specify protected_attributes"

**Problema**: Voc√™ est√° usando auto-detec√ß√£o.

**Significado**: DeepBridge detectou atributos automaticamente mas n√£o √© recomendado para produ√ß√£o.

**Solu√ß√£o**:
```python
# ANTES (auto-detec√ß√£o)
experiment = Experiment(
    dataset=dataset,
    tests=["fairness"]
)

# DEPOIS (expl√≠cito)
experiment = Experiment(
    dataset=dataset,
    tests=["fairness"],
    protected_attributes=['gender', 'race']  # Adicionar explicitamente
)
```

---

### Overall Fairness Score muito baixo mas visual parece OK

**Problema**: Score agregado pode ser enganoso.

**Causa**: Score penaliza QUALQUER m√©trica cr√≠tica, mesmo se maioria est√° OK.

**Exemplo**:
```
Overall Score: 0.65 (parece cr√≠tico)

M√©tricas individuais:
- Statistical Parity: ‚úì 0.08 (OK)
- Equal Opportunity: ‚úì 0.06 (OK)
- Equalized Odds: ‚úì 0.09 (OK)
- Disparate Impact: ‚úó 0.65 (cr√≠tico!) <- Puxa score para baixo
```

**Solu√ß√£o**:
1. **N√£o confiar apenas no Overall Score**
2. Revisar `critical_issues` para identificar problema espec√≠fico
3. Focar mitiga√ß√£o na m√©trica problem√°tica
4. Considerar contexto (algumas m√©tricas s√£o mais importantes que outras)

---

### Relat√≥rio HTML n√£o abre / caracteres estranhos

**Problema**: Encoding UTF-8 n√£o reconhecido.

**Solu√ß√£o**:
```python
# Ao salvar, garantir encoding
result.save_html('report.html', model_name='Model')

# Ao abrir manualmente, especificar encoding
with open('report.html', 'r', encoding='utf-8') as f:
    content = f.read()
```

**Navegadores**: Todos os navegadores modernos (Chrome, Firefox, Safari, Edge) suportam UTF-8 por padr√£o.

---

### An√°lise muito lenta (> 20 minutos)

**Problema**: Dataset muito grande ou muitos atributos/grupos.

**Solu√ß√µes**:

#### 1. Usar configura√ß√£o mais leve
```python
# Ao inv√©s de 'full', usar 'medium' ou 'quick'
result = experiment.run_fairness_tests(config='medium')
```

#### 2. Amostragem estratificada
```python
from sklearn.model_selection import train_test_split

# Amostrar 10K samples mantendo propor√ß√µes
df_sample, _ = train_test_split(
    df,
    train_size=10000,
    stratify=df['target'],
    random_state=42
)

dataset = DBDataset(data=df_sample, ...)
```

#### 3. Reduzir atributos protegidos
```python
# Analisar um atributo por vez
for attr in ['gender', 'race', 'age_group']:
    result = experiment.run_fairness_tests(
        protected_attributes=[attr],  # Um por vez
        config='full'
    )
    result.save_html(f'fairness_{attr}.html')
```

---

## Recursos Adicionais

### Onde encontrar mais informa√ß√µes?

- **Documenta√ß√£o Completa**: `docs/FAIRNESS_BEST_PRACTICES.md`
- **Tutorial Passo-a-Passo**: `docs/FAIRNESS_TUTORIAL.md`
- **Exemplo Completo**: `examples/fairness_complete_example.py`
- **C√≥digo Fonte**: `deepbridge/validation/fairness/`

### Bibliotecas Complementares

- **AIF360** (IBM): https://github.com/Trusted-AI/AIF360
- **Fairlearn** (Microsoft): https://fairlearn.org/
- **What-If Tool** (Google): https://pair-code.github.io/what-if-tool/

### Artigos Acad√™micos Recomendados

1. **"Fairness and Machine Learning"** - Barocas, Hardt, Narayanan (2019)
2. **"A Survey on Bias and Fairness in Machine Learning"** - Mehrabi et al. (2021)
3. **"Fairness Definitions Explained"** - Verma & Rubin (2018)

---

## Ainda tem d√∫vidas?

**Reporte issues**: https://github.com/[seu-repo]/DeepBridge/issues
**Contribua**: Pull requests s√£o bem-vindos!

---

**Vers√£o**: 1.0
**√öltima atualiza√ß√£o**: 2025-11-03
