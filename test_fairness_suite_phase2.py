"""
Script de teste para validar a Fase 2 do FairnessSuite.

Este script testa:
- Config 'quick': 2 m√©tricas p√≥s-treino
- Config 'medium': 5 p√≥s-treino + 4 pr√©-treino + confusion matrix
- Config 'full': 11 p√≥s-treino + 4 pr√©-treino + confusion matrix + threshold analysis
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

from deepbridge.core.db_data import DBDataset
from deepbridge.validation.wrappers.fairness_suite import FairnessSuite

print("=" * 80)
print("TESTE DA FASE 2 - FAIRNESS SUITE EXPANDIDO")
print("=" * 80)

# ============================================================================
# 1. CRIAR DADOS SINT√âTICOS COM VI√âS
# ============================================================================

print("\nüìä 1. Gerando dados sint√©ticos com vi√©s...")

np.random.seed(42)
n_samples = 1000

# Criar dados com features num√©ricas
X_numeric = np.random.randn(n_samples, 5)

# Criar atributos protegidos
gender = np.random.choice(['M', 'F'], n_samples, p=[0.7, 0.3])
race = np.random.choice(['White', 'Black', 'Hispanic'], n_samples, p=[0.6, 0.25, 0.15])

# Criar target com vi√©s (mais positivos para grupo privilegiado)
y = np.zeros(n_samples)
for i in range(n_samples):
    base_prob = 0.3

    # Adicionar vi√©s por g√™nero
    if gender[i] == 'M':
        base_prob += 0.15  # Vi√©s favorecendo homens

    # Adicionar vi√©s por ra√ßa
    if race[i] == 'White':
        base_prob += 0.10  # Vi√©s favorecendo brancos

    y[i] = 1 if np.random.rand() < base_prob else 0

# Criar DataFrame completo
df = pd.DataFrame(X_numeric, columns=[f'feature_{i}' for i in range(5)])
df['gender'] = gender
df['race'] = race
df['target'] = y

print(f"  Total de amostras: {len(df)}")
print(f"  Distribui√ß√£o de g√™nero: M={np.sum(gender=='M')}, F={np.sum(gender=='F')}")
print(f"  Distribui√ß√£o de ra√ßa: White={np.sum(race=='White')}, Black={np.sum(race=='Black')}, Hispanic={np.sum(race=='Hispanic')}")
print(f"  Taxa positiva geral: {np.mean(y):.3f}")
print(f"  Taxa positiva (M): {np.mean(y[gender=='M']):.3f}")
print(f"  Taxa positiva (F): {np.mean(y[gender=='F']):.3f}")

# ============================================================================
# 2. TREINAR MODELO
# ============================================================================

print("\nü§ñ 2. Treinando modelo RandomForest...")

# Separar features (sem atributos protegidos) e target
X_train = df.drop(['gender', 'race', 'target'], axis=1)
y_train = df['target']

model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=5)
model.fit(X_train, y_train)

print(f"  Acur√°cia no treino: {model.score(X_train, y_train):.3f}")

# Gerar predi√ß√µes para todo o dataset
y_pred = model.predict(X_train)
y_pred_proba = model.predict_proba(X_train)

print(f"  Predi√ß√µes geradas: {len(y_pred)}")

# ============================================================================
# 3. CRIAR DBDATASET
# ============================================================================

print("\nüì¶ 3. Criando DBDataset...")

# Criar DataFrame completo incluindo atributos protegidos e predi√ß√µes
df_with_preds = df.copy()
df_with_preds['prediction'] = y_pred
df_with_preds['proba_class_0'] = y_pred_proba[:, 0]
df_with_preds['proba_class_1'] = y_pred_proba[:, 1]

dataset = DBDataset(
    data=df_with_preds,
    target_column='target',
    model=model,
    train_predictions=pd.DataFrame({
        'prediction': y_pred,
        'proba_class_0': y_pred_proba[:, 0],
        'proba_class_1': y_pred_proba[:, 1]
    })
)

print(f"  Dataset shape: {df_with_preds.shape}")
print(f"  Protected attributes: gender, race")

# ============================================================================
# 4. TESTE CONFIG 'QUICK'
# ============================================================================

print("\n" + "=" * 80)
print("üìã TESTE 1: CONFIG 'QUICK' (2 m√©tricas p√≥s-treino)")
print("=" * 80)

fairness_quick = FairnessSuite(
    dataset=dataset,
    protected_attributes=['gender', 'race'],
    verbose=True
)

results_quick = fairness_quick.config('quick').run()

print("\n‚úÖ Resultados 'quick':")
print(f"  - Overall Score: {results_quick['overall_fairness_score']:.3f}")
print(f"  - Pr√©-treino calculado: {bool(results_quick['pretrain_metrics'])}")
print(f"  - P√≥s-treino calculado: {bool(results_quick['posttrain_metrics'])}")
print(f"  - Confusion matrix: {bool(results_quick['confusion_matrix'])}")
print(f"  - Threshold analysis: {results_quick['threshold_analysis'] is not None}")
print(f"  - Warnings: {len(results_quick['warnings'])}")
print(f"  - Critical: {len(results_quick['critical_issues'])}")

# Valida√ß√µes
assert not results_quick['pretrain_metrics'], "Quick n√£o deve ter pr√©-treino"
assert results_quick['posttrain_metrics'], "Quick deve ter p√≥s-treino"
assert not results_quick['confusion_matrix'], "Quick n√£o deve ter confusion matrix"
assert results_quick['threshold_analysis'] is None, "Quick n√£o deve ter threshold analysis"
print("‚úì Todas as valida√ß√µes do 'quick' passaram!")

# ============================================================================
# 5. TESTE CONFIG 'MEDIUM'
# ============================================================================

print("\n" + "=" * 80)
print("üìã TESTE 2: CONFIG 'MEDIUM' (5 p√≥s + 4 pr√© + confusion matrix)")
print("=" * 80)

fairness_medium = FairnessSuite(
    dataset=dataset,
    protected_attributes=['gender', 'race'],
    verbose=True
)

results_medium = fairness_medium.config('medium').run()

print("\n‚úÖ Resultados 'medium':")
print(f"  - Overall Score: {results_medium['overall_fairness_score']:.3f}")
print(f"  - Pr√©-treino calculado: {bool(results_medium['pretrain_metrics'])}")
print(f"  - P√≥s-treino calculado: {bool(results_medium['posttrain_metrics'])}")
print(f"  - Confusion matrix: {bool(results_medium['confusion_matrix'])}")
print(f"  - Threshold analysis: {results_medium['threshold_analysis'] is not None}")
print(f"  - Warnings: {len(results_medium['warnings'])}")
print(f"  - Critical: {len(results_medium['critical_issues'])}")

# Valida√ß√µes
assert results_medium['pretrain_metrics'], "Medium deve ter pr√©-treino"
assert results_medium['posttrain_metrics'], "Medium deve ter p√≥s-treino"
assert results_medium['confusion_matrix'], "Medium deve ter confusion matrix"
assert results_medium['threshold_analysis'] is None, "Medium n√£o deve ter threshold analysis"

# Validar que tem 4 m√©tricas pr√©-treino por atributo
for attr in ['gender', 'race']:
    assert attr in results_medium['pretrain_metrics']
    assert len(results_medium['pretrain_metrics'][attr]) == 4, \
        f"Deve ter 4 m√©tricas pr√©-treino para {attr}"

# Validar confusion matrix
for attr in ['gender', 'race']:
    assert attr in results_medium['confusion_matrix']
    cm = results_medium['confusion_matrix'][attr]
    for group in cm.keys():
        assert all(k in cm[group] for k in ['TP', 'FP', 'TN', 'FN', 'total'])

print("‚úì Todas as valida√ß√µes do 'medium' passaram!")

# ============================================================================
# 6. TESTE CONFIG 'FULL'
# ============================================================================

print("\n" + "=" * 80)
print("üìã TESTE 3: CONFIG 'FULL' (11 p√≥s + 4 pr√© + CM + threshold)")
print("=" * 80)

fairness_full = FairnessSuite(
    dataset=dataset,
    protected_attributes=['gender', 'race'],
    verbose=True
)

results_full = fairness_full.config('full').run()

print("\n‚úÖ Resultados 'full':")
print(f"  - Overall Score: {results_full['overall_fairness_score']:.3f}")
print(f"  - Pr√©-treino calculado: {bool(results_full['pretrain_metrics'])}")
print(f"  - P√≥s-treino calculado: {bool(results_full['posttrain_metrics'])}")
print(f"  - Confusion matrix: {bool(results_full['confusion_matrix'])}")
print(f"  - Threshold analysis: {results_full['threshold_analysis'] is not None}")
print(f"  - Warnings: {len(results_full['warnings'])}")
print(f"  - Critical: {len(results_full['critical_issues'])}")

# Valida√ß√µes
assert results_full['pretrain_metrics'], "Full deve ter pr√©-treino"
assert results_full['posttrain_metrics'], "Full deve ter p√≥s-treino"
assert results_full['confusion_matrix'], "Full deve ter confusion matrix"
assert results_full['threshold_analysis'] is not None, "Full deve ter threshold analysis"

# Validar threshold analysis
ta = results_full['threshold_analysis']
assert 'optimal_threshold' in ta
assert 'optimal_metrics' in ta
assert 'threshold_curve' in ta
assert 'recommendations' in ta
assert 0 < ta['optimal_threshold'] < 1, "Threshold deve estar entre 0 e 1"
assert len(ta['threshold_curve']) > 0, "Deve ter curva de threshold"

print(f"\nüìà Threshold Analysis:")
print(f"  - Optimal threshold: {ta['optimal_threshold']:.3f}")
print(f"  - Disparate Impact @ optimal: {ta['optimal_metrics']['disparate_impact_ratio']:.3f}")
print(f"  - F1 Score @ optimal: {ta['optimal_metrics']['f1_score']:.3f}")
print(f"  - Recommendations: {len(ta['recommendations'])}")
for rec in ta['recommendations']:
    print(f"    ‚Ä¢ {rec}")

print("\n‚úì Todas as valida√ß√µes do 'full' passaram!")

# ============================================================================
# 7. VALIDA√á√ÉO DAS NOVAS M√âTRICAS
# ============================================================================

print("\n" + "=" * 80)
print("üìä VALIDA√á√ÉO DAS NOVAS M√âTRICAS")
print("=" * 80)

# Validar que todas as 11 m√©tricas p√≥s-treino foram calculadas
expected_posttrain = [
    'statistical_parity', 'equal_opportunity', 'equalized_odds',
    'disparate_impact', 'false_negative_rate_difference',
    'conditional_acceptance', 'conditional_rejection',
    'precision_difference', 'accuracy_difference',
    'treatment_equality', 'entropy_index'
]

for attr in ['gender', 'race']:
    metrics_calculated = set(results_full['posttrain_metrics'][attr].keys())
    expected_set = set(expected_posttrain)

    print(f"\n{attr}:")
    print(f"  Esperadas: {len(expected_set)}")
    print(f"  Calculadas: {len(metrics_calculated)}")

    missing = expected_set - metrics_calculated
    if missing:
        print(f"  ‚ùå Faltando: {missing}")
    else:
        print(f"  ‚úÖ Todas as m√©tricas p√≥s-treino presentes")

    assert metrics_calculated == expected_set, f"M√©tricas faltando para {attr}: {missing}"

# Validar m√©tricas pr√©-treino
expected_pretrain = ['class_balance', 'concept_balance', 'kl_divergence', 'js_divergence']

for attr in ['gender', 'race']:
    pretrain_calculated = set(results_full['pretrain_metrics'][attr].keys())
    expected_set = set(expected_pretrain)

    print(f"\nPr√©-treino {attr}:")
    print(f"  Esperadas: {len(expected_set)}")
    print(f"  Calculadas: {len(pretrain_calculated)}")

    assert pretrain_calculated == expected_set, f"M√©tricas pr√©-treino faltando para {attr}"
    print(f"  ‚úÖ Todas as m√©tricas pr√©-treino presentes")

# ============================================================================
# 8. RESUMO FINAL
# ============================================================================

print("\n" + "=" * 80)
print("üéâ RESUMO FINAL - FASE 2")
print("=" * 80)

print("\n‚úÖ SUCESSOS:")
print("  ‚úì Config 'quick' funcionando (2 m√©tricas)")
print("  ‚úì Config 'medium' funcionando (5 p√≥s + 4 pr√© + CM)")
print("  ‚úì Config 'full' funcionando (11 p√≥s + 4 pr√© + CM + TA)")
print("  ‚úì Todas as 15 m√©tricas dispon√≠veis")
print("  ‚úì M√©tricas pr√©-treino implementadas")
print("  ‚úì Confusion matrix por grupo")
print("  ‚úì Threshold analysis funcional")
print("  ‚úì Sistema de warnings/critical expandido")
print("  ‚úì Overall fairness score v2")

print("\nüìä ESTAT√çSTICAS:")
print(f"  - Total de configs testados: 3")
print(f"  - M√©tricas pr√©-treino: {len(expected_pretrain)}")
print(f"  - M√©tricas p√≥s-treino: {len(expected_posttrain)}")
print(f"  - Total de m√©tricas: {len(expected_pretrain) + len(expected_posttrain)}")
print(f"  - Atributos protegidos testados: 2")
print(f"  - Threshold points analisados: {len(ta['threshold_curve'])}")

print("\n" + "=" * 80)
print("‚úÖ FASE 2 - TESTE COMPLETO PASSOU COM SUCESSO!")
print("=" * 80)
