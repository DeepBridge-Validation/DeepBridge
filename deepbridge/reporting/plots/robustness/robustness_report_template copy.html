<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Robustness Analysis Report</title>
    <script src="https://cdn.plot.ly/plotly-2.29.1.min.js"></script>
    <script>
        // Global function for tab switching - defined at the very top
        function showTab(tabId) {
            // Hide all tab contents
            document.querySelectorAll('.tab-content').forEach(content => {
                content.classList.remove('active');
            });
            
            // Deactivate all tabs
            document.querySelectorAll('.tab').forEach(tab => {
                tab.classList.remove('active');
            });
            
            // Show selected tab content
            let selectedContent = document.getElementById(tabId);
            if (selectedContent) {
                selectedContent.classList.add('active');
                
                // Find all plot containers in this tab and trigger a redraw
                let plots = selectedContent.querySelectorAll('.plot-container');
                if (plots.length > 0) {
                    setTimeout(() => {
                        window.dispatchEvent(new Event('resize'));
                    }, 50);
                }
            }
            
            // Activate selected tab
            document.querySelectorAll('.tab').forEach(tab => {
                if (tab.textContent.trim().toLowerCase().includes(tabId.toLowerCase())) {
                    tab.classList.add('active');
                }
            });
            
            // Trigger window resize to ensure plots render correctly
            window.dispatchEvent(new Event('resize'));
            return false;
        }
    </script>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        h1 {
            color: #2c3e50;
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 10px;
            border-bottom: 2px solid #eee;
        }
        h2 {
            color: #3498db;
            margin-top: 30px;
            padding-bottom: 5px;
            border-bottom: 1px solid #eee;
        }
        .summary {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 5px;
            margin-bottom: 30px;
            border-left: 5px solid #3498db;
        }
        .summary h2 {
            margin-top: 0;
            border-bottom: none;
        }
        .metric {
            font-weight: bold;
            margin-right: 5px;
        }
        .plot-container {
            margin: 30px 0;
            padding: 10px;
            background-color: white;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
            border-radius: 5px;
        }
        .two-column {
            display: flex;
            gap: 20px;
            flex-wrap: wrap;
        }
        .two-column > div {
            flex: 1;
            min-width: 400px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px 12px;
            text-align: left;
        }
        th {
            background-color: #f2f2f2;
            font-weight: bold;
        }
        tr:nth-child(even) {
            background-color: #f9f9f9;
        }
        .note {
            background-color: #f8f9fa;
            padding: 10px;
            border-left: 3px solid #ffc107;
            margin: 20px 0;
            font-size: 0.9em;
        }
        .tabs {
            display: flex;
            border-bottom: 1px solid #ddd;
            margin-bottom: 20px;
        }
        .tab {
            padding: 10px 20px;
            cursor: pointer;
            border: 1px solid transparent;
            border-bottom: none;
            background-color: #f1f1f1;
            margin-right: 5px;
            border-radius: 5px 5px 0 0;
        }
        .tab.active {
            background-color: white;
            border-color: #ddd;
            border-bottom: 1px solid white;
            margin-bottom: -1px;
        }
        .tab-content {
            display: none;
        }
        .tab-content.active {
            display: block;
        }
    </style>
</head>
<body>
    <h1>Robustness Analysis Report</h1>
    
    <div class="summary">
        <h2>Summary</h2>
        <div class="two-column">
            <div>
                <h3>Experiment Configuration</h3>
                <p><span class="metric">Primary Model:</span> {{ model_type }}</p>
                <p><span class="metric">Experiment Type:</span> {{ experiment_type }}</p>
                <p><span class="metric">Test Size:</span> {{ test_size }}</p>
                <p><span class="metric">Random State:</span> {{ random_state }}</p>
                {% if n_samples %}
                <p><span class="metric">Number of Samples:</span> {{ n_samples }}</p>
                {% endif %}
                {% if n_features %}
                <p><span class="metric">Number of Features:</span> {{ n_features }}</p>
                {% endif %}
            </div>
            <div>
                <h3>Robustness Results</h3>
                <p><span class="metric">Base Score:</span> {{ base_score|round(4) }}</p>
                <p><span class="metric">Overall Robustness Score:</span> {{ robustness_score|round(4) }}</p>
                <p><span class="metric">Average Impact - Raw Perturbation:</span> {{ avg_raw_impact|round(4) }}</p>
                <p><span class="metric">Average Impact - Quantile Perturbation:</span> {{ avg_quantile_impact|round(4) }}</p>
                <p><span class="metric">Number of Iterations:</span> {{ n_iterations }}</p>
                {% if feature_subset %}
                <p><span class="metric">Features Analyzed:</span> {{ feature_subset|join(', ') }}</p>
                {% endif %}
            </div>
        </div>
        
        <div class="alternative-models">
            <h3>Alternative Models</h3>
            {% if model_names and model_names|length > 0 %}
                <p>Testing against {{ model_names|length }} alternative models: {{ model_names|join(', ') }}</p>
            {% else %}
                <p>No alternative models specified for comparison.</p>
            {% endif %}
        </div>
    </div>

    <div class="tabs">
        <div class="tab active" onclick="showTab('raw')">Raw Perturbation</div>
        <div class="tab" onclick="showTab('quantile')">Quantile Perturbation</div>
        <div class="tab" onclick="showTab('models')">Models Comparison</div>
        <div class="tab" onclick="showTab('features')">Feature Analysis</div>
        <div class="tab" onclick="showTab('metrics')">Metrics Comparison</div>
        <div class="tab" onclick="showTab('detailed')">Detailed Results</div>
    </div>
    
    <script>
        // Make sure tabs work immediately on load
        document.addEventListener('DOMContentLoaded', function() {
            // Add click handlers to all tabs
            document.querySelectorAll('.tab').forEach(tab => {
                tab.addEventListener('click', function() {
                    // Extract the tab ID from the inner text
                    const tabText = this.textContent.trim().toLowerCase();
                    let tabId = '';
                    
                    if (tabText.includes('raw')) tabId = 'raw';
                    else if (tabText.includes('quantile')) tabId = 'quantile';
                    else if (tabText.includes('models')) tabId = 'models';
                    else if (tabText.includes('features')) tabId = 'features';
                    else if (tabText.includes('metrics')) tabId = 'metrics';
                    else if (tabText.includes('detailed')) tabId = 'detailed';
                    
                    if (tabId) showTab(tabId);
                });
            });
        });
    </script>

    <div id="raw" class="tab-content active">
        <h2>Raw Perturbation Analysis</h2>
        <div class="plot-container" id="raw-models-plot"></div>
        
        <h2>Raw Perturbation Distribution</h2>
        <div class="plot-container" id="raw-distribution-plot"></div>
    </div>

    <div id="quantile" class="tab-content">
        <h2>Quantile Perturbation Analysis</h2>
        <div class="plot-container" id="quantile-models-plot"></div>
        
        <h2>Quantile Perturbation Distribution</h2>
        <div class="plot-container" id="quantile-distribution-plot"></div>
    </div>
    
    <div id="models" class="tab-content">
        <h2>Comparison of Perturbation Methods</h2>
        <div class="plot-container" id="perturbation-methods-plot"></div>
        
        <h2>Model Robustness Comparison</h2>
        <div class="plot-container" id="model-comparison-plot"></div>
    </div>

    <div id="features" class="tab-content">
        <h2>Feature Importance for Robustness (Percentage)</h2>
        <div class="plot-container" id="feature-importance-plot"></div>
        
        {% if feature_subset %}
        <h2>Subset Feature Analysis</h2>
        <div class="plot-container" id="feature-subset-comparison-plot"></div>
        {% endif %}
    </div>
    
    <div id="metrics" class="tab-content">
        <h2>Model Metrics Comparison</h2>
        <div class="plot-container" id="metrics-comparison-plot"></div>
        
        <h2>Detailed Metrics</h2>
        
        {% if comparison_metrics_df is defined and comparison_metrics_df|length > 0 %}
        <!-- Use metrics from experiment.compare_all_models() if available -->
        <table>
            <tr>
                <th>Model</th>
                <th>Model Type</th>
                <th>Accuracy</th>
                <th>AUC</th>
                <th>F1 Score</th>
                <th>Precision</th>
                <th>Recall</th>
                <th>Robustness Score</th>
            </tr>
            {% for record in comparison_metrics_df %}
            <tr>
                <td>{{ record.model_name }}</td>
                <td>{{ record.model_type }}</td>
                <td>{{ record.accuracy|default(0)|round(4) }}</td>
                <td>{{ record.roc_auc|default(record.auc|default(record.base_score|default(0)))|round(4) }}</td>
                <td>{{ record.f1|default(0)|round(4) }}</td>
                <td>{{ record.precision|default(0)|round(4) }}</td>
                <td>{{ record.recall|default(0)|round(4) }}</td>
                <td>{{ record.robustness|default(
                    robustness_score if record.model_name == model_name 
                    else alt_metrics[loop.index0-1].robustness|default(0) if loop.index0 < alt_metrics|length 
                    else 0)|round(4) }}</td>
            </tr>
            {% endfor %}
        </table>
        {% elif all_model_metrics is defined and all_model_metrics|length > 0 %}
        <!-- Use all_model_metrics from compare_all_models() if available -->
        <table>
            <tr>
                <th>Model</th>
                <th>Accuracy</th>
                <th>AUC</th>
                <th>F1 Score</th>
                <th>Precision</th>
                <th>Recall</th>
                <th>Robustness Score</th>
            </tr>
            {% for model_name, metrics in all_model_metrics.items() %}
            <tr>
                <td>{{ model_name }}</td>
                <td>{{ metrics.accuracy|default(0)|round(4) }}</td>
                <td>{{ metrics.roc_auc|default(metrics.auc|default(metrics.base_score|default(0)))|round(4) }}</td>
                <td>{{ metrics.f1|default(0)|round(4) }}</td>
                <td>{{ metrics.precision|default(0)|round(4) }}</td>
                <td>{{ metrics.recall|default(0)|round(4) }}</td>
                <td>{{ metrics.robustness|default(
                       robustness_score if loop.index0 == 0 else 
                       alt_metrics[loop.index0-1].robustness|default(0) if loop.index0 <= alt_metrics|length 
                       else 0)|round(4) }}</td>
            </tr>
            {% endfor %}
        </table>
        {% else %}
        <!-- Fallback to traditional method -->
        <table>
            <tr>
                <th>Model</th>
                <th>Accuracy</th>
                <th>AUC</th>
                <th>F1 Score</th>
                <th>Precision</th>
                <th>Recall</th>
                <th>Robustness Score</th>
            </tr>
            <tr>
                <td>Primary Model</td>
                <td>{{ primary_metrics.accuracy|default(0)|round(4) }}</td>
                <td>{{ primary_metrics.roc_auc|default(primary_metrics.auc|default(base_score|default(0)))|round(4) }}</td>
                <td>{{ primary_metrics.f1|default(0)|round(4) }}</td>
                <td>{{ primary_metrics.precision|default(0)|round(4) }}</td>
                <td>{{ primary_metrics.recall|default(0)|round(4) }}</td>
                <td>{{ robustness_score|default(0)|round(4) }}</td>
            </tr>
            {% for model_name in model_names %}
            <tr>
                <td>{{ model_name }}</td>
                <td>{{ alt_metrics[loop.index0].accuracy|default(0)|round(4) }}</td>
                <td>{{ alt_metrics[loop.index0].roc_auc|default(alt_metrics[loop.index0].auc|default(
                       alternative_models[model_name].base_score|default(0)))|round(4) }}</td>
                <td>{{ alt_metrics[loop.index0].f1|default(0)|round(4) }}</td>
                <td>{{ alt_metrics[loop.index0].precision|default(0)|round(4) }}</td>
                <td>{{ alt_metrics[loop.index0].recall|default(0)|round(4) }}</td>
                <td>{{ alt_metrics[loop.index0].robustness|default(0)|round(4) }}</td>
            </tr>
            {% endfor %}
        </table>
        {% endif %}
    </div>

    <div id="detailed" class="tab-content">
        <div class="two-column">
            <div>
                <h2>Raw Perturbation Results</h2>
                <table>
                    <tr>
                        <th>Level</th>
                        <th>Mean Score</th>
                        <th>Std Dev</th>
                        <th>Worst Score</th>
                        <th>Impact</th>
                    </tr>
                    {% for level, data in raw_results %}
                    <tr>
                        <td>{{ level }}</td>
                        <td>{{ data.mean_score|round(4) }}</td>
                        <td>{{ data.std_score|round(4) }}</td>
                        <td>{{ data.worst_score|round(4) }}</td>
                        <td>{{ data.impact|round(4) }}</td>
                    </tr>
                    {% endfor %}
                </table>
            </div>
            
            <div>
                <h2>Quantile Perturbation Results</h2>
                <table>
                    <tr>
                        <th>Level</th>
                        <th>Mean Score</th>
                        <th>Std Dev</th>
                        <th>Worst Score</th>
                        <th>Impact</th>
                    </tr>
                    {% for level, data in quantile_results %}
                    <tr>
                        <td>{{ level }}</td>
                        <td>{{ data.mean_score|round(4) }}</td>
                        <td>{{ data.std_score|round(4) }}</td>
                        <td>{{ data.worst_score|round(4) }}</td>
                        <td>{{ data.impact|round(4) }}</td>
                    </tr>
                    {% endfor %}
                </table>
            </div>
        </div>
        
        <h2>Feature Importance Details (Percentage)</h2>
        <table>
            <tr>
                <th>Feature</th>
                <th>Importance (%)</th>
            </tr>
            {% for feature, importance in feature_importance_pct %}
            <tr>
                <td>{{ feature }}</td>
                <td>{{ importance|round(2) }}%</td>
            </tr>
            {% endfor %}
        </table>
    </div>

    <div class="note">
        <p><strong>Note:</strong> This report analyzes model robustness against data perturbations. Raw perturbation applies Gaussian noise proportional to feature standard deviation, while Quantile perturbation changes features based on their distribution quantiles.</p>
        <p>Importance values indicate how much each feature contributes to model robustness degradation when perturbed.</p>
    </div>

    <script>
        // Tab functionality - Defined at the top to ensure it's always available
        function showTab(tabId) {
            // Hide all tab contents
            document.querySelectorAll('.tab-content').forEach(content => {
                content.classList.remove('active');
            });
            
            // Deactivate all tabs
            document.querySelectorAll('.tab').forEach(tab => {
                tab.classList.remove('active');
            });
            
            // Show selected tab content
            let selectedContent = document.getElementById(tabId);
            if (selectedContent) {
                selectedContent.classList.add('active');
                
                // Find all plot containers in this tab and trigger a redraw
                let plots = selectedContent.querySelectorAll('.plot-container');
                if (plots.length > 0) {
                    setTimeout(() => {
                        window.dispatchEvent(new Event('resize'));
                    }, 50);
                }
            }
            
            // Activate selected tab
            document.querySelectorAll('.tab').forEach(tab => {
                if (tab.textContent.toLowerCase().includes(tabId.toLowerCase())) {
                    tab.classList.add('active');
                }
            });
            
            // Trigger window resize to ensure plots render correctly
            window.dispatchEvent(new Event('resize'));
        }

        // Generate plots from the data
        document.addEventListener('DOMContentLoaded', function() {
            try {
                // Common variables with defaults to prevent errors
                const perturbationLevels = {{ perturbation_levels|default([])|tojson }};
                const rawMeanScores = {{ raw_mean_scores|default([])|tojson }};
                const quantileMeanScores = {{ quantile_mean_scores|default([])|tojson }};
                const rawWorstScores = {{ raw_worst_scores|default([])|tojson }};
                const quantileWorstScores = {{ quantile_worst_scores|default([])|tojson }};
                const modelNames = {{ model_names|default([])|tojson }};
                const rawModelScores = {{ raw_model_scores|default([])|tojson }};
                const quantileModelScores = {{ quantile_model_scores|default([])|tojson }};
            
            // Raw Perturbation Models Plot
            const rawModelsData = [];
            
            // Add primary model
            rawModelsData.push({
                x: perturbationLevels,
                y: rawMeanScores,
                type: 'scatter',
                mode: 'lines+markers',
                name: 'Primary Model',
                line: {
                    color: 'rgb(31, 119, 180)',
                    width: 2
                },
                marker: {
                    size: 8
                }
            });
            
            // Add alternative models if available
            if (rawModelScores && rawModelScores.length > 0) {
                for (let i = 0; i < modelNames.length; i++) {
                    if (rawModelScores[i] && rawModelScores[i].length > 0) {
                        rawModelsData.push({
                            x: perturbationLevels,
                            y: rawModelScores[i],
                            type: 'scatter',
                            mode: 'lines+markers',
                            name: modelNames[i],
                            line: {
                                width: 2
                            },
                            marker: {
                                size: 8
                            }
                        });
                    }
                }
            }
            
            const rawModelsLayout = {
                title: 'Raw Perturbation - Model Comparison',
                xaxis: {
                    title: 'Perturbation Level'
                },
                yaxis: {
                    title: '{{ metric_name }} Score'
                },
                hovermode: 'closest',
                legend: {
                    orientation: 'h',
                    y: -0.2
                }
            };
            
            Plotly.newPlot('raw-models-plot', rawModelsData, rawModelsLayout);
            
            // Quantile Perturbation Models Plot
            const quantileModelsData = [];
            
            // Add primary model
            quantileModelsData.push({
                x: perturbationLevels,
                y: quantileMeanScores,
                type: 'scatter',
                mode: 'lines+markers',
                name: 'Primary Model',
                line: {
                    color: 'rgb(44, 160, 44)',
                    width: 2
                },
                marker: {
                    size: 8
                }
            });
            
            // Add alternative models if available
            if (quantileModelScores && quantileModelScores.length > 0) {
                for (let i = 0; i < modelNames.length; i++) {
                    if (quantileModelScores[i] && quantileModelScores[i].length > 0) {
                        quantileModelsData.push({
                            x: perturbationLevels,
                            y: quantileModelScores[i],
                            type: 'scatter',
                            mode: 'lines+markers',
                            name: modelNames[i],
                            line: {
                                width: 2
                            },
                            marker: {
                                size: 8
                            }
                        });
                    }
                }
            }
            
            const quantileModelsLayout = {
                title: 'Quantile Perturbation - Model Comparison',
                xaxis: {
                    title: 'Perturbation Level'
                },
                yaxis: {
                    title: '{{ metric_name }} Score'
                },
                hovermode: 'closest',
                legend: {
                    orientation: 'h',
                    y: -0.2
                }
            };
            
            Plotly.newPlot('quantile-models-plot', quantileModelsData, quantileModelsLayout);
            
            // Comparison of Perturbation Methods Plot (Primary Model)
            const perturbationMethodsData = [
                {
                    x: perturbationLevels,
                    y: rawMeanScores,
                    type: 'scatter',
                    mode: 'lines+markers',
                    name: 'Raw Method (Mean)',
                    line: {
                        color: 'rgb(31, 119, 180)',
                        width: 2
                    },
                    marker: {
                        size: 8
                    }
                },
                {
                    x: perturbationLevels,
                    y: quantileMeanScores,
                    type: 'scatter',
                    mode: 'lines+markers',
                    name: 'Quantile Method (Mean)',
                    line: {
                        color: 'rgb(44, 160, 44)',
                        width: 2
                    },
                    marker: {
                        size: 8
                    }
                },
                {
                    x: perturbationLevels,
                    y: rawWorstScores,
                    type: 'scatter',
                    mode: 'lines',
                    name: 'Raw Method (Worst)',
                    line: {
                        color: 'rgb(31, 119, 180)',
                        width: 1,
                        dash: 'dash'
                    }
                },
                {
                    x: perturbationLevels,
                    y: quantileWorstScores,
                    type: 'scatter',
                    mode: 'lines',
                    name: 'Quantile Method (Worst)',
                    line: {
                        color: 'rgb(44, 160, 44)',
                        width: 1,
                        dash: 'dash'
                    }
                }
            ];
            
            const perturbationMethodsLayout = {
                title: 'Comparison of Perturbation Methods (Primary Model)',
                xaxis: {
                    title: 'Perturbation Level'
                },
                yaxis: {
                    title: '{{ metric_name }} Score'
                },
                hovermode: 'closest',
                legend: {
                    orientation: 'h',
                    y: -0.2
                }
            };
            
            Plotly.newPlot('perturbation-methods-plot', perturbationMethodsData, perturbationMethodsLayout);
            
            // Model Comparison Plot
            const modelComparisonData = [];
            
            // Calculate average impact for each model
            const primaryImpact = ({{ avg_raw_impact|default(0)|tojson }} + {{ avg_quantile_impact|default(0)|tojson }}) / 2;
            modelComparisonData.push({
                x: ['Primary Model'],
                y: [primaryImpact],
                type: 'bar',
                name: 'Primary Model',
                marker: {
                    color: 'rgb(31, 119, 180)'
                }
            });
            
            // Add alternative models if available
            if (Array.isArray({{ alternative_models_impact|default([])|tojson }}) && {{ alternative_models_impact|default([])|tojson }}.length > 0) {
                const alternativeImpacts = {{ alternative_models_impact|default([])|tojson }};
                const altModelNames = modelNames.slice();
                const altModelImpacts = [];
                
                for (let i = 0; i < altModelNames.length; i++) {
                    if (i < alternativeImpacts.length) {
                        altModelImpacts.push(alternativeImpacts[i]);
                    }
                }
                
                if (altModelImpacts.length > 0) {
                    modelComparisonData.push({
                        x: altModelNames,
                        y: altModelImpacts,
                        type: 'bar',
                        marker: {
                            color: 'rgb(255, 127, 14)'
                        }
                    });
                }
            }
            
            const modelComparisonLayout = {
                title: 'Model Robustness Comparison (Lower is Better)',
                xaxis: {
                    title: 'Model'
                },
                yaxis: {
                    title: 'Average Impact (Higher = Less Robust)'
                },
                barmode: 'group'
            };
            
            Plotly.newPlot('model-comparison-plot', modelComparisonData, modelComparisonLayout);
            
            // Raw Distribution Plot
            const rawDistributions = {{ raw_distributions|tojson }};
            const rawDistributionData = [];
            
            for (const [level, scores] of Object.entries(rawDistributions)) {
                rawDistributionData.push({
                    y: scores,
                    type: 'box',
                    name: `Level ${level}`,
                    boxpoints: 'all',
                    jitter: 0.3,
                    pointpos: -1.8
                });
            }
            
            const rawDistributionLayout = {
                title: 'Raw Perturbation Score Distribution',
                yaxis: {
                    title: '{{ metric_name }} Score'
                },
                boxmode: 'group'
            };
            
            Plotly.newPlot('raw-distribution-plot', rawDistributionData, rawDistributionLayout);
            
            // Quantile Distribution Plot
            const quantileDistributions = {{ quantile_distributions|tojson }};
            const quantileDistributionData = [];
            
            for (const [level, scores] of Object.entries(quantileDistributions)) {
                quantileDistributionData.push({
                    y: scores,
                    type: 'box',
                    name: `Level ${level}`,
                    boxpoints: 'all',
                    jitter: 0.3,
                    pointpos: -1.8
                });
            }
            
            const quantileDistributionLayout = {
                title: 'Quantile Perturbation Score Distribution',
                yaxis: {
                    title: '{{ metric_name }} Score'
                },
                boxmode: 'group'
            };
            
            Plotly.newPlot('quantile-distribution-plot', quantileDistributionData, quantileDistributionLayout);
            
            // Feature Importance Plot (Percentage)
            const featureNames = {{ feature_names|default([])|tojson }};
            const featureImportanceValues = {{ feature_importance_pct_values|default([])|tojson }};
            
            // Data already sorted by importance (percentage)
            // Take top 15 features for better visualization
            const topFeatures = featureNames.slice(0, 15).map((name, i) => ({
                feature: name,
                importance: featureImportanceValues[i]
            }));
            
            const featureImportanceData = [{
                x: topFeatures.map(f => f.importance),
                y: topFeatures.map(f => f.feature),
                type: 'bar',
                orientation: 'h',
                text: topFeatures.map(f => f.importance.toFixed(2) + '%'),
                textposition: 'auto',
                marker: {
                    color: topFeatures.map(f => f.importance >= 0 ? 'rgba(44, 160, 44, 0.7)' : 'rgba(214, 39, 40, 0.7)')
                }
            }];
            
            const featureImportanceLayout = {
                title: 'Feature Importance for Robustness (Percentage)',
                xaxis: {
                    title: 'Impact on Robustness (%)',
                    ticksuffix: '%'
                },
                yaxis: {
                    title: '',
                    automargin: true
                },
                margin: {
                    l: 150
                }
            };
            
            Plotly.newPlot('feature-importance-plot', featureImportanceData, featureImportanceLayout);
            
            // Metrics Comparison Plot
            const metrics = ['Accuracy', 'AUC', 'F1', 'Precision', 'Recall'];
            
            // Check if we have comparison_metrics_df (from experiment.compare_all_models())
            {% if comparison_metrics_df is defined and comparison_metrics_df|length > 0 %}
                // Prepare metrics comparison data directly from the comparison records
                const metricsData = [];
                
                // Process each model in the comparison metrics
                {% for record in comparison_metrics_df %}
                    // Avoid variable redeclaration by using let and a more unique variable name
                    let comparisonModelName_{{ loop.index }} = "{{ record.model_name }}";
                    let comparisonModelType_{{ loop.index }} = "{{ record.model_type }}";
                    // Get AUC from base_score if roc_auc and auc are missing
                    {% set record_auc = record.roc_auc|default(record.auc|default(record.base_score|default(0))) %}
                    let comparisonModelMetrics_{{ loop.index }} = [
                        {{ record.accuracy|default(0) }},
                        {{ record_auc }},
                        {{ record.f1|default(0) }},
                        {{ record.precision|default(0) }},
                        {{ record.recall|default(0) }}
                    ];
                    
                    // Add this model to the chart
                    metricsData.push({
                        x: metrics,
                        y: comparisonModelMetrics_{{ loop.index }},
                        type: 'bar',
                        name: comparisonModelName_{{ loop.index }},
                        marker: {
                            color: comparisonModelType_{{ loop.index }} === 'original' ? 'rgba(31, 119, 180, 0.7)' : 
                                  comparisonModelType_{{ loop.index }} === 'alternative' ? 'rgba(255, 127, 14, 0.7)' : 
                                  comparisonModelType_{{ loop.index }} === 'distilled' ? 'rgba(44, 160, 44, 0.7)' : 
                                  'rgba(214, 39, 40, 0.7)'
                        }
                    });
                {% endfor %}
            {% elif all_model_metrics is defined and all_model_metrics|length > 0 %}
                // Prepare metrics comparison data from all_model_metrics
                const metricsData = [];
                
                // Process each model in all_model_metrics
                {% for model_name, metrics_data in all_model_metrics.items() %}
                    let altModelName_{{ loop.index }} = "{{ model_name }}";
                    // Get AUC from base_score if roc_auc and auc are missing
                    {% set metrics_auc = metrics_data.get('roc_auc', metrics_data.get('auc', metrics_data.get('base_score', 0))) %}
                    let altModelMetrics_{{ loop.index }} = [
                        {{ metrics_data.get('accuracy', 0) }},
                        {{ metrics_auc }},
                        {{ metrics_data.get('f1', 0) }},
                        {{ metrics_data.get('precision', 0) }},
                        {{ metrics_data.get('recall', 0) }}
                    ];
                    
                    // Add this model to the chart
                    metricsData.push({
                        x: metrics,
                        y: altModelMetrics_{{ loop.index }},
                        type: 'bar',
                        name: altModelName_{{ loop.index }},
                        marker: {
                            color: '{{ loop.index0 }}' === '0' ? 'rgba(31, 119, 180, 0.7)' : 
                                   'rgba(' + (44 + {{ loop.index0 }} * 50) + ', ' + 
                                   (160 - {{ loop.index0 }} * 20) + ', ' + 
                                   (44 + {{ loop.index0 }} * 40) + ', 0.7)'
                        }
                    });
                {% endfor %}
            {% else %}
                // Fall back to original metrics data
                // Get base score for AUC if needed
                {% set auc_value = primary_metrics.roc_auc|default(primary_metrics.auc|default(base_score|default(0))) %}
                const primaryMetricsValues = [
                    {{ primary_metrics.accuracy|default(0) }},
                    {{ auc_value }},
                    {{ primary_metrics.f1|default(0) }},
                    {{ primary_metrics.precision|default(0) }},
                    {{ primary_metrics.recall|default(0) }}
                ];
                
                // Create the alternative models metrics values with their AUC values
                const altMetricsValuesArray = [];
                {% for model_name in model_names %}
                    {% set alt_auc = alt_metrics[loop.index0].roc_auc|default(
                                    alt_metrics[loop.index0].auc|default(
                                    alternative_models[model_name].base_score|default(0))) %}
                    altMetricsValuesArray.push([
                        {{ alt_metrics[loop.index0].accuracy|default(0) }},
                        {{ alt_auc }}, 
                        {{ alt_metrics[loop.index0].f1|default(0) }},
                        {{ alt_metrics[loop.index0].precision|default(0) }},
                        {{ alt_metrics[loop.index0].recall|default(0) }}
                    ]);
                {% endfor %}
                
                // Prepare metrics comparison data
                const metricsData = [];
                
                // Primary model metrics
                metricsData.push({
                    x: metrics,
                    y: primaryMetricsValues,
                    type: 'bar',
                    name: 'Primary Model',
                    marker: {
                        color: 'rgba(31, 119, 180, 0.7)'
                    }
                });
                
                // Alternative models metrics
                const modelNamesArray = {{ model_names|default([])|tojson }};
                for (let i = 0; i < modelNamesArray.length; i++) {
                    if (altMetricsValuesArray[i] && Array.isArray(altMetricsValuesArray[i])) {
                        metricsData.push({
                            x: metrics,
                            y: altMetricsValuesArray[i],
                            type: 'bar',
                            name: modelNamesArray[i],
                            marker: {
                                color: i === 0 ? 'rgba(255, 127, 14, 0.7)' : 
                                       i === 1 ? 'rgba(44, 160, 44, 0.7)' :
                                       i === 2 ? 'rgba(214, 39, 40, 0.7)' :
                                       'rgba(148, 103, 189, 0.7)'
                            }
                        });
                    }
                }
            {% endif %}
            
            const metricsLayout = {
                title: 'Model Metrics Comparison',
                xaxis: {
                    title: 'Metric'
                },
                yaxis: {
                    title: 'Score',
                    range: [0, 1.05]
                },
                barmode: 'group',
                legend: {
                    orientation: 'h',
                    y: -0.2
                }
            };
            
            Plotly.newPlot('metrics-comparison-plot', metricsData, metricsLayout);
            
            {% if feature_subset %}
            // Feature Subset Comparison Plot
            const featureSubsetData = {{ feature_subset_comparison|default({})|tojson }};
            
            const featureSubsetPlotData = [
                {
                    x: perturbationLevels,
                    y: featureSubsetData.all_features,
                    type: 'scatter',
                    mode: 'lines+markers',
                    name: 'All Features',
                    line: {
                        color: 'rgb(31, 119, 180)',
                        width: 2
                    }
                },
                {
                    x: perturbationLevels,
                    y: featureSubsetData.feature_subset,
                    type: 'scatter',
                    mode: 'lines+markers',
                    name: 'Feature Subset',
                    line: {
                        color: 'rgb(255, 127, 14)',
                        width: 2
                    }
                }
            ];
            
            const featureSubsetLayout = {
                title: 'Comparison: All Features vs. Feature Subset',
                xaxis: {
                    title: 'Perturbation Level'
                },
                yaxis: {
                    title: '{{ metric_name }} Score'
                }
            };
            
            Plotly.newPlot('feature-subset-comparison-plot', featureSubsetPlotData, featureSubsetLayout);
            {% endif %}
            
            // Listen for tab changes to redraw plots
            document.querySelectorAll('.tab').forEach(tab => {
                tab.addEventListener('click', function() {
                    // Give plots time to become visible before redrawing them
                    setTimeout(function() {
                        window.dispatchEvent(new Event('resize'));
                    }, 50);
                });
            });
            
            } catch(error) {
                console.error("Error creating plots:", error);
                // Show friendly error message in the UI
                document.getElementById('raw').innerHTML = 
                    '<div style="color:red;padding:20px;">Error creating plots: ' + error.message + 
                    '<br><br>Please try refreshing the page or check the data format.</div>';
            }
        });
        
        // Add a fail-safe for tab clicking
        function addTabHandlers() {
            // Ensure all tabs have proper handlers
            document.querySelectorAll('.tab').forEach(tab => {
                tab.onclick = function() {
                    // Extract the tab name
                    const tabText = this.textContent.trim().toLowerCase();
                    if (tabText.includes('raw')) showTab('raw');
                    else if (tabText.includes('quantile')) showTab('quantile');
                    else if (tabText.includes('models')) showTab('models');
                    else if (tabText.includes('features')) showTab('features');
                    else if (tabText.includes('metrics')) showTab('metrics');
                    else if (tabText.includes('detailed')) showTab('detailed');
                    
                    // Prevent default and stop propagation
                    return false;
                };
            });
        }
        
        // Apply tab handlers when document is ready
        document.addEventListener('DOMContentLoaded', addTabHandlers);
        
        // Also apply after a delay to handle potential race conditions
        setTimeout(addTabHandlers, 500);
        
        // Additional global click handler for redundancy
        document.addEventListener('click', function(event) {
            if (event.target.classList.contains('tab')) {
                // Extract the tab name
                const tabText = event.target.textContent.trim().toLowerCase();
                if (tabText.includes('raw')) showTab('raw');
                else if (tabText.includes('quantile')) showTab('quantile');
                else if (tabText.includes('models')) showTab('models');
                else if (tabText.includes('features')) showTab('features');
                else if (tabText.includes('metrics')) showTab('metrics');
                else if (tabText.includes('detailed')) showTab('detailed');
                
                // Prevent default and stop propagation
                event.preventDefault();
                event.stopPropagation();
                return false;
            }
        }, true);
    </script>
</body>
</html>